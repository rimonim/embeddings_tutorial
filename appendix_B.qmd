# Appendix B

# CCR Metrics

```{r}
#| echo: true
#| eval: false
library(tidyverse)
library(quanteda)
source("embedding_scripts.R")
```

@atari_etal_2023 asked participants to describe their core values in their own words (values essay) and to likewise describe their activities in the past week (behaviors essay). They then assessed participants on 22 questionnaire-based scales. This design allowed them to validate CCR by obtaining a contextualized embedding of each questionnaire and comparing it to the contextualized embedding of each essay. In benchmark 1 we partially replicate Atari et al.'s analysis and extend it to various vector-based metrics, including anchored vectors. In benchmark 2, we investigate the extent to which the techniques generalize to a more naturalistic context.

## Benchmark 1: Prompted Essays

The anchored vector is equivalent to the embedding of the questionnaire (positive CCR) minus the embedding of the negated questionnaire (negative CCR). The individualism items were used as a negated form of the collectivism, and vice versa. To obtain negated versions of the remaining questionnaires, we queried GPT-4o and manually curated the results.[^11] We investigated the following metrics:

[^11]: All matrials, including original and negated questionnaire items, are available on our Github repo at https://github.com/rimonim/embeddings_tutorial.

-   **Cosine similarity** with the CCR
-   **Cosine similarity with the anchored vector** (equivalent to projection of the normalized text vector onto the anchored vector)
-   **Dot product** with the CCR
-   **Dot product with the anchored vector** (equivalent to projection of the raw text vector onto the anchored vector)
-   **Dot product with the pre-normalized anchored vector** (i.e. positive and negative CCR embeddings normalized before calculating the anchored vector)
-   **Euclidean distance** from the CCR
-   **Euclidean distance from the anchored vector**

Although the term CCR implies the use of contextualized embeddings, we use it here to refer to any vector embedding of a questionnaire. To obtain embeddings for participant essays and questionnaire items, we used two pretrained models:

-   **SBERT** ([`sentence-transformers/all-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)), a model similar to that used by @atari_etal_2023, designed to produce 384-dimensional contextualized embeddings amenable to use with cosine similarity.
-   **GloVe** ([`glove.twitter.27B.100d`](https://huggingface.co/stanfordnlp/glove/tree/main)) a pretrained word embedding model with a 100-dimensional embedding space. GloVe embeddings each text were obtained by aggregating the embeddings of each word in the text, discounting tokens not available in the pretrained GloVe model. Since GloVe is a decontextualized model, we used it here as a baseline for evaluating the utility of contextualized embeddings in CCR analysis.

As a further baseline, we also performed DDR using dictionaries for each construct and its opposite. These dictionaries were generated by GPT-4o and manually curated, but were not validated in any way prior to testing. We can therefore consider this DDR to be a conservative baseline for evaluating CCR methods. DDR analysis was performed with the same GloVe model described above.

All code is available in the source code for this appendix on Github.

```{r}
#| eval: false
#| echo: false
library(tidyverse)
library(quanteda)
library(text)

#~~~~~~~~~~~
## LOAD DATA

# data from https://osf.io/bu6wg/
CCR_behavioral <- read_csv("benchmarks/CCR/CCR_clean_behavioral.csv")

CCR_items <- read_csv("benchmarks/CCR/Questionnaires - Experiment - Questionnaire.csv") |> 
  pivot_longer(
    everything(),
    names_to = "construct",
    values_to = "item",
    values_drop_na = TRUE
  ) |> 
  mutate(construct = str_to_lower(str_extract(construct, "[:letter:]+"))) |> 
  arrange(construct)

names(CCR_behavioral)[3:24] <- c(
  "care", "equality", "proportionality", "loyalty", "authority", "purity", 
  "individualism", "collectivism", "sd", "po", "un", "ac", "se", "st", "co", 
  "tr", "he", "be", "religiosity", "nfc", "conservatism", "tightness"
  )

# negative items
# - individualism and collectivism were used as each other's opposites
# - otherwise, reverse items were obtained from GPT-4o and curated manually
CCR_items_neg <- read_csv("benchmarks/CCR/questionnaires_neg.csv") |> 
  pivot_longer(
    everything(),
    names_to = "construct",
    values_to = "item",
    values_drop_na = TRUE
  ) |> 
  mutate(construct = str_to_lower(str_extract(construct, "[:letter:]+"))) |> 
  arrange(construct)

# DDR items (obtained from GPT-4o and curated manually)
DDR_items <- read_csv("benchmarks/CCR/ddr_items.csv")
DDR_items_neg <- read_csv("benchmarks/CCR/ddr_items_neg.csv")
#~~~~~~~~~~~
## FUNCTIONS

# relevant scripts
source("embedding_scripts.R")

# consistent embedding scheme
contextualized_embedding <- function(x){
  textEmbed(
    x,
    model = "sentence-transformers/all-MiniLM-L12-v2", # model name
    layers = -1,  # last layer
    dim_name = FALSE,
    keep_token_embeddings = FALSE
  )
}

average_vector <- function(mat){
  mat <- as.matrix(mat)
  apply(mat, 2, mean)
}

# word embeddings
path_to_glove <- "~/Documents/data/embeddings/glove.twitter.27B.100d.txt"
glove_dimensions <- as.numeric(str_extract(path_to_glove, "[:digit:]+(?=d\\.txt)"))
glove_pretrained <- read_delim(
  path_to_glove, 
  delim = " ",
  quote = "",
  escape_double = FALSE,
  col_names = c("token", paste0("dim_", 1:glove_dimensions))
) |> distinct(token, .keep_all = TRUE) |> column_to_rownames("token") |> as.matrix()
class(glove_pretrained) <- "embeddings"

# outputs 
new_scores <- function(dat, cols, pos_mean, neg_mean, prefix = "",
                       schemes = c("mean_dot", "mean_cos", "mean_euc", 
                                   "negdiff_dot", "negdiff_cos", "negdiff_euc", 
                                   "anchoredvec_raw", "anchoredvec_norm")){
  dat <- dat |> rowwise()
  for(scheme in schemes){
    new_col_name = paste0(prefix, scheme)
    ccr <- pos_mean
    ccr_neg <- neg_mean
    if(str_detect(scheme, "negdiff_")){
      ccr <- ccr - ccr_neg
    }
    
    if(str_detect(scheme, "anchoredvec_")){
      if(scheme == "anchoredvec_norm"){
        ccr <- ccr/sqrt(sum(ccr^2))
        ccr_neg <- ccr_neg/sqrt(sum(ccr_neg^2))
      }
      dat <- dat |> mutate(!!new_col_name := anchored_sim(c_across({{cols}}), ccr, ccr_neg))
    }else{
      if(str_detect(scheme, "_dot")){
        dat <- dat |> mutate(!!new_col_name := dot_prod(c_across({{cols}}), ccr))
      }else if(str_detect(scheme, "_cos")){
        dat <- dat |> mutate(!!new_col_name := cos_sim(c_across({{cols}}), ccr))
      }else if(str_detect(scheme, "_euc")){
        dat <- dat |> mutate(!!new_col_name := -euc_dist(c_across({{cols}}), ccr))
      }
    }
  }
  dat |> ungroup() |> select(-{{cols}})
}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## QUESTIONNAIRE EMBEDDINGS (matrices)

CCR_items_glove <- CCR_items$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  bind_cols(CCR_items) |> 
  group_by(construct) |> 
  summarise(across(dim_1:dim_100, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

CCR_items_neg_glove <- CCR_items_neg$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  bind_cols(CCR_items) |> 
  group_by(construct) |> 
  summarise(across(dim_1:dim_100, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

CCR_items_sbert <- contextualized_embedding(CCR_items$item)$texts[[1]] |> 
  bind_cols(CCR_items) |> 
  group_by(construct) |> 
  summarise(across(Dim1:Dim384, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

CCR_items_neg_sbert <- contextualized_embedding(CCR_items_neg$item)$texts[[1]] |> 
  bind_cols(CCR_items_neg) |> 
  group_by(construct) |> 
  summarise(across(Dim1:Dim384, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

DDR_items_glove <- DDR_items$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  bind_cols(DDR_items) |> 
  select(-doc_id, -item) |> 
  column_to_rownames("construct") |> 
  as.matrix()

DDR_items_neg_glove <- DDR_items_neg$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  bind_cols(DDR_items_neg) |> 
  select(-doc_id, -item) |> 
  column_to_rownames("construct") |> 
  as.matrix()

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## PARTICIPANT TEXT EMBEDDINGS

# response embeddings
values_survey_sbert <- contextualized_embedding(CCR_behavioral$ValuesSurvey)
# saveRDS(values_survey_sbert, "~/Projects/ds4psych/data/values_survey_sbert.rds")
behaviors_survey_sbert <- contextualized_embedding(CCR_behavioral$BehaviorsSurvey)
# saveRDS(behaviors_survey_sbert, "~/Projects/ds4psych/data/behaviors_survey_sbert.rds")

values_survey_sbert <- readRDS("~/Projects/ds4psych/data/values_survey_sbert.rds")
behaviors_survey_sbert <- readRDS("~/Projects/ds4psych/data/behaviors_survey_sbert.rds")

values_survey_glove <- CCR_behavioral$ValuesSurvey |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)
behaviors_survey_glove <- CCR_behavioral$BehaviorsSurvey |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)

#~~~~~~~~~~~~~~~~
## COMPUTE SCORES

for (construct in rownames(CCR_items_sbert)) {
  CCR_behavioral <- CCR_behavioral |> 
    bind_cols(
      new_scores(
        values_survey_sbert$texts[[1]],
        Dim1:Dim384, 
        CCR_items_sbert[construct,], 
        CCR_items_neg_sbert[construct,], 
        prefix = paste0(construct,"_values_sbert_")
        ),
      new_scores(
        values_survey_glove,
        dim_1:dim_100, 
        CCR_items_glove[construct,], 
        CCR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_values_glove_")
        ),
      new_scores(
        values_survey_glove,
        dim_1:dim_100, 
        DDR_items_glove[construct,], 
        DDR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_values_ddr_")
        ),
      new_scores(
        behaviors_survey_sbert$texts[[1]],
        Dim1:Dim384, 
        CCR_items_sbert[construct,], 
        CCR_items_neg_sbert[construct,], 
        prefix = paste0(construct,"_behaviors_sbert_")
        ),
      new_scores(
        behaviors_survey_glove,
        dim_1:dim_100, 
        CCR_items_glove[construct,], 
        CCR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_behaviors_glove_")
        ),
      new_scores(
        behaviors_survey_glove,
        dim_1:dim_100, 
        DDR_items_glove[construct,], 
        DDR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_behaviors_ddr_")
        )
    )
}

#~~~~~~~~~~~~
## RUN MODELS

results_grid <- expand_grid(
  construct = rownames(CCR_items_sbert),
  text = c("behaviors", "values"),
  model = c("glove", "sbert", "ddr"),
  scheme = c("mean_dot", "mean_cos", "mean_euc", 
             "negdiff_dot", "negdiff_cos", "negdiff_euc", 
             "anchoredvec_norm")
) |> bind_cols(expand_grid(
  construct_disp = c(
    "Achievement", "Authority", "Benevolence", "Care", 
    "Conformity", "Collectivism", "Conservatism", 
    "Equality", "Hedonism", "Individualism", 
    "Loyalty", "Need for Cognition", "Power", "Proportionality", 
    "Purity", "Religiosity", "Self-Direction", "Safety", 
    "Stimulation", "Tightness", "Tradition", "Universalism"
    ),
  text_disp = c("Behaviors Essay", "Values Essay"),
  model_disp = c("GloVe", "SBERT", "DDR"),
  scheme_disp = c(
    "Dot", "Cosine", "Euclidean", 
    "Dot (Anchored Vector)", "Cosine (Anchored Vector)", "Euclidean (Anchored Vector)", 
    "Dot (Pre-normalized Anchored Vector)"
    )
))

results_grid <- results_grid |> 
  mutate(R2 = NA, Beta = NA, SE = NA, sig = NA)

for (row in 1:nrow(results_grid)) {
  construct <- results_grid$construct[row]
  text <- results_grid$text[row]
  model <- results_grid$model[row]
  scheme <- results_grid$scheme[row]
  
  var_name <- paste(construct, text, model, scheme, sep = "_")
  row_form <- as.formula(paste0("scale(",construct,")~scale(",var_name,")"))
  
  mod <- lm(row_form, data = CCR_behavioral)
  results_grid$R2[row] <- summary(mod)$r.squared
  results_grid$Beta[row] <- summary(mod)$coefficients[[2,"Estimate"]]
  results_grid$SE[row] <- summary(mod)$coefficients[[2,"Std. Error"]]
  results_grid$sig[row] <- summary(mod)$coefficients[[2,"Pr(>|t|)"]]
}

write_csv(results_grid, "benchmarks/CCR/results.csv")
```

### Results: Values Essay

In predicting questionnaire responses using participant values essays, the most effective metrics were as follows:

**DDR:**

-   Dot product with anchored vector (mean R^2^ = 0.016)
-   Dot product with pre-normalized anchored vector (mean R^2^ = 0.015)
-   Cosine similarity with anchored vector (mean R^2^ = 0.012)

**GloVe:**

-   Cosine similarity with anchored vector (mean R^2^ = 0.010)
-   Dot product with pre-normalized anchored vector (mean R^2^ = 0.009)

**SBERT:**

-   Dot product with anchored vector (mean R^2^ = 0.023)
-   Cosine similarity with anchored vector (mean R^2^ = 0.023)
-   Dot product with pre-normalized anchored vector (mean R^2^ = 0.022)

```{r}
#| include: false
results_grid <- read_csv("benchmarks/CCR/results.csv")

construct_agg <- results_grid |> 
  mutate(R2 = if_else(sig < .05 | Beta > 0, R2, 0)) |> 
  group_by(text, construct, construct_disp) |> 
  summarise(R2 = mean(R2),
            R2_sd = sd(R2),
            R2_max = max(R2)) |> 
  group_by(text) |> 
  arrange(R2_max) |> 
  mutate(construct_disp = factor(construct_disp))

scheme_agg <- results_grid |> 
  mutate(R2 = if_else(sig < .05 | Beta > 0, R2, 0)) |> 
  group_by(text, model_disp, scheme_disp) |> 
  summarise(R2 = mean(R2),
            R2_sd = sd(R2),
            R2_max = max(R2)) |> 
  group_by(text, model_disp) |> 
  arrange(R2) |> 
  mutate(scheme_disp = factor(scheme_disp))
```

```{r}
#| label: fig-values_essay
#| echo: false
#| warning: false
#| fig-cap: Values Essay
#| apa-note: Negative or insignificant effects (p > 0.05) are displayed as translucent and are considered to be 0 in metric-wise averages.
#| fig-height: 8
#| fig-width: 8

results_grid |> 
  filter(text == "values") |> 
  mutate(construct_disp = factor(
    construct_disp, 
    levels = construct_agg$construct_disp[construct_agg$text=="values"]
    )) |> 
  ggplot(
    aes(R2*100, construct_disp, 
        xmin = 0, xmax = R2*100,
        color = scheme_disp, alpha = sig < .05 & Beta > 0)
    ) +
    geom_linerange(position = position_dodge(width = 4/5), alpha = 0) +
    annotate("tile", 3, seq(1,21,by=2), width = Inf, alpha = .1) +
    geom_point(position = position_dodge(width = 4/5), size = 2) +
    geom_vline(aes(xintercept = R2*100, color = scheme_disp), 
               linewidth = 1,
               data = scheme_agg |> filter(text=="values")) +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    facet_wrap(~model_disp, ncol = 1) +
    guides(alpha = "none") +
    labs(x = "Variance Explained (%)",
         y = "Questionnaire",
         linetype = "Model",
         color = "Metric") +
    theme_bw() +
    theme(plot.caption = element_text(hjust = .1))
```

For values essays, SBERT was consistently more effective than GloVe, and generally better than DDR. In all models, Euclidean distance metrics were minimally effective and most likely to result in significant negative effects. The success of GloVe-based CCR with metrics that involve anchored vectors is surprising, since most negated questionnaire items only differ from the originals by the words "do not" or similarly uninformative negations.

```{r}
#| label: fig-values_essay_neg
#| echo: false
#| fig-cap: Significant Negative Effects in Values Essay
#| fig-height: 3
#| fig-width: 8
results_grid |> 
  filter(text == "values") |> 
  group_by(model_disp, scheme_disp) |> 
  summarise(prop_neg = sum(Beta < 0 & sig < 0.05)/n(), .groups = "keep") |> 
  ggplot(aes(scheme_disp, prop_neg*100, fill = scheme_disp)) +
    geom_bar(stat = "identity") +
    geom_hline(yintercept = 0, linewidth = 1.5) +
    facet_wrap(~model_disp) +
    scale_fill_manual(values = c(
        "navyblue", "dodgerblue",
        "orange", "red2", "firebrick4",
        "seagreen2", "seagreen4"
      )) +
    labs(y = "Significant Negative Effects (%)",
         fill = "Metric") +
    theme_bw() +
    theme(axis.text.x = element_blank(),
          axis.title.x = element_blank())
```

### Results: Behaviors Essay

In predicting questionnaire responses using participant behaviors essays, the most effective metrics were as follows:

**DDR:**

-   Dot product with anchored vector (mean R^2^ = 0.017)
-   Dot product with CCR (mean R^2^ = 0.016)
-   Euclidean distance from anchored vector (mean R^2^ = 0.014)

**GloVe:**

-   Dot product with CCR (mean R^2^ = 0.014)
-   Euclidean distance from anchored vector (mean R^2^ = 0.014)
-   Dot product with anchored vector (mean R^2^ = 0.006)

**SBERT:**

-   Euclidean distance from anchored vector (mean R^2^ = 0.008)
-   Dot product with CCR (mean R^2^ = 0.006)

```{r}
#| label: fig-behaviors_essay
#| echo: false
#| warning: false
#| fig-cap: Behaviors Essay
#| apa-note: Negative or insignificant effects (p > 0.05) are displayed as translucent and are considered to be 0 in metric-wise averages.
#| fig-height: 8
#| fig-width: 8
results_grid |> 
  filter(text == "behaviors") |> 
  mutate(construct_disp = factor(
    construct_disp, 
    levels = construct_agg$construct_disp[construct_agg$text=="behaviors"]
    )) |> 
  ggplot(
    aes(R2*100, construct_disp, 
        xmin = 0, xmax = R2*100,
        color = scheme_disp, alpha = sig < .05 & Beta > 0)
    ) +
    geom_linerange(position = position_dodge(width = 4/5), alpha = 0) +
    annotate("tile", 3, seq(1,21,by=2), width = Inf, alpha = .1) +
    geom_point(position = position_dodge(width = 4/5), size = 2) +
    geom_vline(aes(xintercept = R2*100, color = scheme_disp), 
               linewidth = 1,
               data = scheme_agg |> filter(text=="behaviors")) +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    facet_wrap(~model_disp, ncol = 1) +
    guides(alpha = "none") +
    labs(x = "Variance Explained (%)",
         y = "Questionnaire",
         linetype = "Model",
         color = "Metric") +
    theme_bw() +
    theme(plot.caption = element_text(hjust = .1))
```

For behaviors essays, GloVe-based CCR was more consistently effective than SBERT, and DDR was most effective overall. While Euclidean distance from the anchored vector was most effective on average for both models, it was also most likely to result in significant negative effects.

```{r}
#| label: fig-behaviors_essay_neg
#| echo: false
#| fig-cap: Significant Negative Effects in Behaviors Essay
#| fig-height: 3
#| fig-width: 8
results_grid |> 
  filter(text == "behaviors") |> 
  group_by(model_disp, scheme_disp) |> 
  summarise(prop_neg = sum(Beta < 0 & sig < 0.05)/n(), .groups = "keep") |> 
  ggplot(aes(scheme_disp, prop_neg*100, fill = scheme_disp)) +
    geom_bar(stat = "identity") +
    geom_hline(yintercept = 0, linewidth = 1.5) +
    facet_wrap(~model_disp) +
    scale_fill_manual(values = c(
        "navyblue", "dodgerblue",
        "orange", "red2", "firebrick4",
        "seagreen2", "seagreen4"
      )) +
    labs(y = "Significant Negative Effects (%)",
         fill = "Metric") +
    theme_bw() +
    theme(axis.text.x = element_blank(),
          axis.title.x = element_blank())
```

## Conclusions

CCR [@atari_etal_2023] proposes to measure psychological constructs in texts by comparing text embeddings to embeddings of questionnaires. This approach to text analysis can be highly effective, but is sensitive to the nature of both questionnaires and texts. Some questionnaires do not appear to be amenable to CCR at all, including those used here for tightness, collectivism, individualism, proportionality, equality, and safety. Among those scales that were amenable to CCR metrics, the pattern of optimal metrics was greatly influenced by the content of the texts being analyzed.

Values essays tend to be similar in content to the values questionnaires being used, since the questionnaire items almost all consist of statements in the first person. Contextualized embeddings from an SBERT model appear to perform best in these sorts of situations.

Behaviors essays, which bear little resemblance to questionnaire items in either tone or content, behave very differently. In particular, GloVe embeddings of the questionnaires tend to perform better. This may be due to the consistent geometric properties of GloVe embeddings, which can more reliably encode semantic relationships between very different contexts. Alternatively, this may be due to the datasets used to train SBERT models, which emphasize topical similarity rather than similarity in tone [@reimers_gurevych_2019]. Unsurprisingly, GloVe embeddings of dictionaries associated with the questionnaires performed best overall.

Overall, the results of this analysis suggest that CCR can be a powerful tool, but that it should be used primarily in contexts in which the content of the questionnaires is similar to that of the texts being analyzed. In such cases, we recommend negating the questionnaire items, computing an anchored vector, and scoring texts by the dot products of their embeddings with the anchored vector.

When the content of the questionnaires is not similar to that of the texts being analyzed, we recommend using DDR with negative and positive versions of each dictionary, and scoring texts by the dot products of their embeddings with the anchored vector. This approach appears to outperform CCR even with unvalidated dictionaries generated by GPT-4o.
