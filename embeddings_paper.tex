\documentclass[
  man,
  floatsintext,
  longtable,
  nolmodern,
  notxfonts,
  notimes,
  colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{apa7}

\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[bidi=default]{babel}
\babelprovide[main,import]{english}


% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}

\RequirePackage{longtable}
\RequirePackage{threeparttablex}

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-.5em}%
	{\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{0.5em}%
	{0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
	{-\z@\relax}%
	{\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother




\usepackage{longtable, booktabs, multirow, multicol, colortbl, hhline, caption, array, float, xpatch}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}

\usepackage{tcolorbox}
\tcbuselibrary{listings,theorems, breakable, skins}
\usepackage{fontawesome5}

\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{ACACAC}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582EC}
\definecolor{quarto-callout-important-color-frame}{HTML}{D9534F}
\definecolor{quarto-callout-warning-color-frame}{HTML}{F0AD4E}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02B875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{FD7E14}

%\newlength\Oldarrayrulewidth
%\newlength\Oldtabcolsep


\usepackage{hyperref}



\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}

\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}





\usepackage{newtx}

\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}





\title{Neural Text Embeddings in Psychological Research: A Guide With
Examples in R}


\shorttitle{Neural Text Embeddings}


\usepackage{etoolbox}








\authorsnames{Louis Teitelbaum,Almog Simchon}





\affiliation{
{Department of Psychology, Ben-Gurion University of the Negev}}




\leftheader{Teitelbaum and Simchon}



\abstract{As the capabilities of neural language models grow,
psychologists in various subfields have begun to use them as tools in
analysis. We introduce leading methods for quantifying psychological
dimensions of text using neural semantic embeddings, and discuss their
respective advantages and disadvantages. For the uninitiated reader, we
provide an intuitive conceptual introduction to semantic embeddings, and
discuss practical and theoretical differences between embeddings
generated by word embedding models (e.g.~word2vec; GloVe) and those
generated by transformer-based large language models (LLMS). We review
three methods of quantifying psychological constructs in embedding
spaces: Distributed Dictionary Representation (DDR), Contextualized
Construct Representation (CCR), and Correlational Anchored Vectors
(CAV). We explore potential pitfalls of each method, and recommend best
practices for their application in research. For each method introduced,
we provide sample code in R.}

\keywords{natural-language-procressing, r, deep-learning, word-embeddings, text-embeddings}

\authornote{\par{\addORCIDlink{Louis
Teitelbaum}{0009-0001-9347-0145}}\par{\addORCIDlink{Almog
Simchon}{0000-0003-2629-2913}} 

\par{       }
\par{Correspondence concerning this article should be addressed to Louis
Teitelbaum, Department of Psychology, Ben-Gurion University of the
Negev, POB 653, Beer Sheva 84105, Israel, Email: louist@post.bgu.ac.il}
}

\makeatletter
\let\endoldlt\endlongtable
\def\endlongtable{
\hline
\endoldlt
}
\makeatother

\urlstyle{same}



\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

% From https://tex.stackexchange.com/a/645996/211326
%%% apa7 doesn't want to add appendix section titles in the toc
%%% let's make it do it
\makeatletter
\xpatchcmd{\appendix}
  {\par}
  {\addcontentsline{toc}{section}{\@currentlabelname}\par}
  {}{}
\makeatother

\begin{document}

\maketitle


\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\setlength\LTleft{0pt}


Psychologists have long insisted that talk therapy can heal, and that
questionnaires can accurately measure psychological phenomena. These are
language-based techniques, which rely on the assumption that language
processing is linked to more fundamental internal states. Even so,
psychological research has historically been unable to study
\emph{naturalistic language}---the sort of language that people produce
in their day-to-day lives.

The first barrier to research of naturalistic language is that it has
historically been difficult to record. Early efforts by linguists to
record language samples from representative populations were heroic;
starting in 1896, Edmond Edmont spent four years traveling around France
on a bicycle conducting specially designed interviews to collect data
for the \emph{Atlas linguistique de la France}. He collected data from
700 participants in total (Crystal, 1997). Since then, microphones have
made it easier to record speech, and the advent of transformer neural
networks has made it possible to accurately transcribe speech at minimal
cost. While these advances in audio processing are important, the most
important recent change in the availability of natural language to
psychologists has come through a different medium: text. Only a few
decades ago, public access to text was limited to highly edited
long-form productions like books, magazines, and newspapers. Some
psychologists studied diaries or personal letters (e.g. Allport, 1942;
Creegan, 1944), but personal documents like these are hard to collect at
scale. With the rise of social media, minimally edited, naturalistic
text has become the norm. Now more than ever before, people communicate
through text---not just in long-distance correspondence, but for
day-to-day socializing with friends and family. Moreover, much of this
textual communication is synchronous and shares many of the same
features as face-to-face spoken conversation (Placiński \& Żywiczyński,
2023). Most importantly, much of this textual communication is freely
available to researchers through social media platforms like Reddit and
YouTube.

A second barrier to the use of naturalistic language in psychology is
the problem of quantification. Language is complex, with near-infinite
ways to describe the same thing. There are no clear rules for measuring
the extent to which a text reflects depression, anxiety, mania,
introspection, or any other psychological construct. Before the past two
decades, the few researchers who tried to extract quantitative
psychological dimensions of text were nearly as heroic as Edmond Edmont
on his four year journey around France. For example, Peterson and
Seligman (1984) administered a questionnaire that prompted participants
to write short explanations of various hypothetical scenarios. They then
carefully read each response, noted each time a phrase like ``because
of'' or ``as a result of'' was mentioned, and marked the accompanying
explanation. These explanations were then typed by hand and shown one at
a time to four trained judges who rated them on various 7-point scales.
Finally, the agreement between the judges was assessed and their ratings
were aggregated into the final variable used in their analysis of risk
factors for depression. Today, this sort of analysis could be performed
in a matter of seconds using the methods discussed in this guide.

Nuanced quantitative measures of language can also solve a third
historical problem with naturalistic language in psychology: its social
nature. People do not write or speak in a vacuum, they participate in
conversations or group discussions, considering their audiences as they
form their words. For the researcher, this means that language is full
of uncontrolled, confounding variables: Is the speaker responding to
another speaker? Who is the other speaker? How many participants are
there in the conversation? Researchers in the field of psycholinguistics
have tried to solve these problems by isolating speakers in a laboratory
setting, contriving situations in which participants process and produce
speech without the uncontrolled variability of conversational partners
(O'Connell \& Kowal, 2003). Nevertheless, the inherently social nature
of language has made it difficult to analyze language behavior in even
remotely naturalistic settings. A few decades ago the question ``How
similar are Daniel's utterances to Amos's utterances?'' would have
seemed hopelessly ill-defined. Similar in what way? Today, answering
this question is simple with vector-based text embeddings.

In this guide, we introduce state of the art methods for using neural
text embeddings to quantify psychological dimensions of text. We aim to
explain the conceptual foundations of these methods in a way that is
accessible to psychology researchers with no background in machine
learning or mathematics. We note methodological concerns and give
concrete recommendations regarding proper usage of the techniques
discussed. Throughout, we provide example code in R using the tidyverse
(Wickham et al., 2019) and Quanteda (Benoit et al., 2018) packages.
Other packages are used more sparingly and will be noted as they become
relevant.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(quanteda)}
\end{Highlighting}
\end{Shaded}

We assume that the reader has basic proficiency in both of these
frameworks. For a broader introduction to psychological text analysis
using the tidyverse and Quanteda, see Teitelbaum and Simchon (2024). To
follow along with the example code for this guide, please also load our
suite of custom functions, available on our Github repo.\footnote{https://github.com/rimonim/embeddings\_tutorial/blob/master/embedding\_scripts.R}
For readers who are primarily interested in the example code, the Github
repo offers full example workflows for each method discussed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"embedding\_scripts.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Abstract Embedding Spaces}\label{abstract-embedding-spaces}

The modern ability to generate nuanced, quantitative representations of
language relies on a key assumption: The meaning of any word (or other
subcomponent of text, called a ``token'') can be represented as a point
in a single, continuous space. This space is referred to as the
\emph{semantic embedding space}, since tokens are ``embedded'' into it
according to their semantic content. Points close to each other in the
embedding space have similar meanings, while those far away convey
different meanings. While certain directions in this space may
correspond to a familiar scale along which meaning can vary
(e.g.~largeness vs.~smallness), the dimensions of the space are
essentially abstract; points are only defined relative to each other,
through the use of distance metrics. Embedding spaces can be difficult
to imagine, not only because of their abstract nature but also because
they have many more than three dimensions---often hundreds or even
thousands. This high dimensionality is necessary to represent the many
ways in which linguistic meaning can vary.

\subsection{Distance and Similarity}\label{distance-and-similarity}

Once words or texts are embedded into the semantic space, how do we
quantify their relative positions? Here we introduce the most intuitive
similarity metric---Euclidean distance---as well as the most commonly
used metric for neural text embeddings---cosine similarity.

\subsubsection{Euclidean Distance}\label{euclidean-distance}

The most straightforward way to measure the similarity between two
points in space is to measure the distance between them. \emph{Euclidean
distance} is the simplest sort of distance---the length of the shortest
straight line between the two points. The Euclidean distance between two
vectors \(A\) and \(B\) can be calculated in any number of dimensions
\(n\) using the following formula:

\[
d\left( A,B\right)   = \sqrt {\sum _{i=1}^{n}  \left( A_{i}-B_{i}\right)^2 }
\]

A low Euclidean distance means two vectors are very similar.

\subsubsection{Cosine Similarity}\label{cosine-similarity}

The most common way to measure the similarity between two vectors is
with cosine similarity. This is the cosine of the angle between the two
vectors. Since the cosine of 0 is 1, a high cosine similarity (close to
1) means two vectors are very similar. To give a simplified example, we
consider the vectors in Table~\ref{tbl-example_vecs}.

\begin{table}

{\caption{{Example Three-Dimensional Vectors}{\label{tbl-example_vecs}}}
\vspace{-20pt}}

\begin{longtable}[]{@{}lrrr@{}}
\toprule\noalign{}
vector & Dim1 & Dim2 & Dim3 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Vector A & 3 & 7 & 2 \\
Vector B & 6 & 4 & 8 \\
\end{longtable}

\end{table}

\begin{figure}

\caption{\label{fig-distmeasures}Two Simple Distance Measures}

\centering{

\includegraphics{embeddings_paper_files/figure-pdf/fig-distmeasures-1.pdf}

}

\end{figure}%

The cosine is convenient because it is always between -1 and 1: When the
two vectors are pointing in a similar direction, the cosine is close to
1, and when they are pointing in a near-opposite direction (180°), the
cosine is close to -1.

Looking at Figure~\ref{fig-distmeasures}, a question arises: Why should
the angle be fixed at the zero point? What does the zero point have to
do with anything? Indeed, cosine similarity works best when the vector
space is centered at zero (or close to it). In other words, it works
best when zero represents a medium level of each variable. This fact is
sometimes taken for granted because, in practice, many vector spaces are
already centered at zero. In the case of neural word embeddings, as we
shall see, this property is guaranteed by the model's architecture. In
the case of transformer neural networks however, the use of specialized
models may be necessary to guarantee zero-centered embedding spaces.

The formula for calculating cosine similarity is as follows:

\[
Cosine(A,B) = \frac{A \cdot B}{|A||B|} = \frac{\sum _{i=1}^{n}  A_{i}B_{i}}{\sqrt {\sum _{i=1}^{n} A_{i}^2} \cdot \sqrt {\sum _{i=1}^{n} B_{i}^2}}
\]

In R, this can be translated to the following code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  {-} \textasciigrave{}x\textasciigrave{}: a numeric vector}
\CommentTok{\#  {-} \textasciigrave{}y\textasciigrave{}: another numeric vector}
\NormalTok{cos\_sim }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y)\{}
\NormalTok{  dot }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ y}
\NormalTok{  normx }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{  normy }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(y}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
  \FunctionTok{as.vector}\NormalTok{( dot }\SpecialCharTok{/}\NormalTok{ (normx}\SpecialCharTok{*}\NormalTok{normy) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Readers comfortable with cosines may be satisfied with the explanation
we have given so far. Nevertheless, many psychologists might find it
helpful to consider the relationship between cosine similarity and a
more familiar statistic that ranges between -1 and 1: the Pearson
correlation coefficient. Cosine similarity measures the similarity
between two \emph{vectors}, while the correlation coefficient measures
the similarity between two \emph{variables}. Now just imagine our
vectors as variables, with each dimension as an observation, as shown in
Figure~\ref{fig-cosineintuition} A.

\begin{figure}

\caption{\label{fig-cosineintuition}Vectors as Variables}

\centering{

\includegraphics{embeddings_paper_files/figure-pdf/fig-cosineintuition-1.pdf}

}

\end{figure}%

Now imagine centering those variables at zero, as shown in
Figure~\ref{fig-cosineintuition} B. When seen like this, the correlation
is the same as the cosine similarity. In other words, the correlation
between two vectors is the same as the cosine similarity between them
when the values of each vector are centered at zero.\footnote{For a
  mathematical presentation of this relationship, see O'Connor (2012)}
Seeing cosine similarity as the non-centered version of correlation
might provide additional intuition for why cosine similarity works best
for vector spaces that are centered at zero.

\subsection{The Distributional
Hypothesis}\label{the-distributional-hypothesis}

How do language models embed tokens in a semantic embedding space? By
what metric is a word considered similar to or different from another?
In answering this question, modern techniques rely on a further
assumption, known as the distributional hypothesis. According to this
assumption, tokens carry similar meanings inasmuch as they occur in
similar contexts. For example, consider the following two sentences from
the paper that introduced the distributional hypothesis, Harris (1954,
emphasis added).

\begin{quote}
``The formation of new \emph{utterances} in the \emph{language} is
therefore based on the distributional relations as changeably perceived
by the \emph{speakers}-among the parts of the previously heard
\emph{utterances}.''
\end{quote}

\begin{quote}
``The correlation between \emph{language} and \emph{meaning} is much
greater when we consider connected discourse.''
\end{quote}

Even if we have no idea what ``utterances'' or ``meaning'' are, we can
learn from these sentences that they must be related somehow, since they
both appear together with the word ``language.'' The more sentences we
observe, the more sure we can be about the distributional patterns
(i.e.~which words tend to have similar words nearby). Words that tend to
have very similar words nearby are likely to be similar in meaning,
while words that have very different contexts are probably unrelated.
Algorithms that learn the meanings of tokens (or at least the relations
between their meanings) from these patterns of co-occurrence are called
Distributional Semantic Models (DSMs).

\paragraph{A Common Misconception.}\label{a-common-misconception}

Two words are NOT considered similar based on whether they appear
together often. Words are similar when they tend to appear in similar
\emph{contexts}. For example, ``fridge'' and ``refrigerator'' almost
never appear together in the same sentence, but they do tend to appear
next to similar groupings of other words (e.g.~``food,'' ``cold,''
etc.).

\subsection{Word Embeddings: word2vec, GloVe, and
FastText}\label{word-embeddings-word2vec-glove-and-fasttext}

word2vec was first introduced by Mikolov, Chen, et al. (2013) and was
refined by Mikolov, Sutskever, et al. (2013). They proposed a few
variations on a simple neural network\footnote{Some experts consider
  word2vec too simple to be called a neural network. The precise
  definition of ``neural network'' is beyond the scope of this guide;
  word2vec could just as productively be thought of as an advanced form
  of logistic regression.} that learns the relationships between words
and contexts. Here we describe the most commonly used
variation---continuous skip-gram with negative sampling.

Imagine training the model on the following sentence: ``Coding is
frustrating.'' The skip-gram training dataset would have one column for
the input word, and another column for words from its immediate context.
The technique is called ``continuous'' because it slides a context
window along the training text, considering each word as input and the
words immediately around it as context (e.g.~10 before and 10 after), as
shown in Table~\ref{tbl-skipgram1}.

\begin{table}

{\caption{{Example Skipgram Training Set}{\label{tbl-skipgram1}}}
\vspace{-20pt}}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
word & context \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
coding & is \\
coding & frustrating \\
is & coding \\
is & frustrating \\
frustrating & coding \\
frustrating & is \\
\end{longtable}

\end{table}

The negative sampling method adds more rows to the training set, this
time from words and contexts that do not go together, drawn at random
from other parts of the corpus. A third column indicates whether the
pair of words are in fact neighbors or not, as shown in
Table~\ref{tbl-skipgram2}.

\begin{table}

{\caption{{Example Skipgram Training Set With Negative
Sampling}{\label{tbl-skipgram2}}}
\vspace{-20pt}}

\begin{longtable}[]{@{}llr@{}}
\toprule\noalign{}
word & context & neighbors \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
coding & is & 1 \\
coding & frustrating & 1 \\
is & coding & 1 \\
is & frustrating & 1 \\
coding & happy & 1 \\
coding & olive & 0 \\
coding & jump & 0 \\
is & happy & 0 \\
is & olive & 0 \\
is & jump & 0 \\
\end{longtable}

\end{table}

The word2vec model takes the first two columns as input and tries to
predict whether the two words are neighbors or not
see~\ref{tbl-skipgram2}. It does this by learning two separate sets of
embeddings: word embeddings and context embeddings. For each row of the
training set, the model looks up the embedding for the target word and
the embedding for the context word, and computes the dot product between
the two vectors. The dot product is closely related to the cosine
similarity---it measures how similar the two embeddings are. If the dot
product is large (i.e.~the word embedding and the context embedding are
very similar), the model predicts that the two words are likely to be
real neighbors. If the dot product is small, the model predicts that the
two words were probably sampled at random.\footnote{To learn why models
  like word2vec use dot products instead of cosine similarity, see
  Section~\ref{sec-embedding-magnitude} below.} During training, the
model learns which word embeddings and context embeddings will do best
at this binary prediction task.

Note that word2vec (as well as fastText and GloVe) give each word two
embeddings: one for when the word is the target and another for when it
is the context (Goldberg \& Levy, 2014). This overcomes two important
challenges of semantic embedding:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{A Nuance of the Distributional Hypothesis.} Recall the case of
  ``fridge'' and ``refrigerator'', which almost never appear together in
  the same sentence, but do tend to appear next to similar groupings of
  other words. Earlier word embedding models such as latent semantic
  analysis (Deerwester et al., 1990), which are based directly on broad
  patterns of covariance in word frequencies, will pick up on the fact
  that ``fridge'' and ``refrigerator'' are negatively correlated and
  push them further apart than they should be. Word2vec, on the other
  hand, can learn a \emph{context embedding} for ``refrigerator'' that
  is not so close to the \emph{word embedding} for ``fridge'', even when
  the word embeddings of the two words are very close. This allows
  word2vec to recognize that ``refrigerator'' and ``fridge'' tend to
  appear in similar contexts, but are unlikely to appear together.
\item
  \textbf{Associative Asymmetry.} The cosine similarity between two word
  embeddings gives the best estimate of \emph{conceptual similarity}
  (Torabi Asr et al., 2018). This is because conceptual similarity is
  not the same as association in language (or in the mind). In fact,
  psycholinguists have long known that human associations between two
  words are asymmetric. For example, people prompted with ``leopard''
  are much more likely to think of ``tiger'' than people prompted with
  ``tiger'' are to think of ``leopard'' (Tversky \& Gati, 1982). These
  sorts of associative connections are closely tied to probabilities of
  co-occurrence in language and are therefore much better represented by
  the cosine similarity (or even the dot product) between a word
  embedding and a context embedding (Torabi Asr et al., 2018). Thus the
  association between ``leopard'' and ``tiger'' would be represented by
  the similarity between the \emph{word embedding} of ``leopard'' and
  the \emph{context embedding} of ``tiger'', allowing for the asymmetry
  observed in mental associations.\footnote{To the best of our
    knowledge, pretrained context embeddings are not available online.
    For associative (rather than conceptual) relationships between
    words, we recommend training a new model. A guide on training a
    custom GloVe model in R can be found at
    https://quanteda.io/articles/pkgdown/replication/text2vec.html.}
  Since older models like latent semantic analysis only produce one
  embedding per word, they cannot capture this asymmetry.
\end{enumerate}

Some pretrained word2vec models can be easily downloaded from the
Internet\footnote{See for example
  https://github.com/maxoodf/word2vec?tab=readme-ov-file\#basic-usage
  and
  https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained}.
Because these models are trained on very large datasets and are already
known to perform well, it not necessary to train a new word2vec from
scratch.\footnote{Training custom word embeddings may be useful in
  certain specialized cases. For example, a researcher who is interested
  in quantifying differences in individual word use between multiple
  large groups of text might train a GloVe model on texts written by
  conservatives and another on texts written by liberals, and
  demonstrate that the word ``skirt'' is closer to the word ``woman'' in
  conservative language than it is in liberal language. A guide on
  training custom GloVe models in R can be found at
  https://quanteda.io/articles/pkgdown/replication/text2vec.html.}

A downloaded pretrained model (generally a .bin file), can be opened in
R with the word2vec package.\footnote{Available at
  https://cran.r-project.org/web/packages/word2vec/readme/README.html}
In the example code below, we use a model trained on Google news, which
uses 300-dimensional embeddings.\footnote{Available at
  https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained/download?datasetVersionNumber=1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(word2vec)}

\CommentTok{\# model file path}
\NormalTok{word2vec\_mod }\OtherTok{\textless{}{-}} \StringTok{"data/GoogleNews{-}vectors{-}negative300.bin"}

\CommentTok{\# open model}
\NormalTok{word2vec\_mod }\OtherTok{\textless{}{-}} \FunctionTok{read.word2vec}\NormalTok{(}\AttributeTok{file =}\NormalTok{ word2vec\_mod, }\AttributeTok{normalize =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To find embeddings of specific words using the word2vec package, use
\texttt{predict(word2vec\_mod,\ c("word1",\ "word2"),\ type\ =\ "embedding")}.
To get embeddings for full documents, average the embeddings of the
words in the document. In our github repo, we provide the
\texttt{textstat\_embedding} to compute document embeddings directly
from a document-feature matrix (DFM).\footnote{https://github.com/rimonim/embeddings\_tutorial/blob/master/embedding\_scripts.R}

\paragraph{Advantages and Disadvantages of
Word2vec.}\label{advantages-and-disadvantages-of-word2vec}

By distinguishing between target and context words, word2vec allows
greater fidelity to the distributional hypothesis. Since it is not based
on counts, it also avoids problems with non-linear relationships and
irregular distributional properties of word frequencies (see Baayen,
2001). Word2vec, like other word embedding models, is also efficient for
use on large datasets, since pretrained embeddings can be looked up for
each word. The main disadvantage of word2vec is that it assumes that
each word has only one meaning. This means that it has trouble with
words that can mean more than one thing (e.g.~deep learning \emph{model}
vs.~fashion \emph{model}). Word2vec will learn the average of these
meanings. Furthermore, word2vec is primarily effective in English. This
is because English words are generally spelled the same no matter where
they are in a sentence. Word2vec does not work as well for languages
that have more prefixes, suffixes, conjugations, etc., since it has to
relearn the meaning for each form of the word. Finally, word2vec is made
less appealing to researchers due to the relative paucity of pretrained
models available online.

\subsubsection{GloVe}\label{glove}

Word2vec produces spectacularly rich and reliable vector embeddings, but
their reliance on randomly sampled pairs of words and contexts makes
them somewhat noisy and overly sensitive to frequent tokens. The
developers of word2vec managed to fix these problems by strategically
filtering the training dataset. Pennington et al. (2014), in contrast,
developed a refactored model that corrects for frequency internally:
Global Vectors (GloVe) is designed on the same principles as word2vec,
but it is computed from global patterns of co-occurrence rather than
individual examples.

Although GloVe uses a different method of training, the embeddings it
generates are very similar to those generated by word2vec. Because GloVe
embeddings are so similar to word2vec embeddings, we will not go into
detail here about the way the GloVe algorithm works. Nevertheless, GloVe
does have one very important advantage over word2vec: Better pretrained
models are available online. Whereas the most easily available word2vec
model is trained on news articles, the GloVe website\footnote{https://nlp.stanford.edu/projects/glove/}
offers models trained on social media (\texttt{glove.twitter.27B.zip})
and on large portions of the Internet (Rana, 2010). These models
generalize better to social media texts (since they were trained on
similar texts) and are likely to have richer representations of
emotional or social content, since more examples of that content appear
on social media than in the news or on Wikipedia.\footnote{Another
  notable difference between GloVe and word2vec is that the GLoVe
  averages the word embeddings and context embeddings rather than using
  only the word embeddings as word2vec does. This makes GloVe embeddings
  slightly better at representing overall meaning, but may blur the
  distinction between conceptual similarity and mental/linguistic
  association (Torabi Asr et al., 2018).}

Since the pretrained GloVe models are available in .txt format, a
wrapper package is not required to use them in R. The following code
loads a downloaded pretrained model for use.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{path\_to\_glove }\OtherTok{\textless{}{-}} \StringTok{"\textasciitilde{}/Projects/ds4psych/data/glove/glove.twitter.27B.100d.txt"}
\NormalTok{glove\_pretrained }\OtherTok{\textless{}{-}} \FunctionTok{load\_embeddings\_txt}\NormalTok{(path\_to\_glove)}
\end{Highlighting}
\end{Shaded}

\paragraph{Advantages and Disadvantages of
GloVe.}\label{advantages-and-disadvantages-of-glove}

GloVe's efficient training procedure has made it a popular choice for
researchers who wish to train their own models. GloVe is also appealing
for researchers looking for high quality, psychologically sensitive,
pretrained models, since a number of such models are available for
download online. Like word2vec, GloVe's primary downsides are that it
relies on word-level meaning and that it has limited applicability to
languages other than English.

\subsubsection{FastText}\label{fasttext}

FastText (Bojanowski et al., 2017) is a specialized version of word2vec,
designed to work with languages in which words take different forms
depending on their grammatical place. Rather than learning a word
embedding and a context embedding for each full word (e.g.~``quantify''
and ``quantification'' each get their own embedding), fastText learns a
vector for each string of characters within a word (such a string is
sometimes referred to as a shingle). For example, ``quantify'' might be
broken up into ``quant'', ``uanti'', ``antif'', and ``ntify''. But it
doesn't treat each shingle as its own word. Rather, it trains on words
just like word2vec and GloVe, but makes sure that the embedding of a
word is equal to the \emph{sum} of all of the shingle vectors inside it.

This approach is mostly unnecessary for English, where words are
generally spelled the same wherever they appear. But for more
morphologically rich languages like Hebrew, Arabic, French, or Finnish,
fastText works much better than word2vec and GloVe. This is because
there might not be enough data for word2vec and GloVe to learn reliable
representations of every form of every word, especially rare forms.
FastText, on the other hand, can focus on the important subcomponents of
the words that stay the same across different forms. This way it can
learn rich representations even of rare forms of a word that don't
appear in the training dataset (e.g.~it could quantify the meaning of
``morphologically'' even if it were only trained on ``morphology'',
``morpheme'', and ``chronologically'').

After downloading a pretrained model from Grave et al. (2018), you can
use fastText in R through the fastTextR package.\footnote{Available at
  https://cran.r-project.org/web/packages/fastTextR/vignettes/Word\_representations.html}
FastTextR includes a dedicated function for obtaining full text
embeddings, \texttt{ft\_sentence\_vectors()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fastTextR)}

\CommentTok{\# example Hebrew texts}
\NormalTok{heb\_words }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"מחשבותייך"}\NormalTok{, }\StringTok{"מחשבה"}\NormalTok{)}
\NormalTok{heb\_texts }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"הדבור מיחד את האדם מן החי, הדומם והצומח, הלשון היא – נפש חיה – רוח ממללה"}\NormalTok{, }\StringTok{"לשון היא המבדלת בין אומה אחת לחברתה, והיא החוט, שעליו נחרזות תמורות הנפש הרבות"}\NormalTok{)}

\CommentTok{\# load pretrained model from file}
\NormalTok{heb\_model }\OtherTok{\textless{}{-}} \FunctionTok{ft\_load}\NormalTok{(}\StringTok{"data/cc.he.300.bin"}\NormalTok{)}

\CommentTok{\# get word embeddings}
\NormalTok{word\_vecs }\OtherTok{\textless{}{-}} \FunctionTok{ft\_word\_vectors}\NormalTok{(heb\_model, heb\_words)}

\CommentTok{\# get text embeddings}
\NormalTok{text\_vecs }\OtherTok{\textless{}{-}} \FunctionTok{ft\_sentence\_vectors}\NormalTok{(heb\_model, heb\_texts)}
\end{Highlighting}
\end{Shaded}

\paragraph{Advantages and Disadvantages of
FastText.}\label{advantages-and-disadvantages-of-fasttext}

Relative to word2vec and GloVe, fastText performs much better on
morphologically rich languages. Pretrained models for 157 languages are
available online.\footnote{https://fasttext.cc/docs/en/crawl-vectors.html}
Furthermore, fastText has increased performance for rare words, and is
able to infer embeddings for words that were not in the training data.
The cost of this gain is that fastText models are more complex,
resulting in larger files to download when using pretrained models. The
added complexity may also increase the risk of overfitting.

\subsubsection{Interpreting Advanced Word
Embeddings}\label{sec-embedding-magnitude}

Advanced word embedding algorithms like word2vec, GloVe, and fastText
use the dot product of embeddings to measure how likely two words are to
appear together. The dot product is the same as cosine similarity,
except that it gets larger as the vectors get farther away from the
origin (i.e.~cosine similarity is the dot product of two normalized
vectors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dot\_prod }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y)\{}
\NormalTok{  dot }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ y}
  \FunctionTok{as.vector}\NormalTok{(dot)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\caption{\label{fig-dotprod}Dot Product vs.~Cosine Similarity}

\centering{

\includegraphics{embeddings_paper_files/figure-pdf/fig-dotprod-1.pdf}

}

\end{figure}%

Recall that in models like word2vec and GloVe, the dot product
corresponds to the probability that two words occur together. Vectors
that are farther away from the origin will result in very positive or
very negative dot products, making the model more confident in the pair
of words either being neighbors or not. This means that the distance of
a word embedding from the origin (also called the norm or magnitude) is
proportional to the informativeness of the word (Oyama et al., 2023;
Schakel \& Wilson, 2015). For example, the word ``the'' has a very low
magnitude because it does not indicate a specific context, while the
word ``psychology'' has a very high magnitude because its use is
associated with a very specific context. Therefore, the magnitude of the
embedding measures how representative it is of certain contexts as
opposed to others.

This is the reason why an accurate embedding of a full text can be
obtained by averaging the embeddings of each of its words. One may
assume that averaging word embeddings will lead to overvaluing common
words, like ``the'' and ``I'', which appear more frequently but are not
very informative about the text's meaning. In fact, however, the
magnitude of a word embedding is smaller for common words, which means
that common words have less impact on the average (Ethayarajh et al.,
2019).

Once average embeddings are computed, most researchers use cosine
similarity to assess the relationships between embeddings. The cosine
similarity measures only the meanings of the two embeddings, while
ignoring how specific they are to those meanings. If the specificity of
texts to a construct of interest is important to the analysis, the dot
product may be more appropriate than the cosine similarity. For this
reason, the dot product may sometimes be optimal for analyzing texts
with decontextualized embeddings (See Appendix A).

\subsection{Contextualization With Large Language
Models}\label{contextualization-with-large-language-models}

The models we have discussed so far represent the meaning of each token
as a point in multidimensional space: a word embedding. Word embeddings
generated by models like word2vec or GloVe are often referred to as
\textbf{decontextualized embeddings}. This name may be confusing, since
the purpose of these models is to associate tokens with the contexts in
which they tend to appear. Nevertheless, these models are
decontextualized in that they can allow only one context representation
per token---they therefore represent the average of the contexts in
which a token appears throughout the training corpus. For example,
consider the following uses of the token ``short''.

\begin{quote}
My dad is very \emph{short}.
\end{quote}

\begin{quote}
My blender broke because of a \emph{short} circuit.
\end{quote}

\begin{quote}
That video was anything but \emph{short}.
\end{quote}

\begin{quote}
I can't pay because I'm \emph{short} on cash at the moment.
\end{quote}

Any speaker of English can easily see that the word ``short'' means
something different in each one of these examples. But because word2vec
and similar models are trained to predict the context based on only a
single word at a time, their representation of the word \emph{short}
will only capture that word's average meaning.

How can we move beyond the average meaning and capture the different
meanings words take on in different contexts? Readers may be familiar
with Large Language Models (LLMs) like ChatGPT and Claude. At their
core, much of what these models do is exactly this: They find the
intricate relationships between tokens in a text and use them to develop
a new understanding of what these tokens mean in the particular context
of that text. For this reason, embeddings produced by these models are
often referred to as \textbf{contextualized embeddings}.

The core of all modern LLMs is a model called the \emph{transformer}. We
will not cover exactly how transformers work. For the purposes of this
guide, we offer only the broad overview necessary to make proper use of
them in research: In processing a text, transformers first convert all
the words into word embeddings, as word2vec or GloVe would do. At the
start, these word embeddings represent the average meaning of each word.
The transformer then estimates how each word in the text might be
relevant for better understanding the meaning of the other words. For
example, if ``circuit'' appears immediately following ``short'', the
embedding of ``short'' should probably be tweaked. Once it has
identified this connection, the transformer computes what ``circuit''
should add to a word that it is associated with, moving the ``short''
embedding closer to embeddings for electrical concepts. A full LLM has
many \emph{layers}. In each layer, the LLM identifies more connections
between embeddings and shifts the embeddings in the vector space to add
more nuance to their representations. When it gets to the final layer,
the LLM uses the enriched embeddings of the words in the text for
whatever task it was trained to do (e.g.~predicting what the next word
will be, or identifying whether the text is spam or not).

In order to extract an LLM's rich, contextualized embeddings,
researchers can run it on a text and stop it before it finishes
predicting the next word (or anything else it was trained to do). This
way, we can read the LLM's mind, capturing all of the rich associations
it has with the text.

\subsubsection{Hugging Face and the text
Package}\label{hugging-face-and-the-text-package}

Leading commercial LLMs like GPT-4 are hidden behind APIs so that their
inner workings are kept secret. Researchers therefore cannot access the
embeddings of these high profile models. Nevertheless, plenty of models
that are almost as good are open source and easily accessible through
Hugging Face Transformers.\footnote{https://huggingface.co/docs/transformers/index}
Any text-based transformer model can be accessed in R using the
\texttt{text} package (Kjell et al., 2021).\footnote{The \texttt{text}
  package runs Python code behind the scenes, so new users will have to
  set up a Python environment for it to run properly. For instructions
  on how to do this, see
  https://www.r-text.org/articles/huggingface\_in\_r\_extended\_installation\_guide.html.}

The key tool provided by the \texttt{text} package is
\texttt{textEmbed()}. This function takes in raw texts and generates
contextualized embeddings using a specified open source model. Here we
use \texttt{BAAI/bge-base-en-v1.5} (Xiao et al., 2023), a relatively
lightweight model that nonetheless achieves state-of-the-art performance
on the MTEB benchmark, a leading resource for evaluating text embedding
models (Muennighoff et al., 2022).\footnote{The current MTEB leaderboard
  can be found at https://huggingface.co/spaces/mteb/leaderboard.} By
default, \texttt{textEmbed()} creates the full text embedding by
averaging the contextualized embeddings of each token in the text.
However, \texttt{BAAI/bge-base-en-v1.5} is trained to provide optimal
text embeddings in the final layer embedding of the \texttt{{[}CLS{]}}
token in particular.\footnote{The \texttt{{[}CLS{]}} token is a special
  token that models based on BERT (Devlin et al., 2019) add to each
  text. Because the \texttt{{[}CLS{]}} token does not have a ``real''
  meaning, but rather is inserted at the same place in every text, its
  contextualized embedding represents the gist of each text as a whole.
  In training, traditional BERT models use the contextualized embedding
  of the \texttt{{[}CLS{]}} token to predict whether a given text does
  or does not come after the input text. \texttt{BAAI/bge-base-en-v1.5}
  has a somewhat different training paradigm, but it maintains the
  conventional \texttt{{[}CLS{]}} token.} When using
\texttt{BAAI/bge-base-en-v1.5} to generate embeddings, it is therefore
necessary to specify \texttt{tokens\_select\ =\ "{[}CLS{]}"} and
\texttt{layers\ =\ -1}. This has the added benefit of speeding up the
computation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(text)}

\NormalTok{example\_texts }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \StringTok{"This is one text."}\NormalTok{,}
  \StringTok{"This is another."}
\NormalTok{  )}

\CommentTok{\# run model}
\NormalTok{example\_texts\_bge }\OtherTok{\textless{}{-}} \FunctionTok{textEmbed}\NormalTok{(}
\NormalTok{  example\_texts,}
  \AttributeTok{model =} \StringTok{"BAAI/bge{-}base{-}en{-}v1.5"}\NormalTok{, }\CommentTok{\# model name}
  \AttributeTok{tokens\_select =} \StringTok{"[CLS]"}\NormalTok{, }\CommentTok{\# select only [CLS] token embedding}
  \AttributeTok{layers =} \SpecialCharTok{{-}}\DecValTok{1}  \CommentTok{\# last layer}
\NormalTok{)}

\CommentTok{\# extract dataframe of embeddings}
\NormalTok{example\_texts\_bge }\OtherTok{\textless{}{-}}\NormalTok{ example\_texts\_bge}\SpecialCharTok{$}\NormalTok{texts[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Managing Computational
Load}\label{managing-computational-load}

Running large neural networks on a personal computer can be time
consuming. The following are some ways to lessen the computational load
when calling \texttt{textEmbed()}:

\begin{itemize}
\tightlist
\item
  Use a smaller model. Hugging Face model pages generally state how many
  parameters the model has. This is a good indication of how much
  computational capacity the model needs. For example, consider using
  \texttt{distilbert-base-uncased} (67M params) or
  \texttt{albert-base-v2} (11.8M params) instead of
  \texttt{bert-base-uncased} (110M params).
\item
  Only retrieve one layer at a time (e.g.~\texttt{layers\ =\ -2}).
\item
  If individual token embeddings are not needed, set
  \texttt{keep\_token\_embeddings\ =\ FALSE}.
\item
  To avoid aggregation costs, only querry individual token embeddings
  (e.g.~\texttt{tokens\_select\ =\ "{[}CLS{]}"}).
\item
  Run the model on the GPU with
  \texttt{device\ =\ \textquotesingle{}gpu\textquotesingle{}} (not
  available for Apple M1 and M2 chips).
\item
  Break up the dataset into smaller groups of texts, run each on its
  own, and join them back together afterward.
\end{itemize}

Before running \texttt{textEmbed()} on a full dataset, always try
running it on two or three texts first. This will give a sense of how
long the full computation will take, and will ensure that the output is
as expected.

\subsubsection{Choosing the Right Model}\label{choosing-the-right-model}

New LLMs are published on Hugging Face every day, and choosing one can
be daunting. Part of the beauty of LLMs is their ability to
generalize---most popular models today are trained on enormous datasets
with a wide variety of content, and even smaller models perform more
than well enough to capture straightforward psychological concepts like
emotional valence (Kjell et al., 2022). Nevertheless, it is important to
note one recent development: the popularization of models that are
trained explicitly to produce high quality embeddings. Such models have
made significant progress in solving a major limitation of LLM
embeddings. Word2vec, GloVe, and related models have architectures that
guarantee vector spaces with consistent geometric properties, allowing
researchers to confidently compute averages between vectors and
interpret linear directions as encoding unique semantic meanings. In
contrast, the ways that LLMs organize meaning in their embedding spaces
are not well understood and may not always lend themselves to simple
measures like those described in this guide {[}Cai et al. (2021); Li et
al. (2020); Reif et al. (2019); zhou\_etal\_2022{]}. Recently,
researchers have tried to fix this problem by creating models that
encourage embeddings to spread out evenly in the embedding space, or
that explicitly optimize for reliable cosine similarity metrics. Many of
these models can be found on the leaderboard for the MTEB benchmark
(Muennighoff et al., 2022).\footnote{The current MTEB leaderboard can be
  found at https://huggingface.co/spaces/mteb/leaderboard.}

\paragraph{Advantages and Disadvantages of Contextualized
Embeddings.}\label{advantages-and-disadvantages-of-contextualized-embeddings}

Contextualized embeddings can represent multiple senses of each word.
This is especially important for analyzing shorter texts---since the
sample of words is small, the need to correctly characterize each one is
greater. Contextualized embeddings are also sensitive to word order and
negation (e.g.~LLMs can tell the difference between ``he's funny but not
smart'' and ``he's smart but not funny''). Despite these advantages, LLM
embeddings come with two downsides. First, they are computationally
expensive to generate, making them unfeasible for application to large
datasets. Second, unlike simple word embedding models, LLMs may
sometimes organize embeddings in nonlinear patterns. For example, GPT
models tend to arrange their embeddings in a spiral (Cai et al., 2021).
This is presumably why BERT text embeddings perform worse than word2vec
and GloVe when analyzed using cosine similarity (Reimers \& Gurevych,
2019). Specialized models such as those listed on the leaderboard for
the MTEB benchmark (Muennighoff et al., 2022) can help with this
problem, but are unlikely to solve it entirely.

\subsection{Representing Psychological
Constructs}\label{representing-psychological-constructs}

With modern vector-based methods, any language-based psychological
measure can be represented as a vector. Psychology has used
language-based measures like dictionaries and questionnaires for over a
century. To smoothly continue this existing research in the age of
vector spaces, we introduce three methods for quantifying existing
psychological constructs in high-dimensional vector space: Distributed
Dictionary Representation (DDR), Contextualized Construct Representation
(CCR), and Correlational Anchored Vectors (CAV). As we introduce each
method, we use it to quantify a psychological construct: either
self-referential processing or the related concept of locus of
control---the feeling that one has control over one's own life (Rotter,
1966). Finally, we use each metric to test for significant differences
between posts in the Reddit r/depression community (N = 995) and posts
in r/TodayIamHappy community (N = 781). As their names suggest, the
former is a support group for users struggling with depression, while
the latter is a forum for sharing happy experiences. This small sample
of posts, which we name \texttt{reddit\_emotion}, was collected from the
Pushshift Reddit archives (Baumgartner et al., 2020) and is used here
purely for the sake of demonstration.

\subsubsection{Distributed Dictionary Representation
(DDR)}\label{distributed-dictionary-representation-ddr}

We begin with a straightforward sort of psychological measure---the
dictionary (also known as a word list or lexicon). A dictionary is a
list of words (or other units of text, called generally ``tokens'')
associated with a given psychological or other construct. For example, a
dictionary for depression might include words like ``sleepy'' and
``down.'' Psychology researchers have been constructing, validating, and
publicizing dictionaries for decades. But these dictionaries are
generally designed to be used by counting dictionary words in a
text---How do we apply them to a vector-based analysis? Garten et al.
(2018) propose a simple solution: Generate word embeddings for each word
in the dictionary, and average them together to create a single
Distributed Dictionary Representation (DDR). The dictionary construct
can then be measured by comparing text embeddings to the DDR.

DDR cannot entirely replace word counts; for linguistic concepts like
conjunctions or use of the passive voice, word counts may still be
psychometrically optimal. But DDR is ideal for studies of abstract
constructs like emotions, that refer to the general gist of a text
rather than particular words. The rich representation of word embeddings
allows DDR to capture even the subtlest associations between words and
constructs, and to precisely reflect the \emph{extent} to which each
word is associated with each construct. It can do this even for texts
that do not contain any dictionary words. Because embeddings are
continuous and already calibrated to the probabilities of word use in
language, DDR also avoids the difficult statistical problems that arise
due to the strange distributions of word counts (see Teitelbaum \&
Simchon, 2024, Chapter 16).

Garten et al. (2018) found that DDR works best with smaller dictionaries
of only the words most directly connected to the construct being
measured (around 30 words worked best in their experiments). Word
embeddings work by overvaluing informative words---a desirable property
for raw texts, in which uninformative words tend to be very
frequent.\footnote{For more information on this property, see
  ``Interpreting Advanced Word Embeddings'' above. Note that this
  property emerges naturally from the way decontextualized models like
  word2vec and GloVe are trained, and therefore may not hold true for
  contextualized embeddings.} But dictionaries only include one of each
word. In longer dictionaries with more infrequent, tangentially
connected words, averaging word embeddings will therefore overvalue
those infrequent words and skew the DDR. This can be fixed with Garten
et al.'s method of picking out only the most informative words.
Alternatively, it could be fixed by measuring the frequency of each
dictionary word in a corpus and weighting the average embedding by that
frequency. This method is actually more consistent with the way most
dictionaries are validated, by counting the frequencies of dictionary
words in text (for preliminary empirical validation of this method, see
Appendix A).

In the example code below, we create a DDR for self-referential
processing using the Linguistic Inquiry and Word Count (LIWC) dictionary
of first person singular pronouns (Boyd et al., 2022). Since LIWC
dictionaries include stems in addition to full words, we first expand
the dictionary by running the LIWC software on each unique word in
\texttt{reddit\_emotion} (full code is available in our Github repo).
This results in a character vector of dictionary words, called
\texttt{i\_dict} in the code below. To correct for word informativeness,
we weight the dictionary word embeddings by their frequency in
\texttt{reddit\_emotion}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Embed Dictionary Words}
\NormalTok{i\_dict\_glove }\OtherTok{\textless{}{-}}\NormalTok{ i\_dict }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tokens}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{dfm}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{textstat\_embedding}\NormalTok{(glove\_pretrained) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{doc\_id) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as.matrix}\NormalTok{()}
\FunctionTok{rownames}\NormalTok{(i\_dict\_glove) }\OtherTok{\textless{}{-}}\NormalTok{ i\_dict}
  
\CommentTok{\# Get Dictionary Word Frequencies}
\NormalTok{i\_dict\_freqs }\OtherTok{\textless{}{-}}\NormalTok{ reddit\_emotion\_dfm }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{dfm\_keep}\NormalTok{(i\_dict) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  quanteda.textstats}\SpecialCharTok{::}\FunctionTok{textstat\_frequency}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(frequency, }\AttributeTok{name =}\NormalTok{ feature)}
  
\CommentTok{\# Aggregate dictionary embeddings with weighted average}
\NormalTok{i\_DDR }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(i\_dict\_glove, }\DecValTok{2}\NormalTok{, weighted.mean, }\AttributeTok{w =}\NormalTok{ i\_dict\_freqs)}
\end{Highlighting}
\end{Shaded}

To measure the similarity of Reddit posts to this theoretical construct,
we now need only to compute the cosine similarity between the DDR and
the embedding of each text. Beginning with a DFM of
\texttt{reddit\_emotion}, the following code performs this computation
and conducts a t-test for differences between r/depression and
r/TodayIamHappy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Embed Texts of Interest}
\NormalTok{reddit\_emotion\_glove }\OtherTok{\textless{}{-}}\NormalTok{ reddit\_emotion\_dfm }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{textstat\_embedding}\NormalTok{(glove\_pretrained) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{doc\_id)}
  
\CommentTok{\# Calculate Distance Metrics}
\NormalTok{reddit\_emotion\_scores }\OtherTok{\textless{}{-}}\NormalTok{ reddit\_emotion\_glove }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{selfref =} \FunctionTok{cos\_sim}\NormalTok{(}\FunctionTok{c\_across}\NormalTok{(V1}\SpecialCharTok{:}\NormalTok{V100), i\_DDR)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(selfref)}

\CommentTok{\# Test Hypothesis}
\NormalTok{test\_DDR }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ reddit\_emotion\_scores[reddit\_emotion}\SpecialCharTok{$}\NormalTok{subreddit}\SpecialCharTok{==}\StringTok{"depression"}\NormalTok{],}
  \AttributeTok{y =}\NormalTok{ reddit\_emotion\_scores[reddit\_emotion}\SpecialCharTok{$}\NormalTok{subreddit}\SpecialCharTok{==}\StringTok{"TodayIamHappy"}\NormalTok{]}
\NormalTok{  )}
\NormalTok{test\_DDR}\SpecialCharTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.453378e-07
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_DDR}\SpecialCharTok{$}\NormalTok{estimate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean of x mean of y 
0.9266877 0.9195364 
\end{verbatim}

We find that posts in r/depression have more self-referential language
(p \textless{} .001). This is consistent with existing research
indicating that the use of self-referential language increases during
depressive episodes (Rude et al., 2004).

\paragraph{Advantages and Disadvantages of
DDR.}\label{advantages-and-disadvantages-of-ddr}

DDR produces richer, more robust construct representation than
traditional word counting methods (cf. Boyd et al., 2022). Since DDR
representations are continuous, they also avoid statistical problems
associated with the distribution of word counts (see Baayen, 2001).
Another advantage of DDR over dictionary-based word counts is that DDR
enables word-by-word analysis of text. It is not very informative to
count how many dictionary words are in each word (it will either be one
or zero), but the embedding of each word can be compared to the
DDR---how close are they in the vector space? This allows researchers to
see how a construct spreads out within a single text. Relatedly, DDR
only needs a dictionary that captures the essence of the construct being
measured. For many constructs, this could be only a few words. This
property can allow researchers to create rich construct representations
for novel constructs without investing in the development of a large
dictionary. One downside of DDR is that it can implicitly encode
associated constructs. For example, if surprised texts tend to have
positive valence in the data used to train the word embedding model, the
DDR for surprise may embed some positive valence as well. This can be
remedied by constructing a DDR for positive valence too, and using it as
a statistical control when testing hypotheses. Lastly, DDR may not be
appropriate for linguistic measures. Word embeddings encode the general
gist of a text, whereas constructs like passive voice or pronoun use
refer to specific words. Constructs that are concretely tied to words or
grammatical forms in the text should therefore be analyzed using
standard word counting methods (see Boyd et al., 2022).

\subsubsection{Contextualized Construct Representation
(CCR)}\label{contextualized-construct-representation-ccr}

Dictionaries are not the only validated psychological measures that we
can apply using embeddings. With contextualized embeddings, we can
extract the gist of any text and compare it to that of any other text.
Atari et al. (2023) propose to do this with the most popular form of
psychometric scale: the questionnaire. Psychologists have been using
questionnaires to measure things for over a century, and tens of
thousands of validated questionnaires are now available online. The LLM
embedding of a questionnaire is referred to as a Contextualized
Construct Representation (CCR).

We can use CCR to measure internal locus of control by using the
questionnaire introduced by Rotter (1966). The questionnaire includes
items measuring an internal locus of control (e.g.~``What happens to me
is my own doing''), and items measuring an external locus of control
(e.g.~``Sometimes I feel that I don't have enough control over the
direction my life is taking''). In the code below, we compute a CCR for
the internal locus of control items.\footnote{Many questionnaires
  include reverse-coded items (e.g.~``I often feel happy'' on a
  depression questionnaire). The easiest way to deal with these is to
  manually add negations to flip their meaning (e.g.~``I \emph{do not}
  often feel happy'').}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Embed Questionnaire Items}
\NormalTok{internal\_bge }\OtherTok{\textless{}{-}}\NormalTok{ text}\SpecialCharTok{::}\FunctionTok{textEmbed}\NormalTok{(}
\NormalTok{  internal\_items,}
  \AttributeTok{model =} \StringTok{"BAAI/bge{-}base{-}en{-}v1.5"}\NormalTok{, }\CommentTok{\# model name}
  \AttributeTok{tokens\_select =} \StringTok{"[CLS]"}\NormalTok{, }\CommentTok{\# select only [CLS] token embedding}
  \AttributeTok{layers =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,  }\CommentTok{\# last layer}
  \AttributeTok{dim\_name =} \ConstantTok{FALSE}\NormalTok{,}
  \AttributeTok{keep\_token\_embeddings =} \ConstantTok{FALSE}
\NormalTok{)}
\NormalTok{internal\_bge }\OtherTok{\textless{}{-}}\NormalTok{ internal\_bge}\SpecialCharTok{$}\NormalTok{texts[[}\DecValTok{1}\NormalTok{]]}
  
\CommentTok{\# Aggregate Questionnaire Embeddings}
\NormalTok{internal\_bge }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(internal\_bge), }\DecValTok{2}\NormalTok{, mean)}
\end{Highlighting}
\end{Shaded}

The ubiquity of questionnaire measures in psychology research makes CCR
a powerful tool. Nevertheless, researchers should keep in mind what is
being represented by the CCR. Embeddings capture the overall feel of a
text, including its tone and dialect. With CCR, we are comparing the
feel of a questionnaire written by academics to the feel of posts
written by Reddit users. By comparing these vectors, we are not just
measuring how much self-referential language is in each text---we are
also measuring the extent to which each text is in the style of a
questionnaire written by academics. This has the potential to introduce
a confounding variable into the analysis---questionnaire-ness.

The questionnaire-ness problem suggests that CCR is most effective for
analyzing texts that bear a strong similarity to the questionnaire
itself. For example, the texts of interest may be participant
descriptions of their own values, while the questionnaire items are
statements about values in the first person (as is commonly the case).
In cases like this, CCR is likely to perform optimally. With this
method, researchers can compare participant responses to the
questionnaire without actually administering the questionnaire itself;
participants can answer in their own words, which CCR will compare to
the wording of the questionnaire (for preliminary empirical validation
of this recommendation, see Appendix B).

\paragraph{Advantages and Disadvantages of
CCR.}\label{advantages-and-disadvantages-of-ccr}

CCR is a powerful tool because it can apply existing questionnaires---a
fundamental tool in psychological research---to modern automated text
analysis. In this sense it makes effective use of contextualized
embeddings provided by state of the art LLMs. CCR thus allows
psychologists to efficiently implement questionnaire-based research
designs that nevertheless allow participants to compose responses in
their own words. One limitation of CCR is its limited ability to
generalize---it is thus contraindicated for texts that do not contain
similar content and wording to the embedded questionnaire. Nevertheless
the risk of questionnaire-ness described above can be mitigated by
constructing an anchored vector, discussed below.

\subsection{Reasoning in Vector Space: Additive Analogies and Anchored
Vectors}\label{reasoning-in-vector-space-additive-analogies-and-anchored-vectors}

An oft-touted property of word embeddings is that they can be added to
each other in order to arrive at new concepts. A prototypical example is
shown in Figure~\ref{fig-parallelogram}. Subtracting the embedding of
``man'' from the embedding of ``woman'' results in the vector shown in
blue. This vector represents the move from male to female gender. A
vector between two embeddings is called an \textbf{anchored vector}. So
when the man-woman anchored vector is added to the embedding of
``aunt'', the result is very close to the embedding of ``uncle''. This
property was first noted in word2vec (Mikolov, Yih, et al., 2013), and
GloVe (Pennington et al., 2014) was specifically designed with it in
mind.

\begin{figure}[!htbp]

{\caption{{An Additive Analogy in Word
Embeddings}{\label{fig-parallelogram}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-parallelogram-1.pdf}

{\noindent \emph{Note.} Word embeddings were acquired from the
glove.twitter.27B.100d pretrained model and reduced to two dimensions
with Principle Component Analysis (PCA).}

\end{figure}

\paragraph{Additive Analogies in Contextualized
Embeddings.}\label{additive-analogies-in-contextualized-embeddings}

Note in Figure~\ref{fig-parallelogram} that the analogical property
relies on the magnitude of the vectors---if some vectors were shorter or
longer than necessary, the parallelogram would not fit. This means that
analogical reasoning may not be applicable to LLM embeddings, which are
often organized in nonlinear patterns (Cai et al., 2021; Ethayarajh,
2019; Gao et al., 2019). Even specialized models like sentence
transformers are generally not designed with the additive analogical
property in mind (Reimers \& Gurevych, 2019). Even though the
geometrically motivated methods described in this guide work fairly well
in LLM embeddings, there is lots of room for improvement in this
area.\footnote{There are some promising methods for getting more
  geometrically regular embeddings out of non-specialized LLMs. For
  example, averaging the last two layers of the model seems to help (Li
  et al., 2020). Taking a different approach, Ethayarajh (2019) created
  static word embeddings from an LLM by running it on a large corpus and
  taking the set of each word's contextualized representations from all
  the places it appears in the corpus. The loadings of the first
  principal component of this set represent the dimensions along which
  the meaning of the word changes across different contexts. These
  loadings can themselves be used as a vector embedding which can
  out-perform GloVe and FastText embeddings on many word vector
  benchmarks, including analogy solving. This approach worked best for
  embeddings from the early layers of the LLM.}

Additive analogical reasoning can help overcome a fundamental challenge
of semantic embeddings, a more general form of the questionnaire-ness
problem discussed above in the context of CCR. Consider the embeddings
for ``happy'' and ``sad''. These may seem like opposites, but actually
they are likely to be very close to each other in vector space because
they both relate to emotional valence. This means that if we try to
measure the happiness of words by comparing their embeddings to the
embedding for ``happy'', we will actually be measuring the extent to
which the words relate to emotion in general. The word ``depression''
might seem happier than the word ``table'', since depression is more
emotion-related. This problem can be solved by using \textbf{anchored
vectors}. Just as we created an anchored vector between ``man'' and
``woman'' to represent masculinity (as opposed to femininity) in
Figure~\ref{fig-parallelogram}, we can create an anchored vector between
``happy'' and ``sad'' to represent happiness (as opposed to sadness).
Such anchored vectors can be applied wherever necessary in embedding
space.

In the code below, we compute an anchored vector for locus of control by
subtracting the CCR of the internal locus of control items from that of
the external locus of control items. This anchored vector then measures
internal \emph{as opposed to external} locus of control.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Anchored vector}
\NormalTok{loc\_anchored }\OtherTok{\textless{}{-}}\NormalTok{ internal\_bge }\SpecialCharTok{{-}}\NormalTok{ external\_bge}
\end{Highlighting}
\end{Shaded}

To measure a construct with an anchored vector, take the dot product of
a text embedding with the anchored vector. This is the equivalent of
``projecting'' the embeddings down onto the scale between one end of the
anchored vector and the other, as shown in
Figure~\ref{fig-projection}.\footnote{For an intuitive explanation of
  why the dot product is equivalent to a projection, see 3blue1brown's
  video on the subject, at
  https://youtu.be/LyGKycYT2v0?si=86cfrN9DP9xw5HUx. Incidentally, the
  dot product with the anchored vector is also equivalent to the dot
  product with the positive embedding (e.g.~``happy'') minus the dot
  product with the negative vector (e.g.~``sad'').}

\begin{figure}[!htbp]

{\caption{{Projection of Embeddings Onto an Anchored
Vector}{\label{fig-projection}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-projection-1.pdf}

{\noindent \emph{Note.} Word embeddings were acquired from the
glove.twitter.27B.100d pretrained model and reduced to two dimensions
with Principle Component Analysis (PCA).}

\end{figure}

By projecting each embedding down onto the anchored vector between happy
and sad, we create a scale from happy to sad.\footnote{Taking the dot
  product with an anchored vector yields an unstandardized version of
  this scale. For a standardized version of this function, in which
  ``sad'' is at 0 and ``happy'' is at 1 on the scale, use the
  \texttt{anchored\_sim()} function included in our Github repo at
  https://github.com/rimonim/embeddings\_tutorial/blob/master/embedding\_scripts.R.}
This is sometimes referred to as \textbf{semantic projection} (Grand et
al., 2022).

Anchored vectors can be applied to any construct that has a clear
opposite, whether operationalized through CCR, DDR, or other methods.
Although their application to contextualized embeddings relies on strong
assumptions about the linearity of the contextualized embedding space,
they have been shown to work reasonably well for a variety of constructs
and models (Grand et al., 2022). For an empirical demonstration of the
robustness of anchored vectors for DDR, see Appendix A. For an empirical
demonstration of their applicability to CCR, see Appendix B.

In the following code, we use the anchored vector for locus of control
that we computed above (\texttt{loc\_anchored}) to measure locus of
control in \texttt{reddit\_emotions} and to test the hypothesis that
posts in r/depression exhibit more external locus of control than those
in r/TodayIamHappy (cf. Simchon et al., 2023).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate Dot Product With Anchored Vector}
\NormalTok{reddit\_emotion\_loc }\OtherTok{\textless{}{-}}\NormalTok{ reddit\_emotion\_bge }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{loc =} \FunctionTok{dot\_prod}\NormalTok{(}\FunctionTok{c\_across}\NormalTok{(Dim1}\SpecialCharTok{:}\NormalTok{Dim768), loc\_anchored)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(loc)}
  
\CommentTok{\# Test Hypothesis}
\NormalTok{test\_CCR }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ reddit\_emotion\_loc[reddit\_emotion}\SpecialCharTok{$}\NormalTok{subreddit}\SpecialCharTok{==}\StringTok{"depression"}\NormalTok{],}
  \AttributeTok{y =}\NormalTok{ reddit\_emotion\_loc[reddit\_emotion}\SpecialCharTok{$}\NormalTok{subreddit}\SpecialCharTok{==}\StringTok{"TodayIamHappy"}\NormalTok{]}
\NormalTok{)}
\NormalTok{test\_CCR}\SpecialCharTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.981485e-153
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_CCR}\SpecialCharTok{$}\NormalTok{estimate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mean of x mean of y 
-5.543292 -2.492451 
\end{verbatim}

We find that posts in r/TodayIamHappy have more internal (as opposed to
external) locus of control than those in r/depression (p \textless{}
.001).

\subsubsection{Correlational Anchored Vectors
(CAV)}\label{correlational-anchored-vectors-cav}

An anchored vector is simply a direction in the embedding space. Rather
than finding this direction by subtracting a negative construct
embedding from a positive one, as we have done so far, we can use
machine learning to find the direction that best represents the
contstruct in a training dataset. This provides an alternative to DDR
and CCR, by which construct representations are learned directly from
examples. Such examples may be the product of human raters or of an
experimental manipulation. For the example below we use data from
Kasprzyk and Calin-Jageman (2014), who asked participants to write about
either incidents in which they had power over others (Power) or
incidents in which other people had power over them (Control).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Embed Training Data}
\NormalTok{d\_train }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"example\_data/power\_narratives.csv"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{condition =} \FunctionTok{factor}\NormalTok{(condition, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Power"}\NormalTok{)))}

\NormalTok{d\_train\_bge }\OtherTok{\textless{}{-}} \FunctionTok{textEmbed}\NormalTok{(}
\NormalTok{  d\_train}\SpecialCharTok{$}\NormalTok{text,}
  \AttributeTok{model =} \StringTok{"BAAI/bge{-}base{-}en{-}v1.5"}\NormalTok{, }\CommentTok{\# model name}
  \AttributeTok{tokens\_select =} \StringTok{"[CLS]"}\NormalTok{, }\CommentTok{\# select only [CLS] token embedding}
  \AttributeTok{layers =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,  }\CommentTok{\# last layer}
  \AttributeTok{dim\_name =} \ConstantTok{FALSE}\NormalTok{,}
  \AttributeTok{keep\_token\_embeddings =} \ConstantTok{FALSE}
\NormalTok{)}
  
\CommentTok{\# Rejoin Embeddings to Labelled Training Data}
\NormalTok{d\_train }\OtherTok{\textless{}{-}}\NormalTok{ d\_train }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{bind\_cols}\NormalTok{(d\_train\_bge}\SpecialCharTok{$}\NormalTok{texts[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

With Partial Least Squares (PLS) regression (Mevik \& Wehrens, 2007;
Wold et al., 2001), which finds directions in the feature space that
best correlate with the dependent variable (in this case Power as
opposed to Control), we create a CAV. PLS is implemented here with the
caret package (Kuhn \& Max, 2008).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Partial Least Squares (PLS) regression}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2024}\NormalTok{)}
\NormalTok{pls\_control }\OtherTok{\textless{}{-}}\NormalTok{ caret}\SpecialCharTok{::}\FunctionTok{train}\NormalTok{(}
\NormalTok{  condition }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
  \AttributeTok{data =} \FunctionTok{select}\NormalTok{(d\_train, condition, Dim1}\SpecialCharTok{:}\NormalTok{Dim768), }
  \AttributeTok{method =} \StringTok{"pls"}\NormalTok{,}
  \AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{,  }\CommentTok{\# keep original embedding dimensions}
  \AttributeTok{trControl =}\NormalTok{ caret}\SpecialCharTok{::}\FunctionTok{trainControl}\NormalTok{(}\StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{),  }\CommentTok{\# cross{-}validation}
  \AttributeTok{tuneLength =} \DecValTok{1}  \CommentTok{\# only 1 component (our anchored vector)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract CAV}
\NormalTok{projection }\OtherTok{\textless{}{-}}\NormalTok{ pls\_control}\SpecialCharTok{$}\NormalTok{finalModel}\SpecialCharTok{$}\NormalTok{projection[,}\DecValTok{1}\NormalTok{]}
\NormalTok{power\_loading }\OtherTok{\textless{}{-}}\NormalTok{ pls\_control}\SpecialCharTok{$}\NormalTok{finalModel}\SpecialCharTok{$}\NormalTok{Yloadings[}\StringTok{"Power"}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{power\_cav }\OtherTok{\textless{}{-}}\NormalTok{ projection}\SpecialCharTok{*}\NormalTok{power\_loading}

\CommentTok{\# Calculate Distance Metrics}
\NormalTok{reddit\_emotion\_control }\OtherTok{\textless{}{-}}\NormalTok{ reddit\_emotion\_bge }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{control =} \FunctionTok{dot\_prod}\NormalTok{(}\FunctionTok{c\_across}\NormalTok{(Dim1}\SpecialCharTok{:}\NormalTok{Dim768), power\_cav)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(control)}

\CommentTok{\# Test Hypothesis}
\NormalTok{test\_correlational }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ reddit\_emotion\_control[reddit\_emotion}\SpecialCharTok{$}\NormalTok{subreddit}\SpecialCharTok{==}\StringTok{"depression"}\NormalTok{],}
  \AttributeTok{y =}\NormalTok{ reddit\_emotion\_control[reddit\_emotion}\SpecialCharTok{$}\NormalTok{subreddit}\SpecialCharTok{==}\StringTok{"TodayIamHappy"}\NormalTok{]}
\NormalTok{)}
\NormalTok{test\_correlational}\SpecialCharTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_correlational}\SpecialCharTok{$}\NormalTok{estimate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 mean of x  mean of y 
-0.3093040  0.1046436 
\end{verbatim}

We find that posts in r/TodayIamHappy share more characteristics in
common with narratives of the speaker having control over others than
did posts in r/depression (p \textless{} .001).

\paragraph{Advantages and Disadvantages of
CAV.}\label{advantages-and-disadvantages-of-cav}

CAV allows the application of closely related texts. Given a training
set that closely resembles the texts of interest, CAV provides a
relatively simple metric that is likely to be highly accurate and
unconfounded by differences in text genre. On the other hand, CAV
requires applicable training data and introduces questions of validity
when applying patterns in the training set to the texts of interest.

\subsubsection{Other Machine Learning
Methods}\label{other-machine-learning-methods}

Using PLS regression to generate a CAV is a very simple way to leverage
machine learning for interpreting embeddings. The simplicity of dealing
with a single direction in embedding space can provide useful intuition
and transparency for researchers. Nevertheless, it sacrifices some of
the predictive capacities available with modern machine learning
methods. With more complex algorithms, the nonlinearity of
contextualized embedding spaces becomes less of a problem. Given enough
training data, one can specify a model that can capture nonlinear
patterns, such as a support vector machine. More complex methods also
allow the simultaneous use of embeddings from multiple layers of an LLM.
A comprehensive review of machine learning algorithms for psychological
features is beyond the scope of this guide. See Kjell et al. (2022) for
a useful introductory primer.

\section{Conclusion}\label{conclusion}

In this guide, we have introduced the fundamentals of neural word
embedding models including word2vec, GloVe, and fastText. In doing so,
we have put special emphasis on the geometric properties of the
embeddings created by these models. For example, the correspondence
between the magnitude of a word embedding and its semantic
informativeness is guaranteed by the architecture of such models. This
correspondence also forms the basis for the much-touted additive
analogical property of word embeddings (Ethayarajh et al., 2019;
Mikolov, Chen, et al., 2013). This lies in contrast to embeddings
generated by LLMs, which may have non-linear or non-zero-centered
organizational patterns (Cai et al., 2021). Decontextualized word
embedding models therefore have two major advantages when compared to
LLM embeddings: their reliable geometries and their computational
efficiency. On the other hand, the contextualizing abilities of LLMs can
often make up for such deficiencies, especially with the help of recent
models specialized for use in semantic embedding.

With the necessary background provided above, we have reviewed three
methods of quantifying psychological constructs in embedding spaces:
DDR, CCR, and CAV. We recommend DDR for abstract constructs relating to
the overall feel of a text, especially when the research requires that
these constructs generalize to various genres of text. When using
dictionaries validated for use in word counting, we recommend weighting
words by their frequency when computing the average DDR. We recommend
CCR for cases in which texts are relatively similar in content to the
embedded questionnaire, such as experiments in which participants are
asked to respond to a related prompt. CAV is appropriate only when
suitably large and reliable training data is available.

\section{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-allport_1942}
Allport, G. W. (1942). The use of personal documents in psychological
science. \emph{Social Science Research Council Bulletin}, \emph{49}, xix
+ 210--xix + 210.

\bibitem[\citeproctext]{ref-atari_etal_2023}
Atari, M., Omrani, A., \& Dehghani, M. (2023). \emph{Contextualized
construct representation: Leveraging psychometric scales to advance
theory-driven text analysis}. PsyArXiv.
\url{https://doi.org/10.31234/osf.io/m93pd}

\bibitem[\citeproctext]{ref-baayen_2001}
Baayen, R. H. (2001). \emph{Word frequency distributions}. Springer
Netherlands.
\url{https://link.springer.com/book/10.1007/978-94-010-0844-0}

\bibitem[\citeproctext]{ref-baumgartner_etal_2020}
Baumgartner, J., Zannettou, S., Keegan, B., Squire, M., \& Blackburn, J.
(2020). \emph{The pushshift reddit dataset}. Zenodo.
\url{https://doi.org/10.5281/zenodo.3608135}

\bibitem[\citeproctext]{ref-benoit_etal_2018}
Benoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., \&
Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis
of textual data. \emph{Journal of Open Source Software}, \emph{3}(30),
774. \url{https://doi.org/10.21105/joss.00774}

\bibitem[\citeproctext]{ref-bojanowski_etal_2017}
Bojanowski, P., Grave, E., Joulin, A., \& Mikolov, T. (2017).
\emph{Enriching word vectors with subword information}.
\url{https://arxiv.org/abs/1607.04606}

\bibitem[\citeproctext]{ref-boyd_etal_2022}
Boyd, R., Ashokkumar, A., Seraj, S., \& Pennebaker, J. (2022). \emph{The
development and psychometric properties of LIWC-22}.
\url{https://doi.org/10.13140/RG.2.2.23890.43205}

\bibitem[\citeproctext]{ref-cai_etal_2021}
Cai, X., Huang, J., Bian, Y., \& Church, K. (2021). Isotropy in the
contextual embedding space: Clusters and manifolds. \emph{International
Conference on Learning Representations}.
\url{https://openreview.net/forum?id=xYGNO86OWDH}

\bibitem[\citeproctext]{ref-creegan_1944}
Creegan, R. F. (1944). The phenomenological analysis of personal
documents. \emph{The Journal of Abnormal and Social Psychology},
\emph{39}(2), 244--266. \url{https://doi.org/10.1037/h0062816}

\bibitem[\citeproctext]{ref-crystal_1997}
Crystal, D. (1997). \emph{The cambridge encyclopedia of language}
(Second edition.). Cambridge University Press.

\bibitem[\citeproctext]{ref-deerwester_etal_1990}
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., \&
Harshman, R. A. (1990). Indexing by latent semantic analysis.
\emph{Journal of the Association for Information Science and
Technology}, \emph{41}(6), 391--407.
\url{https://doi.org/10.1002/(SICI)1097-4571(199009)41:6\%3C391::AID-ASI1\%3E3.0.CO;2-9}

\bibitem[\citeproctext]{ref-devlin_etal_2019}
Devlin, J., Chang, M.-W., Lee, K., \& Toutanova, K. (2019). \emph{BERT:
Pre-training of deep bidirectional transformers for language
understanding}. \url{https://arxiv.org/abs/1810.04805}

\bibitem[\citeproctext]{ref-ethayarajh_2019}
Ethayarajh, K. (2019). \emph{How contextual are contextualized word
representations? Comparing the geometry of BERT, ELMo, and GPT-2
embeddings}. \url{https://arxiv.org/abs/1909.00512}

\bibitem[\citeproctext]{ref-ethayarajh_etal_2019}
Ethayarajh, K., Duvenaud, D., \& Hirst, G. (2019). \emph{Towards
understanding linear word analogies}.
\url{https://arxiv.org/abs/1810.04882}

\bibitem[\citeproctext]{ref-frimer_etal_2019}
Frimer, J. A., Boghrati, R., Haidt, J., Graham, J., \& Dehgani, M.
(2019). Moral foundations dictionary for linguistic analyses 2.0.
\emph{Unpublished Manuscript}.

\bibitem[\citeproctext]{ref-gao_etal_2019}
Gao, J., He, D., Tan, X., Qin, T., Wang, L., \& Liu, T.-Y. (2019).
\emph{Representation degeneration problem in training natural language
generation models}. \url{https://arxiv.org/abs/1907.12009}

\bibitem[\citeproctext]{ref-garten_etal_2018}
Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., \&
Dehghani, M. (2018). Dictionaries and distributions: Combining expert
knowledge and large scale textual data content analysis: Distributed
dictionary representation. \emph{Behavior Research Methods}, \emph{50},
344--361.

\bibitem[\citeproctext]{ref-goldberg_levy_2014}
Goldberg, Y., \& Levy, O. (2014). \emph{word2vec explained: Deriving
mikolov et al.'s negative-sampling word-embedding method}.
\url{https://arxiv.org/abs/1402.3722}

\bibitem[\citeproctext]{ref-grand_etal_2022}
Grand, G., Blank, I. A., Pereira, F., \& Fedorenko, E. (2022). Semantic
projection recovers rich human knowledge of multiple object features
from word embeddings. \emph{Nature Human Behaviour}, \emph{6}(7),
975--987. \url{https://doi.org/10.1038/s41562-022-01316-8}

\bibitem[\citeproctext]{ref-grave_etal_2018}
Grave, E., Bojanowski, P., Gupta, P., Joulin, A., \& Mikolov, T. (2018).
Learning word vectors for 157 languages. \emph{Proceedings of the
International Conference on Language Resources and Evaluation (LREC
2018)}. \url{https://fasttext.cc/docs/en/crawl-vectors.html}

\bibitem[\citeproctext]{ref-harris_1954}
Harris, Z. S. (1954). Distributional {Structure}. \emph{WORD},
\emph{10}(2-3), 146--162.
\url{https://doi.org/10.1080/00437956.1954.11659520}

\bibitem[\citeproctext]{ref-kasprzyk_calinjageman_2014}
Kasprzyk, L., \& Calin-Jageman, R. (2014). The effect of power on visual
perspective: Replications of galinksy, et al. 2006. \emph{Preprint at
Https://Doi. Org/10.17605/OSF. IO/WCH5R}.

\bibitem[\citeproctext]{ref-kjell_etal_2021}
Kjell, O., Giorgi, S., \& Schwartz, H. A. (2021). \emph{The
text-package: An r-package for analyzing and visualizing human language
using natural language processing and deep learning}. PsyArXiv.
\url{https://doi.org/10.31234/osf.io/293kt}

\bibitem[\citeproctext]{ref-kjell_etal_2022}
Kjell, O., Sikström, S., Kjell, K., \& Schwartz, H. (2022). Natural
language analyzed with AI-based transformers predict traditional
subjective well-being measures approaching the theoretical upper limits
in accuracy. \emph{Scientific Reports}, \emph{12}, 3918.
\url{https://doi.org/10.1038/s41598-022-07520-w}

\bibitem[\citeproctext]{ref-kuhn_2008}
Kuhn, \& Max. (2008). Building predictive models in r using the caret
package. \emph{Journal of Statistical Software}, \emph{28}(5), 1--26.
\url{https://doi.org/10.18637/jss.v028.i05}

\bibitem[\citeproctext]{ref-li_etal_2020}
Li, B., Zhou, H., He, J., Wang, M., Yang, Y., \& Li, L. (2020). \emph{On
the sentence embeddings from pre-trained language models}.
\url{https://arxiv.org/abs/2011.05864}

\bibitem[\citeproctext]{ref-mevik_wehrens_2007}
Mevik, B.-H., \& Wehrens, R. (2007). The pls package: Principal
component and partial least squares regression in r. \emph{Journal of
Statistical Software}, \emph{18}(2), 1--23.
\url{https://doi.org/10.18637/jss.v018.i02}

\bibitem[\citeproctext]{ref-mikolov_etal_2013b}
Mikolov, T., Chen, K., Corrado, G., \& Dean, J. (2013). \emph{Efficient
estimation of word representations in vector space}.
\url{https://arxiv.org/abs/1301.3781}

\bibitem[\citeproctext]{ref-mikolov_etal_2013c}
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., \& Dean, J. (2013).
\emph{Distributed representations of words and phrases and their
compositionality}. \url{https://arxiv.org/abs/1310.4546}

\bibitem[\citeproctext]{ref-mikolov_etal_2013}
Mikolov, T., Yih, W., \& Zweig, G. (2013). Linguistic regularities in
continuous space word representations. In L. Vanderwende, H. Daumé III,
\& K. Kirchhoff (Eds.), \emph{Proceedings of the 2013 conference of the
north {A}merican chapter of the association for computational
linguistics: Human language technologies} (pp. 746--751). Association
for Computational Linguistics. \url{https://aclanthology.org/N13-1090}

\bibitem[\citeproctext]{ref-muennighoff_etal_2022}
Muennighoff, N., Tazi, N., Magne, L., \& Reimers, N. (2022). MTEB:
Massive text embedding benchmark. \emph{arXiv Preprint
arXiv:2210.07316}. \url{https://doi.org/10.48550/ARXIV.2210.07316}

\bibitem[\citeproctext]{ref-oconnell_kowal_2003}
O'Connell, D., \& Kowal, S. (2003). Psycholinguistics: A half century of
monologism. \emph{The American Journal of Psychology}, \emph{116},
191--212. \url{https://doi.org/10.2307/1423577}

\bibitem[\citeproctext]{ref-oconnor_2012}
O'Connor, B. (2012). Cosine similarity, {Pearson} correlation, and {OLS}
coefficients. In \emph{AI and Social Science}.
\url{https://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/}

\bibitem[\citeproctext]{ref-oyama_etal_2023}
Oyama, M., Yokoi, S., \& Shimodaira, H. (2023). \emph{Norm of word
embedding encodes information gain}.
\url{https://arxiv.org/abs/2212.09663}

\bibitem[\citeproctext]{ref-pang_lee_2005}
Pang, B., \& Lee, L. (2005). \emph{Seeing stars: Exploiting class
relationships for sentiment categorization with respect to rating
scales}. \url{https://arxiv.org/abs/cs/0506075}

\bibitem[\citeproctext]{ref-pennington_etal_2014}
Pennington, J., Socher, R., \& Manning, C. D. (2014). GloVe: Global
vectors for word representation. \emph{Empirical Methods in Natural
Language Processing (EMNLP)}, 1532--1543.
\url{http://www.aclweb.org/anthology/D14-1162}

\bibitem[\citeproctext]{ref-peterson_seligman_1984}
Peterson, C., \& Seligman, M. E. (1984). Causal explanations as a risk
factor for depression: Theory and evidence. \emph{Psychological Review},
\emph{91}(3), 347--374.

\bibitem[\citeproctext]{ref-placinski_zywiczynski_2023}
Placiński, M., \& Żywiczyński, P. (2023). Modality effect in interactive
alignment: Differences between spoken and text-based conversation.
\emph{Lingua}, \emph{293}, 103592.
https://doi.org/\url{https://doi.org/10.1016/j.lingua.2023.103592}

\bibitem[\citeproctext]{ref-rana_2010}
Rana, A. (2010). \emph{Common crawl -- building an open web-scale crawl
using hadoop}.
\url{https://www.slideshare.net/hadoopusergroup/common-crawlpresentation}

\bibitem[\citeproctext]{ref-reif_etal_2019}
Reif, E., Yuan, A., Wattenberg, M., Viegas, F. B., Coenen, A., Pearce,
A., \& Kim, B. (2019). Visualizing and measuring the geometry of BERT.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, \&
R. Garnett (Eds.), \emph{Advances in neural information processing
systems} (Vol. 32). Curran Associates, Inc.
\url{https://proceedings.neurips.cc/paper_files/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf}

\bibitem[\citeproctext]{ref-reimers_gurevych_2019}
Reimers, N., \& Gurevych, I. (2019). Sentence-{BERT}: Sentence
embeddings using {S}iamese {BERT}-networks. In K. Inui, J. Jiang, V. Ng,
\& X. Wan (Eds.), \emph{Proceedings of the 2019 conference on empirical
methods in natural language processing and the 9th international joint
conference on natural language processing (EMNLP-IJCNLP)} (pp.
3982--3992). Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/D19-1410}

\bibitem[\citeproctext]{ref-rotter_1966}
Rotter, J. B. (1966). Generalized expectancies for internal versus
external control of reinforcement. \emph{Psychological Monographs:
General and Applied}, \emph{80}(1), 1.

\bibitem[\citeproctext]{ref-rude_etal_2004}
Rude, S., Gortner, E.-M., \& Pennebaker, J. (2004). Language use of
depressed and depression-vulnerable college students. \emph{Cognition \&
Emotion - COGNITION EMOTION}, \emph{18}, 1121--1133.
\url{https://doi.org/10.1080/02699930441000030}

\bibitem[\citeproctext]{ref-schakel_wilson_2015}
Schakel, A. M. J., \& Wilson, B. J. (2015). \emph{Measuring word
significance using distributed representations of words}.
\url{https://arxiv.org/abs/1508.02297}

\bibitem[\citeproctext]{ref-simchon_etal_2023}
Simchon, A., Hadar, B., \& Gilead, M. (2023). A computational text
analysis investigation of the relation between personal and linguistic
agency. \emph{Communications Psychology}, 1--9.
\url{https://doi.org/10.1038/s44271-023-00020-1}

\bibitem[\citeproctext]{ref-teitelbaum_simchon_2024}
Teitelbaum, L., \& Simchon, A. (2024). \emph{Data science for
psychology: Natural language}.
https://doi.org/\url{https://zenodo.org/doi/10.5281/zenodo.10908366}

\bibitem[\citeproctext]{ref-torabi-asr_etal_2018}
Torabi Asr, F., Zinkov, R., \& Jones, M. (2018). Querying word
embeddings for similarity and relatedness. In M. Walker, H. Ji, \& A.
Stent (Eds.), \emph{Proceedings of the 2018 conference of the north
{A}merican chapter of the association for computational linguistics:
Human language technologies, volume 1 (long papers)} (pp. 675--684).
Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/N18-1062}

\bibitem[\citeproctext]{ref-trager_etal_2022}
Trager, J., Ziabari, A. S., Davani, A. M., Golazazian, P.,
Karimi-Malekabadi, F., Omrani, A., Li, Z., Kennedy, B., Reimer, N. K.,
Reyes, M., Cheng, K., Wei, M., Merrifield, C., Khosravi, A., Alvarez,
E., \& Dehghani, M. (2022). \emph{The moral foundations reddit corpus}.
\url{https://arxiv.org/abs/2208.05545}

\bibitem[\citeproctext]{ref-tversky_gati_1982}
Tversky, A., \& Gati, I. (1982). Similarity, separability, and the
triangle inequality. \emph{Psychological Review}, \emph{89}, 123--154.
\url{https://doi.org/10.1037/0033-295X.89.2.123}

\bibitem[\citeproctext]{ref-wickham_etal_2019}
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D.,
François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M.,
Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J.,
Robinson, D., Seidel, D. P., Spinu, V., \ldots{} Yutani, H. (2019).
Welcome to the {tidyverse}. \emph{Journal of Open Source Software},
\emph{4}(43), 1686. \url{https://doi.org/10.21105/joss.01686}

\bibitem[\citeproctext]{ref-wold_etal_2001}
Wold, S., Sjöström, M., \& Eriksson, L. (2001). PLS-regression: A basic
tool of chemometrics. \emph{Chemometrics and Intelligent Laboratory
Systems}, \emph{58}, 109--130.
\url{https://api.semanticscholar.org/CorpusID:11920190}

\bibitem[\citeproctext]{ref-xiao_etal_2023}
Xiao, S., Liu, Z., Zhang, P., \& Muennighoff, N. (2023). \emph{C-pack:
Packaged resources to advance general chinese embedding}.
\url{https://arxiv.org/abs/2309.07597}

\end{CSLReferences}

\appendix

\section{DDR Metrics}\label{ddr-metrics}

Garten et al. (2018) found that DDR works best with smaller dictionaries
of only the words most directly connected to the construct being
measured (around 30 words worked best in their experiments). Here we
replicate their study using slightly different methods, and extend it to
a variety of vector-based metrics, including anchored vectors. We also
investigate the impact of weighting the averaged dictionary
representation by token frequency, which we suggested would eliminate
the observed superiority of smaller dictionaries.

\subsection{Benchmark 1: Negative Sentiment in Movie
Reviews}\label{benchmark-1-negative-sentiment-in-movie-reviews}

As an initial benchmark, we used the same data used by Garten et al.
(2018) in their investigation of dictionary size: a set of 2000 movie
reviews, half labeled as negative and half as positive (Pang \& Lee,
2005). For vector representations of words and texts, we used a publicly
available GloVe model trained on 2B Tweets to produce 100-dimensional
embeddings (Pennington et al., 2014). For construct representations, we
used the positive tone and negative tone dictionaries from LIWC-22 (Boyd
et al., 2022), expanded on the movie reviews dataset. The primary DDR
was the average embedding of the negative tone dictionary, while for
anchored vectors the average embedding of the positive tone dictionary
was used as a second anchor.

We investigated the following metrics:

\begin{itemize}
\tightlist
\item
  \textbf{Cosine similarity} with the negative tone DDR
\item
  \textbf{Cosine similarity with the anchored vector} (equivalent to
  projection of the normalized text vector onto the anchored vector)
\item
  \textbf{Dot product} with the negative tone DDR
\item
  \textbf{Dot product with the anchored vector} (equivalent to
  projection of the raw text vector onto the anchored vector)
\item
  \textbf{Dot product with the pre-normalized anchored vector}
  (i.e.~positive and negative DDR embeddings normalized before
  calculating the anchored vector)
\item
  \textbf{Euclidean distance} from the negative tone DDR
\item
  \textbf{Euclidean distance from the anchored vector}
\end{itemize}

To evaluate the predictive value of each metric, we trained a univariate
logistic regression model for each metric at each dictionary size. We
then computed an F1 score for the model's predictions on the training
set, with the model's threshold set at 0.5 (i.e.~any text given a
probability of greater than 0.5 of being negative was considered as
having been predicted to be negative).

When using only the primary DDR, we found that the performance of equal
weighting drops sharply with increasing dictionary size, while the
performance of frequency weighting continues to rise. Indeed, DDRs
computed with equal weighting were likely to result in negative
associations between the predictor and the outcome, even at small
dictionary sizes. Surprisingly, metrics based on anchored vectors were
robust to this instability. Additionally, frequency weighting was not
superior to equal weighting for metrics based on anchored vectors. The
overall best performing metric across all dictionary sizes was cosine
similarity with the anchored vector calculated using equal weighting.

\begin{figure}

{\caption{{Mean F1 by Dictionary Size}{\label{fig-f1\_by\_dictsize}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-f1_by_dictsize-1.pdf}

{\noindent \emph{Note.} Each data point represents the mean of 200
samples. F1 scores arising from negative associations between the
predictor and the outcome are counted as negative.}

\end{figure}

The pattern of results was similar among the highest performing
dictionaries at each size, with two notable exceptions. First, frequency
weighting was slightly better than equal weighting for all metrics
(including those based on anchored vectors) except cosine similarity
with the anchored vector. Second, the performance of frequency weighting
among metrics using only the primary DDR did not increase with sample
size.

\begin{figure}

{\caption{{Mean F1 Score by Dictionary Size for Dictionaries Above the
80th Percentile}{\label{fig-f1\_by\_dictsize\_top}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-f1_by_dictsize_top-1.pdf}

{\noindent \emph{Note.} Each data point represents the mean of 200
samples. F1 scores arising from negative associations between the
predictor and the outcome are counted as negative.}

\end{figure}

\begin{figure}

{\caption{{Significant Negative Effects by Dictionary
Size}{\label{fig-neg\_by\_dictsize}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-neg_by_dictsize-1.pdf}

{\noindent \emph{Note.} 200 samples per data point.}

\end{figure}

As a further validation of our proposed frequency weighting method, we
investigated the performance of dictionary representations as a function
of the variance of their word frequencies. We found that dictionaries
with higher variation in word frequencies result in a large advantage
for frequency weighted aggregation. As before, we found that this did
not hold for metrics based on anchored vectors.

\begin{figure}

{\caption{{Mean F1 Score by Dictionary Frequency
Variance}{\label{fig-neg\_by\_freq\_variance}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-neg_by_freq_variance-1.pdf}

{\noindent \emph{Note.} Smoothing lines are computed with LOESS
regression. F1 scores arising from negative associations between the
predictor and the outcome are counted as negative.}

\end{figure}

\subsection{Benchmark 2: Moral Foundations in Reddit
Comments}\label{benchmark-2-moral-foundations-in-reddit-comments}

To see whether the patterns observed for negative valence extend to more
complex psychological constructs, we evaluated the same metrics and
aggregation methods on a large dataset of Reddit comments (N = 16,123),
which were manually annotated with six moral foundations: authority,
care, fairness, loyalty, sanctity, and vice (Trager et al., 2022). Each
text was annotated by at least three annotators, giving a total of
53,545 cases.

To construct distributed construct representations for the moral
foundations, we used the Moral Foundations Dictionary 2.0 (Frimer et
al., 2019), which includes more than 200 words per foundation. Since the
moral foundations do not have clear opposites, we constructed a neutral
embedding by averaging the embeddings of all six foundations. This
neutral embedding was used as the second anchor in anchored vector
metrics.

Since the Reddit dataset was heavily imbalanced (i.e.~most comments were
not labeled as reflecting any given foundation), we set the classifier
threshold at the empirical probability of each rating. For example, if
20\% of texts in the dataset were labeled as reflecting loyalty, we
would consider any text given a probability of greater than 0.2 to
reflect loyalty according to the model. Using these predictions, F1
scores were computed as for the first benchmark.

We found that frequency weighted aggregation performed better than equal
weighting in all five moral foundations except loyalty, for which all
metrics performed comparably. Furthermore, the pattern of optimal
metrics was somewhat erratic for equal weighting, whereas with frequency
weighted aggregation, cosine similarity with the DDR performed best in
all five foundations.

Curiously, anchored vector metrics did not perform as well as simple
similarity scores. This may be attributable to the use of a neutral
embedding as the second anchor, rather than a true opposite.

\begin{figure}

{\caption{{Equal vs.~Frequency Weights for Moral Foundation
DDRs}{\label{fig-morality\_equalvsfreq}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-morality_equalvsfreq-1.pdf}

{\noindent \emph{Note.} Vertical lines represent mean F1 scores across
moral foundations.}

\end{figure}

For equal weighting, Euclidean distance from the anchored vector
resulted in significant negative effects in 1/5 foundations. For
frequency weighted aggregation, the dot product with the pre-normalized
anchored vector resulted in significant negative effects in 3/5
foundations. Otherwise no negative effects were observed.

\subsection{Conclusions}\label{conclusions}

The results of our experiments support our suggestion that the
diminishing performance of larger dictionaries is due to the influence
of less frequent words. We found that computing the DDR as a weighted
average generally improves performance, especially for larger
dictionaries of a few hundred words. We further found that metrics based
on anchored vectors were largely robust to the influence of term
weighting. Nevertheless, metrics based on anchored vectors require a
construct with a clear opposite---while anchored vectors performed well
for negative vs.~positive sentiment, they did not perform well for moral
foundations as compared to a neutral moral foundation embedding.

\section{CCR Metrics}\label{ccr-metrics}

Atari et al. (2023) asked participants to describe their core values in
their own words (values essay) and to likewise describe their activities
in the past week (behaviors essay). They then assessed participants on
22 questionnaire-based scales. This design allowed them to validate CCR
by obtaining a contextualized embedding of each questionnaire and
comparing it to the contextualized embedding of each essay. In benchmark
1 we partially replicate Atari et al.'s analysis and extend it to
various vector-based metrics, including anchored vectors. In benchmark
2, we investigate the extent to which the techniques generalize to a
more naturalistic context.

\subsection{Benchmark 1: Prompted
Essays}\label{benchmark-1-prompted-essays}

The anchored vector is equivalent to the embedding of the questionnaire
(positive CCR) minus the embedding of the negated questionnaire
(negative CCR). The individualism items were used as a negated form of
the collectivism, and vice versa. To obtain negated versions of the
remaining questionnaires, we queried GPT-4o and manually curated the
results.\footnote{All matrials, including original and negated
  questionnaire items, are available on our Github repo at
  https://github.com/rimonim/embeddings\_tutorial.} We investigated the
following metrics:

\begin{itemize}
\tightlist
\item
  \textbf{Cosine similarity} with the CCR
\item
  \textbf{Cosine similarity with the anchored vector} (equivalent to
  projection of the normalized text vector onto the anchored vector)
\item
  \textbf{Dot product} with the CCR
\item
  \textbf{Dot product with the anchored vector} (equivalent to
  projection of the raw text vector onto the anchored vector)
\item
  \textbf{Dot product with the pre-normalized anchored vector}
  (i.e.~positive and negative CCR embeddings normalized before
  calculating the anchored vector)
\item
  \textbf{Euclidean distance} from the CCR
\item
  \textbf{Euclidean distance from the anchored vector}
\end{itemize}

Although the term CCR implies the use of contextualized embeddings, we
use it here to refer to any vector embedding of a questionnaire. To
obtain embeddings for participant essays and questionnaire items, we
used two pretrained models:

\begin{itemize}
\tightlist
\item
  \textbf{SBERT}
  (\href{https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2}{\texttt{sentence-transformers/all-MiniLM-L12-v2}}),
  a model similar to that used by Atari et al. (2023), designed to
  produce 384-dimensional contextualized embeddings amenable to use with
  cosine similarity.
\item
  \textbf{GloVe}
  (\href{https://huggingface.co/stanfordnlp/glove/tree/main}{\texttt{glove.twitter.27B.100d}})
  a pretrained word embedding model with a 100-dimensional embedding
  space. GloVe embeddings each text were obtained by aggregating the
  embeddings of each word in the text, discounting tokens not available
  in the pretrained GloVe model. Since GloVe is a decontextualized
  model, we used it here as a baseline for evaluating the utility of
  contextualized embeddings in CCR analysis.
\end{itemize}

As a further baseline, we also performed DDR using dictionaries for each
construct and its opposite. These dictionaries were generated by GPT-4o
and manually curated, but were not validated in any way prior to
testing. We can therefore consider this DDR to be a conservative
baseline for evaluating CCR methods. DDR analysis was performed with the
same GloVe model described above.

All code is available in the source code for this appendix on Github.

\subsubsection{Results: Values Essay}\label{results-values-essay}

In predicting questionnaire responses using participant values essays,
the most effective metrics were as follows:

\textbf{DDR:}

\begin{itemize}
\tightlist
\item
  Dot product with anchored vector (mean R\textsuperscript{2} = 0.016)
\item
  Dot product with pre-normalized anchored vector (mean
  R\textsuperscript{2} = 0.015)
\item
  Cosine similarity with anchored vector (mean R\textsuperscript{2} =
  0.012)
\end{itemize}

\textbf{GloVe:}

\begin{itemize}
\tightlist
\item
  Cosine similarity with anchored vector (mean R\textsuperscript{2} =
  0.010)
\item
  Dot product with pre-normalized anchored vector (mean
  R\textsuperscript{2} = 0.009)
\end{itemize}

\textbf{SBERT:}

\begin{itemize}
\tightlist
\item
  Dot product with anchored vector (mean R\textsuperscript{2} = 0.023)
\item
  Cosine similarity with anchored vector (mean R\textsuperscript{2} =
  0.023)
\item
  Dot product with pre-normalized anchored vector (mean
  R\textsuperscript{2} = 0.022)
\end{itemize}

\begin{figure}

{\caption{{Values Essay}{\label{fig-values\_essay}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-values_essay-1.pdf}

{\noindent \emph{Note.} Negative or insignificant effects (p
\textgreater{} 0.05) are displayed as translucent and are considered to
be 0 in metric-wise averages.}

\end{figure}

For values essays, SBERT was consistently more effective than GloVe, and
generally better than DDR. In all models, Euclidean distance metrics
were minimally effective and most likely to result in significant
negative effects. The success of GloVe-based CCR with metrics that
involve anchored vectors is surprising, since most negated questionnaire
items only differ from the originals by the words ``do not'' or
similarly uninformative negations.

\begin{figure}

\caption{\label{fig-values_essay_neg}Significant Negative Effects in
Values Essay}

\centering{

\includegraphics{embeddings_paper_files/figure-pdf/fig-values_essay_neg-1.pdf}

}

\end{figure}%

\subsubsection{Results: Behaviors Essay}\label{results-behaviors-essay}

In predicting questionnaire responses using participant behaviors
essays, the most effective metrics were as follows:

\textbf{DDR:}

\begin{itemize}
\tightlist
\item
  Dot product with anchored vector (mean R\textsuperscript{2} = 0.017)
\item
  Dot product with CCR (mean R\textsuperscript{2} = 0.016)
\item
  Euclidean distance from anchored vector (mean R\textsuperscript{2} =
  0.014)
\end{itemize}

\textbf{GloVe:}

\begin{itemize}
\tightlist
\item
  Dot product with CCR (mean R\textsuperscript{2} = 0.014)
\item
  Euclidean distance from anchored vector (mean R\textsuperscript{2} =
  0.014)
\item
  Dot product with anchored vector (mean R\textsuperscript{2} = 0.006)
\end{itemize}

\textbf{SBERT:}

\begin{itemize}
\tightlist
\item
  Euclidean distance from anchored vector (mean R\textsuperscript{2} =
  0.008)
\item
  Dot product with CCR (mean R\textsuperscript{2} = 0.006)
\end{itemize}

\begin{figure}

{\caption{{Behaviors Essay}{\label{fig-behaviors\_essay}}}}

\includegraphics{embeddings_paper_files/figure-pdf/fig-behaviors_essay-1.pdf}

{\noindent \emph{Note.} Negative or insignificant effects (p
\textgreater{} 0.05) are displayed as translucent and are considered to
be 0 in metric-wise averages.}

\end{figure}

For behaviors essays, GloVe-based CCR was more consistently effective
than SBERT, and DDR was most effective overall. While Euclidean distance
from the anchored vector was most effective on average for both models,
it was also most likely to result in significant negative effects.

\begin{figure}

\caption{\label{fig-behaviors_essay_neg}Significant Negative Effects in
Behaviors Essay}

\centering{

\includegraphics{embeddings_paper_files/figure-pdf/fig-behaviors_essay_neg-1.pdf}

}

\end{figure}%

\subsection{Conclusions}\label{conclusions-1}

CCR (Atari et al., 2023) proposes to measure psychological constructs in
texts by comparing text embeddings to embeddings of questionnaires. This
approach to text analysis can be highly effective, but is sensitive to
the nature of both questionnaires and texts. Some questionnaires do not
appear to be amenable to CCR at all, including those used here for
tightness, collectivism, individualism, proportionality, equality, and
safety. Among those scales that were amenable to CCR metrics, the
pattern of optimal metrics was greatly influenced by the content of the
texts being analyzed.

Values essays tend to be similar in content to the values questionnaires
being used, since the questionnaire items almost all consist of
statements in the first person. Contextualized embeddings from an SBERT
model appear to perform best in these sorts of situations.

Behaviors essays, which bear little resemblance to questionnaire items
in either tone or content, behave very differently. In particular, GloVe
embeddings of the questionnaires tend to perform better. This may be due
to the consistent geometric properties of GloVe embeddings, which can
more reliably encode semantic relationships between very different
contexts. Alternatively, this may be due to the datasets used to train
SBERT models, which emphasize topical similarity rather than similarity
in tone (Reimers \& Gurevych, 2019). Unsurprisingly, GloVe embeddings of
dictionaries associated with the questionnaires performed best overall.

Overall, the results of this analysis suggest that CCR is can be a
powerful tool, but that it should be used primarily in contexts in which
the content of the questionnaires is similar to that of the texts being
analyzed. In such cases, we recommend negating the questionnaire items,
computing an anchored vector, and scoring texts by the dot products of
their embeddings with the anchored vector.

When the content of the questionnaires is not similar to that of the
texts being analyzed, we recommend using DDR with negative and positive
versions of each dictionary, and scoring texts by the dot products of
their embeddings with the anchored vector. This approach appears to
outperform CCR even with unvalidated dictionaries generated by GPT-4o.






\end{document}
