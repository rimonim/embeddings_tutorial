---
title: "Neural Text Embeddings in Psychological Research: A Guide With Examples in R"
shorttitle: "Neural Text Embeddings"
author:
  - name: Louis Teitelbaum
    corresponding: true
    orcid: 0009-0001-9347-0145
    email: louist@post.bgu.ac.il
    affiliations:
      - id: id1
        name: "Ben-Gurion University of the Negev"
        department: Department of Psychology
        address: POB 653
        city: Beer Sheva
        country: Israel
        postal-code: 84105
  - name: Almog Simchon
    corresponding: false
    orcid: 0000-0003-2629-2913
    email: almogsi@post.bgu.ac.il
    affiliations:
      - ref: id1
abstract: "As the capabilities of neural language models grow, psychologists in various subfields have begun to use them as tools in analysis. We introduce leading methods for quantifying psychological dimensions of text using neural semantic embeddings, and discuss their respective advantages and disadvantages. For the uninitiated reader, we provide an intuitive conceptual introduction to semantic embeddings, and discuss practical and theoretical differences between embeddings generated by word embedding models (e.g. word2vec; GloVe) and those generated by transformer-based large language models (LLMS). We review three methods of quantifying psychological constructs in embedding spaces: Distributed Dictionary Representation (DDR), Contextualized Construct Representation (CCR), and Correlational Anchored Vectors (CAV). We explore potential pitfalls of each method, and recommend best practices for their application in research. For each method introduced, we provide sample code in R."
keywords: [natural-language-procressing, r, deep-learning, word-embeddings, text-embeddings]
author-note:
  disclosures: Materials for this guide were originally developed for the “Data Science for Psychology Lab” course in the psychology department at Ben-Gurion University of the Negev (BGU), and are included in Teitelbaum and Simchon (2024). Development was supported by funding from BGU.
  data-sharing: Full code for this paper is available at https://github.com/rimonim/embeddings_tutorial
floatsintext: true
numbered-lines: false
bibliography: references.bib
suppress-title-page: false
link-citations: false
lang: en
format:
  apaquarto-docx: default
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: man
    keep-tex: true
---

```{r}
#| label: setup
#| include: false
library(conflicted)
library(tidyverse)
library(knitr)
library(tidyverse)
library(quanteda)
conflicts_prefer(dplyr::filter, .quiet = TRUE)

source("embedding_scripts.R")
path_to_glove <- "~/Projects/ds4psych/data/glove/glove.twitter.27B.100d.txt"
glove_pretrained <- load_embeddings_txt(path_to_glove)
```

Psychologists have long insisted that talk therapy can heal, and that questionnaires can accurately measure psychological phenomena. These are language-based techniques, which rely on the assumption that language processing is linked to more fundamental internal states. Even so, psychological research has historically been unable to study _naturalistic language_---the sort of language that people produce in their day-to-day lives. 

The first barrier to research of naturalistic language is that it has historically been difficult to record. Early efforts by linguists to record language samples from representative populations were heroic; starting in 1896, Edmond Edmont spent four years traveling around France on a bicycle conducting specially designed interviews to collect data for the _Atlas linguistique de la France_. He collected data from 700 participants in total [@crystal_1997]. Since then, microphones have made it easier to record speech, and the advent of transformer neural networks has made it possible to accurately transcribe speech at minimal cost. While these advances in audio processing are important, the most important recent change in the availability of natural language to psychologists has come through a different medium: text. Only a few decades ago, public access to text was limited to highly edited long-form productions like books, magazines, and newspapers. Some psychologists studied diaries or personal letters [e.g. @allport_1942; @creegan_1944], but personal documents like these are hard to collect at scale. With the rise of social media, minimally edited, naturalistic text has become the norm. Now more than ever before, people communicate through text---not just in long-distance correspondence, but for day-to-day socializing with friends and family. Moreover, much of this textual communication is synchronous and shares many of the same features as face-to-face spoken conversation [@placinski_zywiczynski_2023]. Most importantly, much of this textual communication is freely available to researchers through social media platforms like Reddit and YouTube.

A second barrier to the use of naturalistic language in psychology is the problem of quantification. Language is complex, with near-infinite ways to describe the same thing. There are no clear rules for measuring the extent to which a text reflects depression, anxiety, mania, introspection, or any other psychological construct. Before the past two decades, the few researchers who tried to extract quantitative psychological dimensions of text were nearly as heroic as Edmond Edmont on his four year journey around France. For example, @peterson_seligman_1984 administered a questionnaire that prompted participants to write short explanations of various hypothetical scenarios. They then carefully read each response, noted each time a phrase like "because of" or "as a result of" was mentioned, and marked the accompanying explanation. These explanations were then typed by hand and shown one at a time to four trained judges who rated them on various 7-point scales. Finally, the agreement between the judges was assessed and their ratings were aggregated into the final variable used in their analysis of risk factors for depression. Today, this sort of analysis could be performed in a matter of seconds using the methods discussed in this guide. 

Nuanced quantitative measures of language can also solve a third historical problem with naturalistic language in psychology: its social nature. People do not write or speak in a vacuum, they participate in conversations or group discussions, considering their audiences as they form their words. For the researcher, this means that language is full of uncontrolled, confounding variables: Is the speaker responding to another speaker? Who is the other speaker? How many participants are there in the conversation? Researchers in the field of psycholinguistics have tried to solve these problems by isolating speakers in a laboratory setting, contriving situations in which participants process and produce speech without the uncontrolled variability of conversational partners [@oconnell_kowal_2003]. Nevertheless, the inherently social nature of language has made it difficult to analyze language behavior in even remotely naturalistic settings. A few decades ago the question "How similar are Daniel's utterances to Amos's utterances?" would have seemed hopelessly ill-defined. Similar in what way? Today, answering this question is simple with vector-based text embeddings. 

In this guide, we introduce state of the art methods for using neural text embeddings to quantify psychological dimensions of text. We aim to explain the conceptual foundations of these methods in a way that is accessible to psychological researchers with no background in machine learning or mathematics. We note methodological concerns and give concrete recommendations regarding proper usage of the techniques discussed. Throughout, we provide example code in R using the *tidyverse* [@wickham_etal_2019] and *Quanteda* [@benoit_etal_2018] packages. Other packages are used more sparingly and will be noted as they become relevant.

```{r}
#| eval: false
#| echo: true
library(tidyverse)
library(quanteda)
```

We assume that the reader has basic proficiency in both of these frameworks. For a broader introduction to psychological text analysis using the tidyverse and Quanteda, see @teitelbaum_simchon_2024. To follow along with the example code for this guide, please also load our suite of custom functions, available on our Github repo.[^intro-1] For readers who are primarily interested in the example code, the Github repo offers full example workflows for each method discussed. 

[^intro-1]: https://github.com/rimonim/embeddings_tutorial/blob/master/embedding_scripts.R

```{r}
#| echo: true
#| eval: false
source("embedding_scripts.R")
```

# Abstract Embedding Spaces

The modern ability to generate nuanced, quantitative representations of language relies on a key assumption: The meaning of any word (or other subcomponent of text, called a "token") can be represented as a point in a single, continuous space. This space is referred to as the *semantic embedding space*, since tokens are "embedded" into it according to their semantic content. Points close to each other in the embedding space have similar meanings, while those far away convey different meanings. While certain directions in this space may correspond to a familiar scale along which meaning can vary (e.g. largeness vs. smallness), the dimensions of the space are essentially abstract; points are only defined relative to each other, through the use of distance metrics. Embedding spaces can be difficult to imagine, not only because of their abstract nature but also because they have many more than three dimensions---often hundreds or even thousands. This high dimensionality is necessary to represent the many ways in which linguistic meaning can vary.

## Navigating Vector Spaces: Distance and Similarity

Once words or texts are embedded into the semantic space, how do we quantify their relative positions? Here we introduce three metrics by which two points in an embedding space can be compared:  most Euclidean distance, cosine similarity, and the dot product. While these do not constitute a comprehensive review of available similarity metrics, we consider them sufficient for making effective use of neural text embeddings in psychological research.

### Euclidean Distance

The most straightforward way to measure the similarity between two points in space is to measure the distance between them. Euclidean distance is the simplest sort of distance---the length of the shortest straight line between the two points. The Euclidean distance between two vectors $A$ and $B$ can be calculated in any number of dimensions $n$ using the following formula:

$$
d\left( A,B\right)   = \sqrt {\sum _{i=1}^{n}  \left( A_{i}-B_{i}\right)^2 }
$$

A low Euclidean distance means two vectors are very similar. Euclidean distance has the advantage of being familiar to many non-specialists, but it is not usually well suited to the organizational structure of neural text embeddings and can result in higher rates of false positives than other similarity metrics (see Appendix B). 

### Cosine Similarity

The most common way to measure the similarity between two embedding vectors is with cosine similarity. This is the cosine of the angle between the two vectors. Since the cosine of 0 is 1, a high cosine similarity (close to 1) means two vectors are very similar. To give a simplified example, we consider the vectors in @tbl-example_vecs.

```{r}
#| label: tbl-example_vecs
#| echo: false
#| tbl-cap: Example Three-Dimensional Vectors
example_vecs <- tribble(
  ~vector,    ~Dim1, ~Dim2, ~Dim3, 
  "Vector A", 3,   7,   2,
  "Vector B", 6,   4,   8,
)
example_vecs |> knitr::kable()
```

```{r}
#| label: fig-distmeasures
#| echo: false
#| warning: false
#| fig-cap: Two Simple Distance Measures
#| fig-height: 4
#| fig-width: 8
library(patchwork)

cos_plot <- example_vecs |> 
  ggplot() +
    geom_segment(aes(Dim2, Dim1, xend = 0, yend = 0)) +
    ggforce::geom_arc(
      aes(x0 = 0, y0 = 0, r = 4, start = 1.1659, end = 0.588),
      linewidth = 1, color = "red4", arrow = arrow(ends = "both", length = unit(0.25, "cm"))
      ) +
    annotate("text", label = "cos(θ) = 0.84", 
             x = 4.5, y = 3.1, color = "red4", fontface = "bold") +
    geom_point(aes(Dim2, Dim1), size = 4) +
    geom_text(aes(Dim2 + .5, Dim1 + .5, label = vector)) +
    coord_fixed(xlim = c(0, 8), ylim = c(0, 7)) +
    labs(title = "Cosine Similarity") +
    theme_minimal() +
    theme(plot.title = element_text(size = 12, hjust = .5, color = "red4"))

euc_plot <- example_vecs |> 
  ggplot() +
    # geom_segment(aes(Dim2, Dim1, xend = 0, yend = 0)) +
    geom_point(aes(Dim2, Dim1), size = 4) +
    geom_segment(
      aes(Dim2, Dim1, xend = lead(Dim2), yend = lead(Dim1)),
      linewidth = 1, color = "royalblue", arrow = arrow(ends = "both", length = unit(0.25, "cm"))
      ) +
    annotate("text", label = "4.24", 
             x = 5, y = 4.1, color = "royalblue", fontface = "bold") +
    geom_text(aes(Dim2 + .7, Dim1 + .5, label = vector)) +
    guides(color = "none") +
    coord_fixed(xlim = c(0, 8), ylim = c(0, 7)) +
    labs(title = "Euclidean Distance") +
    theme_minimal() +
    theme(plot.title = element_text(size = 12, hjust = .5, color = "royalblue"))

euc_plot + cos_plot
```

The cosine is convenient because it is always between -1 and 1: When the two vectors are pointing in a similar direction, the cosine is close to 1, and when they are pointing in a near-opposite direction (180°), the cosine is close to -1.

Looking at @fig-distmeasures, a question arises: Why should the angle be fixed at the zero point? What does the zero point have to do with anything? Indeed, cosine similarity works best when the vector space is centered at zero (or close to it). In other words, it works best when zero represents a medium level of each variable. This fact is sometimes taken for granted because, in practice, many vector spaces are already centered at zero. In the case of neural word embeddings, as we shall see, this property is guaranteed by the model's architecture. In the case of transformer neural networks however, the use of specialized models may be necessary to guarantee zero-centered embedding spaces.

The formula for calculating cosine similarity is as follows:

$$
Cosine(A,B) = \frac{A \cdot B}{|A||B|} = \frac{\sum _{i=1}^{n}  A_{i}B_{i}}{\sqrt {\sum _{i=1}^{n} A_{i}^2} \cdot \sqrt {\sum _{i=1}^{n} B_{i}^2}}
$$

In R, this can be translated to the following code:

```{r}
#| echo: true
#  - `x`: a numeric vector
#  - `y`: another numeric vector
cos_sim <- function(x, y){
  dot <- x %*% y
  normx <- sqrt(sum(x^2))
  normy <- sqrt(sum(y^2))
  as.vector( dot / (normx*normy) )
}
```

Readers comfortable with cosines may be satisfied with the explanation we have given so far. Nevertheless, many psychologists might find it helpful to consider the relationship between cosine similarity and a more familiar statistic that ranges between -1 and 1: the Pearson correlation coefficient. Cosine similarity measures the similarity between two _vectors_, while the correlation coefficient measures the similarity between two _variables_. Now just imagine our vectors as variables, with each dimension as an observation, as shown in @fig-cosineintuition A.

```{r}
#| label: fig-cosineintuition
#| echo: false
#| warning: false
#| fig-cap: Vectors as Variables
#| fig-height: 4
#| fig-width: 8
cosineintuition1 <- example_vecs |> 
  pivot_longer(Dim3:Dim1, names_to = "dimension") |> 
  pivot_wider(names_from = "vector") |> 
  ggplot(aes(`Vector A`, `Vector B`, label = dimension)) +
    geom_point(size = 3) +
    geom_text(aes(`Vector A` + .3, `Vector B` + .3)) +
    coord_equal(xlim = c(1, 8), ylim = c(4, 9)) +
    labs(title = "A") +
    theme_minimal()

cosineintuition2 <- example_vecs |> 
  pivot_longer(Dim3:Dim1, names_to = "dimension") |> 
  pivot_wider(names_from = "vector") |> 
  mutate(across(`Vector A`:`Vector B`, ~.x - mean(.x))) |> 
  ggplot(aes(`Vector A`, `Vector B`, label = dimension)) +
    geom_hline(yintercept = 0, color = "grey") +
    geom_vline(xintercept = 0, color = "grey") +
    geom_point(size = 3) +
    geom_text(aes(`Vector A` + .3, `Vector B` + .3)) +
    coord_equal(xlim = c(-4, 4), ylim = c(-3, 3)) +
    labs(title = "B") +
    theme_minimal()

cosineintuition1 + cosineintuition2
```

Now imagine centering those variables at zero, as shown in @fig-cosineintuition B. When seen like this, the correlation is the same as the cosine similarity. In other words, the correlation between two vectors is the same as the cosine similarity between them when the values of each vector are centered at zero.^[For a mathematical presentation of this relationship, see @oconnor_2012] Seeing cosine similarity as the non-centered version of correlation might provide additional intuition for why cosine similarity works best for vector spaces that are centered at zero.

### Dot Product

The dot product (sometimes called the inner product) of two vectors is similar to cosine similarity, with the exception that it gets larger as the vectors get further away from the origin (i.e. cosine similarity is the dot product of two normalized vectors).

```{r}
#| echo: true
#  - `x`: a numeric vector
#  - `y`: another numeric vector
dot_prod <- function(x, y){
  dot <- x %*% y
  as.vector(dot)
}
```

```{r}
#| label: fig-dotprod
#| echo: false
#| fig-cap: Dot Product vs. Cosine Similarity
#| fig-height: 6
#| fig-width: 8
library(patchwork)

vec_plot <- function(x, y, subtitle, angle){
  tibble(
    vec = c("Vector A", "Vector B"),
    x = x,
    y = y
  ) |> 
  ggplot() +
    geom_hline(yintercept = 0, linetype = 2, color = "grey") +
    geom_vline(xintercept = 0, linetype = 2, color = "grey") +
    geom_segment(aes(x, y, xend = 0, yend = 0)) +
    ggforce::geom_arc(
      aes(x0 = 0, y0 = 0, r = 1/3, start = pi/2, end = angle),
      linewidth = 2, color = "red4",
      data = tibble()
      ) +
    geom_point(aes(x, y, color = vec), size = 4) +
    geom_text(aes(x + .2*sign(-x)*if_else(y>0, 1, -1), y + .4, label = vec)) +
    guides(color = "none") +
    coord_fixed(xlim = c(-2, 2), ylim = c(-1, 3)) +
    labs(
      subtitle = subtitle,
      x = "", y = ""
      ) +
    theme_minimal() +
    theme(plot.subtitle = element_text(size = 12, hjust = .5, color = "red4", face = "bold"))
}

dot0_1 <- vec_plot(c(0, 1), c(1, 0), "Cosine Similarity = 0\nDot Product = 0", 0)
dot0_2 <- vec_plot(c(0, 1), c(2, 0), "Cosine Similarity = 0\nDot Product = 0", 0)
dot1_1 <- vec_plot(c(sqrt(2)/2, 1), c(sqrt(2)/2, 0), "Cosine Similarity = 0.7\nDot Product = 0.7", pi/4)
dot1_2 <- vec_plot(c(sqrt(2), 1), c(sqrt(2), 0), "Cosine Similarity = 0.7\nDot Product = 1.4", pi/4)
dot3_1 <- vec_plot(c(-sqrt(2)/2, 1), c(sqrt(2)/2, 0), "Cosine Similarity = -0.7\nDot Product = -0.7", -pi/4)
dot3_2 <- vec_plot(c(-sqrt(2), 1), c(sqrt(2), 0), "Cosine Similarity = -0.7\nDot Product = -1.4", -pi/4)

(dot0_1 | dot1_1 | dot3_1) /
(dot0_2 | dot1_2 | dot3_2)
```

As we shall see later in this guide, the dot product is used internally in some neural embedding models to quantify the relationships between embeddings [e.g. @mikolov_etal_2013b; @pennington_etal_2014], providing the basis for much geometrical interpretation of embeddings produced by these models [@ethayarajh_etal_2019]. We discuss the implications of this fact later in this guide, after introducing the models in question.

### Additive Analogies and Anchored Vectors

An oft-touted property of embeddings from some models is that they can be added to each other in order to arrive at new concepts. A prototypical example is shown in @fig-parallelogram. Subtracting the embedding of "man" from the embedding of "woman" results in the vector shown in blue. This vector represents the move from male to female gender. A vector between two embeddings is called an *anchored vector*. When the man-woman anchored vector is added to the embedding of "aunt", the result is very close to the embedding of "uncle". This property was first noted in word2vec [@mikolov_etal_2013], and GloVe [@pennington_etal_2014] was specifically designed with it in mind. 

```{r}
#| label: fig-parallelogram
#| echo: false
#| fig-cap: An Additive Analogy in Word Embeddings
#| apa-note: Word embeddings were acquired from the glove.twitter.27B.100d pretrained model and reduced to two dimensions with Principle Component Analysis (PCA).
#| fig-height: 4
#| fig-width: 6
analogy_words <- c("uncle", "aunt", "man", "woman")

analogy_pca <- predict(glove_pretrained, analogy_words, type = "embedding") |> 
  as_tibble(rownames = "word") |> 
  reduce_dimensionality(dim_1:dim_100, reduce_to = 2)

woman_vec <- analogy_pca[analogy_words == "woman",2:3]
man_vec <- analogy_pca[analogy_words == "man",2:3]
aunt_vec <- analogy_pca[analogy_words == "aunt",2:3]

gender <- woman_vec - man_vec
parentsibling <- woman_vec - aunt_vec

analogy_pca |> 
  ggplot(aes(PC1, PC2, label = word)) +
    geom_segment(
      aes(xend = PC1 - gender$PC1, 
          yend = PC2 - gender$PC2),
      arrow = arrow(type = "closed"),
      linewidth = 2,
      color = "navyblue",
      data = analogy_pca |> filter(word %in% c("woman", "aunt"))
      ) +
    geom_segment(
      aes(xend = PC1 - parentsibling$PC1, 
          yend = PC2 - parentsibling$PC2),
      arrow = arrow(type = "closed"),
      linewidth = 2,
      color = "red4",
      data = analogy_pca |> filter(word %in% c("woman", "man"))
      ) +
    geom_label() +
    coord_cartesian(xlim = c(-5, 5), ylim = c(-5, 5)) +
    theme_bw()
```

Additive analogical reasoning can help overcome a fundamental challenge of semantic embeddings. Consider the embeddings for "happy" and "sad". These may seem like opposites, but actually they are likely to be very close to each other in vector space because they both relate to emotional valence. This means that if we try to measure the happiness of words by comparing their embeddings to the embedding for "happy", we will actually be measuring the extent to which the words relate to emotion in general. The word "depression" might seem happier than the word "table", since depression is more emotion-related. This problem can be solved by using anchored vectors. Just as we created an anchored vector between "man" and "woman" to represent masculinity (as opposed to femininity) in @fig-parallelogram, we can create an anchored vector between "happy" and "sad" to represent happiness (as opposed to sadness). Such anchored vectors can be applied wherever necessary in embedding space. 

To measure a construct with an anchored vector, take the dot product of a text embedding with the anchored vector. This is the equivalent of "projecting" the embeddings down onto the scale between one end of the anchored vector and the other, as shown in @fig-projection.[^anchored-vecs-1] 

[^anchored-vecs-1]: For an intuitive explanation of why the dot product is equivalent to a projection, see 3blue1brown's video on the subject, at https://youtu.be/LyGKycYT2v0?si=86cfrN9DP9xw5HUx. Incidentally, the dot product with the anchored vector is also equivalent to the dot product with the positive embedding (e.g. "happy") minus the dot product with the negative vector (e.g. "sad").

```{r}
#| label: fig-projection
#| echo: false
#| warning: false
#| fig-cap: Projection of Embeddings Onto an Anchored Vector
#| apa-note: Word embeddings were acquired from the glove.twitter.27B.100d pretrained model and reduced to two dimensions with Principle Component Analysis (PCA).
#| fig-height: 4
#| fig-width: 6
example_words <- c(
  "ecstatic", "angry", "depressed", "frustrated", "pleased", "satisfied",
  "bored", "glad", "excited", "happy", "sad", "devastated"
  )

happiness_pca <- predict(glove_pretrained, example_words, type = "embedding") |> 
  as_tibble(rownames = "word") |> 
  reduce_dimensionality(dim_1:dim_100, reduce_to = 2)

happy_vec_pca <- happiness_pca |> 
  filter(word == "happy") |> 
  select(-word) |> 
  unlist()

sad_vec_pca <- happiness_pca |> 
  filter(word == "sad") |> 
  select(-word) |> 
  unlist()
  
happiness_pca |> 
  bind_cols(
    project_points_onto_line(
      happy_vec_pca, 
      sad_vec_pca, 
      happiness_pca |> select(-word)
      )
    ) |> 
  ggplot(
    aes(
      PC1, PC2, 
      xend = X1, yend = X2,
      label = word, 
      fill = word %in% c("happy", "sad")
      )
    ) +
    geom_segment(
      aes(xend = lead(PC1), yend = lead(PC2)),
      data = happiness_pca |> filter(word %in% c("happy", "sad"))
      ) +
    geom_segment(color = "skyblue2", linetype = 2) +
    geom_point(aes(X1, X2), color = "skyblue3") +
    geom_label() +
    scale_fill_manual(values = c("white", "skyblue2")) +
    coord_equal(xlim = c(-5, 5), ylim = c(-4, 4)) +
    guides(fill = "none") +
    theme_bw()
```

By projecting each embedding down onto the anchored vector between happy and sad, we create a scale from happy to sad.[^anchored-vecs-2] This is sometimes referred to as **semantic projection** [@grand_etal_2022]. 

[^anchored-vecs-2]: Taking the dot product with an anchored vector yields an unstandardized version of this scale. For a standardized version of this function, in which "sad" is at 0 and "happy" is at 1 on the scale, use the `anchored_sim()` function included in our Github repo at https://github.com/rimonim/embeddings_tutorial/blob/master/embedding_scripts.R. 

## The Distributional Hypothesis

How do language models embed tokens in a semantic embedding space? By what metric is a word considered similar to or different from another? In answering this question, modern techniques rely on a further assumption, known as the distributional hypothesis. According to this assumption, tokens carry similar meanings inasmuch as they occur in similar contexts. For example, consider the following two sentences from the paper that introduced the distributional hypothesis, @harris_1954 [emphasis added].

> "The formation of new *utterances* in the *language* is therefore based on the distributional relations as changeably perceived by the *speakers*-among the parts of the previously heard *utterances*."

> "The correlation between *language* and *meaning* is much greater when we consider connected discourse. "

Even if we have no idea what "utterances" or "meaning" are, we can learn from these sentences that they must be related somehow, since they both appear together with the word "language." The more sentences we observe, the more sure we can be about the distributional patterns (i.e. which words tend to have similar words nearby). Words that tend to have very similar words nearby are likely to be similar in meaning, while words that have very different contexts are probably unrelated. Algorithms that learn the meanings of tokens (or at least the relations between their meanings) from these patterns of co-occurrence are called Distributional Semantic Models (DSMs).

#### A Common Misconception

Two words are NOT considered similar based on whether they appear together often. Words are similar when they tend to appear in similar *contexts*. For example, "fridge" and "refrigerator" almost never appear together in the same sentence, but they do tend to appear next to similar groupings of other words (e.g. "food," "cold," etc.).

# Word Embeddings: word2vec, GloVe, and FastText

## Word2vec 

word2vec was first introduced by @mikolov_etal_2013b and was refined by @mikolov_etal_2013c. They proposed a few variations on a simple neural network[^word2vec-1] that learns the relationships between words and contexts. Here we describe the most commonly used variation---continuous skip-gram with negative sampling. 

[^word2vec-1]:    Some experts consider word2vec too simple to be called a neural network. The precise definition of "neural network" is beyond the scope of this guide; word2vec could just as productively be thought of as an advanced form of logistic regression.

Imagine training the model on the following sentence: "Coding is frustrating." The skip-gram training dataset would have one column for the input word, and another column for words from its immediate context. The technique is called "continuous" because it slides a context window along the training text, considering each word as input and the words immediately around it as context (e.g. 10 before and 10 after), as shown in @tbl-skipgram1. 

```{r}
#| label: tbl-skipgram1
#| echo: false
#| tbl-cap: Example Skipgram Training Set
text1 <- "coding is frustrating"

skipgram_df <- function(input, context){
  context <- tokens(context) |> 
    tokens_remove(input) |> 
    as.character()
  tibble(word = input, context = context)
}

bind_rows(lapply(str_split_1(text1, " "), skipgram_df, text1)) |> knitr::kable()
```

The negative sampling method adds more rows to the training set, this time from words and contexts that do not go together, drawn at random from other parts of the corpus. A third column indicates whether the pair of words are in fact neighbors or not, as shown in @tbl-skipgram2.

```{r}
#| label: tbl-skipgram2
#| echo: false
#| tbl-cap: Example Skipgram Training Set With Negative Sampling
neg_text <- "happy olive"

bind_rows(
  lapply(str_split_1(text1, " ")[1:2], skipgram_df, text1),
  lapply(str_split_1(text1, " ")[1:2], skipgram_df, neg_text)
  ) |> 
  mutate(
    neighbors = rep(c(1,0), each = 4)
    ) |> 
  knitr::kable()
```

The word2vec model takes the first two columns as input and tries to predict whether the two words are neighbors or not [see @tbl-skipgram2]. It does this by learning two separate sets of embeddings: word embeddings and context embeddings. For each row of the training set, the model looks up the embedding for the target word and the embedding for the context word, and computes the dot product between the two vectors. If the dot product is large (i.e. the word embedding and the context embedding are very similar), the model predicts that the two words are likely to be real neighbors. If the dot product is small, the model predicts that the two words were probably sampled at random.[^word2vec-3] During training, the model learns which word embeddings and context embeddings will do best at this binary prediction task.

[^word2vec-3]: To learn why models like word2vec use dot products instead of cosine similarity, see "Interpreting Word Embeddings" below.

Note that word2vec (as well as fastText and GloVe) give each word two embeddings: one for when the word is the target and another for when it is the context [@goldberg_levy_2014]. This overcomes two important challenges of semantic embedding:

1.  **A Nuance of the Distributional Hypothesis.** Recall the case of "fridge" and "refrigerator", which almost never appear together in the same sentence, but do tend to appear next to similar groupings of other words. Earlier word embedding models such as latent semantic analysis [@deerwester_etal_1990], which are based directly on broad patterns of covariance in word frequencies, will pick up on the fact that "fridge" and "refrigerator" are negatively correlated and push them further apart than they should be. Word2vec, on the other hand, can learn a *context embedding* for "refrigerator" that is not so close to the *word embedding* for "fridge", even when the word embeddings of the two words are very close. This allows word2vec to recognize that "refrigerator" and "fridge" tend to appear in similar contexts, but are unlikely to appear together.
2.  **Associative Asymmetry.** The cosine similarity between two word embeddings gives the best estimate of *conceptual similarity* [@torabi-asr_etal_2018]. This is because conceptual similarity is not the same as association in language (or in the mind). In fact, psycholinguists have long known that human associations between two words are asymmetric. For example, people prompted with "leopard" are much more likely to think of "tiger" than people prompted with "tiger" are to think of "leopard" [@tversky_gati_1982]. These sorts of associative connections are closely tied to probabilities of co-occurrence in language and are therefore much better represented by the cosine similarity (or even the dot product) between a word embedding and a context embedding [@torabi-asr_etal_2018]. Thus the association between "leopard" and "tiger" would be represented by the similarity between the *word embedding* of "leopard" and the *context embedding* of "tiger", allowing for the asymmetry observed in mental associations.[^word2vec-4] Since older models like latent semantic analysis only produce one embedding per word, they cannot capture this asymmetry.

[^word2vec-4]:    To the best of our knowledge, pretrained context embeddings are not available online. For associative (rather than conceptual) relationships between words, we recommend training a new model. A guide on training a custom GloVe model in R can be found at https://quanteda.io/articles/pkgdown/replication/text2vec.html.

Some pretrained word2vec models can be easily downloaded from the Internet[^6]. Because these models are trained on very large datasets and are already known to perform well, it not necessary to train a new word2vec from scratch.[^word2vec-6]

[^6]: See for example https://github.com/maxoodf/word2vec?tab=readme-ov-file#basic-usage and https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained

[^word2vec-6]:    Training custom word embeddings may be useful in certain specialized cases. For example, a researcher who is interested in quantifying differences in individual word use between multiple large groups of text might train a GloVe model on texts written by conservatives and another on texts written by liberals, and demonstrate that the word "skirt" is closer to the word "woman" in conservative language than it is in liberal language. A guide on training custom GloVe models in R can be found at https://quanteda.io/articles/pkgdown/replication/text2vec.html.

A downloaded pretrained model (generally a .bin file), can be opened in R with the *word2vec* package.[^8] In the example code below, we use a model trained on Google news, which uses 300-dimensional embeddings.[^9]

[^8]: Available at https://cran.r-project.org/web/packages/word2vec/readme/README.html

[^9]: Available at https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained/download?datasetVersionNumber=1

```{r}
#| eval: false
#| echo: true
library(word2vec)

# model file path
word2vec_mod <- "data/GoogleNews-vectors-negative300.bin"

# open model
word2vec_mod <- read.word2vec(file = word2vec_mod, normalize = TRUE)
```

To find embeddings of specific words using the *word2vec* package, use `predict(word2vec_mod, c("word1", "word2"), type = "embedding")`. To get embeddings for full documents, average the embeddings of the words in the document. In our github repo, we provide the `textstat_embedding` to compute document embeddings directly from a document-feature matrix (DFM).^[https://github.com/rimonim/embeddings_tutorial/blob/master/embedding_scripts.R]

#### Word2vec: Key Takeaways

By distinguishing between target and context words, word2vec allows greater fidelity to the distributional hypothesis. Since it is not based on counts, it also avoids problems with non-linear relationships and irregular distributional properties of word frequencies [see @baayen_2001]. Word2vec, like other word embedding models, is also efficient for use on large datasets, since pretrained embeddings can be looked up for each word. The main disadvantage of word2vec is that it assumes that each word has only one meaning. This means that it has trouble with words that can mean more than one thing (e.g. deep learning *model* vs. fashion *model*). Word2vec will learn the average of these meanings. Furthermore, word2vec is primarily effective in English. This is because English words are generally spelled the same no matter where they are in a sentence. Word2vec does not work as well for languages that have more prefixes, suffixes, conjugations, etc., since it has to relearn the meaning for each form of the word. Finally, word2vec is made less appealing to researchers due to the relative paucity of pretrained models available online. 

## GloVe

Word2vec produces spectacularly rich and reliable vector embeddings, but their reliance on randomly sampled pairs of words and contexts makes them somewhat noisy and overly sensitive to frequent tokens. The developers of word2vec managed to fix these problems by strategically filtering the training dataset. @pennington_etal_2014, in contrast, developed a refactored model that corrects for frequency internally: Global Vectors (GloVe) is designed on the same principles as word2vec, but it is computed from global patterns of co-occurrence rather than individual examples.

Although GloVe uses a different method of training, the embeddings it generates are very similar to those generated by word2vec. Because GloVe embeddings are so similar to word2vec embeddings, we will not go into detail here about the way the GloVe algorithm works. Nevertheless, GloVe does have one very important advantage over word2vec: Better pretrained models are available online. Whereas the most easily available word2vec model is trained on news articles, the GloVe website[^glove-1] offers models trained on social media (`glove.twitter.27B.zip`) and on large portions of the Internet [@rana_2010]. These models generalize better to social media texts (since they were trained on similar texts) and are likely to have richer representations of emotional or social content, since more examples of that content appear on social media than in the news or on Wikipedia.[^glove-2]

[^glove-1]:   https://nlp.stanford.edu/projects/glove/

[^glove-2]:   Another notable difference between GloVe and word2vec is that the GLoVe averages the word embeddings and context embeddings rather than using only the word embeddings as word2vec does. This makes GloVe embeddings slightly better at representing overall meaning, but may blur the distinction between conceptual similarity and mental/linguistic association [@torabi-asr_etal_2018].

Since the pretrained GloVe models are available in .txt format, a wrapper package is not required to use them in R. The following code loads a downloaded pretrained model for use.

```{r}
#| echo: true
#| eval: true
path_to_glove <- "~/Projects/ds4psych/data/glove/glove.twitter.27B.100d.txt"
glove_pretrained <- load_embeddings_txt(path_to_glove)
```

#### GloVe: Key Takeaways

GloVe's efficient training procedure has made it a popular choice for researchers who wish to train their own models. GloVe is also appealing for researchers looking for high quality, psychologically sensitive, pretrained models, since a number of such models are available for download online. Like word2vec, GloVe's primary downsides are that it relies on word-level meaning and that it has limited applicability to languages other than English.

## FastText

FastText [@bojanowski_etal_2017] is a specialized version of word2vec, designed to work with languages in which words take different forms depending on their grammatical place. Rather than learning a word embedding and a context embedding for each full word (e.g. "quantify" and "quantification" each get their own embedding), fastText learns a vector for each string of characters within a word (such a string is sometimes referred to as a shingle). For example, "quantify" might be broken up into "quant", "uanti", "antif", and "ntify". But it doesn't treat each shingle as its own word. Rather, it trains on words just like word2vec and GloVe, but makes sure that the embedding of a word is equal to the _sum_ of all of the shingle vectors inside it. 

This approach is mostly unnecessary for English, where words are generally spelled the same wherever they appear. But for more morphologically rich languages like Hebrew, Arabic, French, or Finnish, fastText works much better than word2vec and GloVe. This is because there might not be enough data for word2vec and GloVe to learn reliable representations of every form of every word, especially rare forms. FastText, on the other hand, can focus on the important subcomponents of the words that stay the same across different forms. This way it can learn rich representations even of rare forms of a word that don't appear in the training dataset (e.g. it could quantify the meaning of "morphologically" even if it were only trained on "morphology", "morpheme", and "chronologically"). 

After downloading a pretrained model from @grave_etal_2018, you can use fastText in R through the *fastTextR* package.[^fasttext-1] *FastTextR* includes a dedicated function for obtaining full text embeddings, `ft_sentence_vectors()`.

[^fasttext-1]:   Available at https://cran.r-project.org/web/packages/fastTextR/vignettes/Word_representations.html

```{r}
#| eval: false
#| echo: true

library(fastTextR)

# example French texts
fr_words <- c("Français", "française")
fr_texts <- c("Ce qui n'est pas clair n'est pas français.", 
              "Ce que l'on conçoit bien s'énonce clairement")

# load pretrained model from file
fr_model <- ft_load("data/cc.fr.300.bin")

# get word embeddings
word_vecs <- ft_word_vectors(fr_model, fr_words)

# get text embeddings
text_vecs <- ft_sentence_vectors(fr_model, fr_texts)
```

#### FastText: Key Takeaways

Relative to word2vec and GloVe, fastText performs much better on morphologically rich languages. Pretrained models for 157 languages are available online.^[https://fasttext.cc/docs/en/crawl-vectors.html] Furthermore, fastText has increased performance for rare words, and is able to infer embeddings for words that were not in the training data. The cost of this gain is that fastText models are more complex, resulting in larger files to download when using pretrained models. The added complexity may also increase the risk of overfitting. 

## Interpreting Word Embeddings

Advanced word embedding algorithms like word2vec, GloVe, and fastText use the dot product of two word embeddings to measure how likely the probability that the words occur together. Vectors that are farther away from the origin will result in very positive or very negative dot products, making the model more confident in the pair of words either being neighbors or not. This means that the distance of a word embedding from the origin (also called the norm or magnitude) is proportional to the informativeness of the word [@schakel_wilson_2015; @oyama_etal_2023]. For example, the word "the" has a very low magnitude because it does not indicate a specific context, while the word "psychology" has a very high magnitude because its use is associated with a very specific context. Therefore, the magnitude of the embedding measures how representative it is of certain contexts as opposed to others. 

This is the reason why an accurate embedding of a full text can be obtained by averaging the embeddings of each of its words. One may assume that averaging word embeddings will lead to overvaluing common words, like "the" and "I", which appear more frequently but are not very informative about the text's meaning. In fact, however, the magnitude of a word embedding is smaller for common words, which means that common words have less impact on the average [@ethayarajh_etal_2019].

Once average embeddings are computed, most researchers use cosine similarity to assess the relationships between embeddings. The cosine similarity measures only the meanings of the two embeddings, while ignoring how specific they are to those meanings. If the specificity of texts to a construct of interest is important to the analysis, the dot product may be more appropriate than the cosine similarity. For this reason, the dot product may sometimes be optimal for analyzing texts with decontextualized embeddings (See Appendix A).

# Contextualization With Large Language Models

The models we have discussed so far represent the meaning of each token as a point in multidimensional space: a word embedding. Word embeddings generated by models like word2vec or GloVe are often referred to as **decontextualized embeddings**. This name may be confusing, since the purpose of these models is to associate tokens with the contexts in which they tend to appear. Nevertheless, these models are decontextualized in that they can allow only one context representation per token---they therefore represent the average of the contexts in which a token appears throughout the training corpus. For example, consider the following uses of the token "short".

> My dad is very _short_.

> My blender broke because of a _short_ circuit.

> That video was anything but _short_.

> I can't pay because I'm _short_ on cash at the moment.

Any speaker of English can easily see that the word "short" means something different in each one of these examples. But because word2vec and similar models are trained to predict the context based on only a single word at a time, their representation of the word _short_ will only capture that word's average meaning. 

How can we move beyond the average meaning and capture the different meanings words take on in different contexts? Readers may be familiar with Large Language Models (LLMs) like ChatGPT and Claude. At their core, much of what these models do is exactly this: They find the intricate relationships between tokens in a text and use them to develop a new understanding of what these tokens mean in the particular context of that text. For this reason, embeddings produced by these models are often referred to as **contextualized embeddings**.

The core of all modern LLMs is a model called the _transformer_ [@vaswani_etal_2017]. We will not cover exactly how transformers work. For the purposes of this guide, we offer only the broad overview necessary to make proper use of them in research: In processing a text, transformers first convert all the words into word embeddings, as word2vec or GloVe would do. At the start, these word embeddings represent the average meaning of each word. The transformer then estimates how each word in the text might be relevant for better understanding the meaning of the other words. For example, if "circuit" appears immediately following "short", the embedding of "short" should probably be tweaked. Once it has identified this connection, the transformer computes what "circuit" should add to a word that it is associated with, moving the "short" embedding closer to embeddings for electrical concepts. A full LLM has many _layers_. In each layer, the LLM identifies more connections between embeddings and shifts the embeddings in the vector space to add more nuance to their representations. When it gets to the final layer, the LLM uses the enriched embeddings of the words in the text for whatever task it was trained to do (e.g. predicting what the next word will be, or identifying whether the text is spam or not). 

In order to extract an LLM's rich, contextualized embeddings, researchers can run it on a text and stop it before it finishes predicting the next word (or anything else it was trained to do). This way, we can read the LLM's mind, capturing all of the rich associations it has with the text.

## Hugging Face and the *text* Package

Leading commercial LLMs like GPT-4 are hidden behind APIs so that their inner workings are kept secret. Researchers therefore cannot access the embeddings of these high profile models. Nevertheless, plenty of models that are almost as good are open source and easily accessible through Hugging Face Transformers.[^text-1] Any text-based transformer model can be accessed in R using the *text* package [@kjell_etal_2021].[^text-2] 

[^text-1]:    https://huggingface.co/docs/transformers/index

[^text-2]:    The *text* package runs Python code behind the scenes, so new users will have to set up a Python environment for it to run properly. For instructions on how to do this, see https://www.r-text.org/articles/huggingface_in_r_extended_installation_guide.html. 

The key tool provided by the *text* package is `textEmbed()`. This function takes in raw texts and generates contextualized embeddings using a specified open source model. Here we use `BAAI/bge-base-en-v1.5` [@xiao_etal_2023], a relatively lightweight model that nonetheless achieves state-of-the-art performance on the MTEB benchmark, a leading resource for evaluating text embedding models [@muennighoff_etal_2022].[^text-3] By default, `textEmbed()` creates the full text embedding by averaging the contextualized embeddings of each token in the text. However, `BAAI/bge-base-en-v1.5` is trained to provide optimal text embeddings in the final layer embedding of the `[CLS]` token in particular.[^text-4] When using `BAAI/bge-base-en-v1.5` to generate embeddings, it is therefore necessary to specify `tokens_select = "[CLS]"` and `layers = -1`. This has the added benefit of speeding up the computation.

[^text-3]:    The current MTEB leaderboard can be found at https://huggingface.co/spaces/mteb/leaderboard.

[^text-4]:    The `[CLS]` token is a special token that models based on BERT [@devlin_etal_2019] add to each text. Because the `[CLS]` token does not have a "real" meaning, but rather is inserted at the same place in every text, its contextualized embedding represents the gist of each text as a whole. In training, traditional BERT models use the contextualized embedding of the `[CLS]` token to predict whether a given text does or does not come after the input text. `BAAI/bge-base-en-v1.5` has a somewhat different training paradigm, but it maintains the conventional `[CLS]` token.

```{r}
#| eval: false
#| echo: true
library(text)

example_texts <- c(
  "This is one text.",
  "This is another."
  )

# run model
example_texts_bge <- textEmbed(
  example_texts,
  model = "BAAI/bge-base-en-v1.5", # model name
  tokens_select = "[CLS]", # select only [CLS] token embedding
  layers = -1  # last layer
)

# extract dataframe of embeddings
example_texts_bge <- example_texts_bge$texts[[1]]
```

### Managing Computational Load

Running large neural networks on a personal computer can be time consuming. The following are some ways to lessen the computational load when calling `textEmbed()`:

-   Use a smaller model. Hugging Face model pages generally state how many parameters the model has. This is a good indication of how much computational capacity the model needs. For example, consider using `distilbert-base-uncased` (67M params) or `albert-base-v2` (11.8M params) instead of `bert-base-uncased` (110M params).
-   Only retrieve one layer at a time (e.g. `layers = -2`).
-   If individual token embeddings are not needed, set `keep_token_embeddings = FALSE`.
-   To avoid aggregation costs, only querry individual token embeddings (e.g. `tokens_select = "[CLS]"`).
-   Run the model on the GPU with `device = 'gpu'` (not available for Apple M1 and M2 chips).
-   Break up the dataset into smaller groups of texts, run each on its own, and join them back together afterward.

Before running `textEmbed()` on a full dataset, always try running it on two or three texts first. This will give a sense of how long the full computation will take, and will ensure that the output is as expected.

### Choosing the Right Model

Part of the beauty of LLMs is their ability to generalize---most popular models today are trained on enormous datasets with a wide variety of content, and even smaller models perform more than well enough to capture straightforward psychological concepts like emotional valence [@kjell_etal_2022]. Nevertheless, there is a notable major limitation of LLM embeddings. Word2vec, GloVe, and related models have architectures that guarantee vector spaces with consistent geometric properties, allowing researchers to confidently compute averages between vectors and interpret linear directions as encoding unique semantic meanings. In contrast, the ways that LLMs organize meaning in their embedding spaces are not well understood and may not always lend themselves to simple measures like those described in this guide [@cai_etal_2021; @li_etal_2020; @reif_etal_2019; zhou_etal_2022]. 

Recently, researchers have tried to fix this problem by creating models that are trained explicitly to produce high quality embeddings. These models encourage embeddings to spread out evenly in the embedding space, or explicitly optimize for reliable cosine similarity metrics. Many of these models can be found on the leaderboard for the MTEB benchmark [@muennighoff_etal_2022].[^text-3]

## Additive Analogies in Contextualized Embeddings

Note in @fig-parallelogram that the analogical property relies on the magnitude of the vectors---if some vectors were shorter or longer than necessary, the parallelogram would not fit. This means that analogical reasoning may not be applicable to LLM embeddings, which are often organized in nonlinear patterns [@cai_etal_2021; @ethayarajh_2019; @gao_etal_2019]. Even specialized models like sentence transformers are generally not designed with the additive analogical property in mind [@reimers_gurevych_2019]. Even though the geometrically motivated methods described in this guide work fairly well in LLM embeddings, there is room for improvement in this area.[^analogies-1]

[^analogies-1]: Some promising methods have been suggested for extracting geometrically regular embeddings out of non-specialized LLMs. For example, averaging the last two layers of the model seems to help [@li_etal_2020]. Taking a different approach, @ethayarajh_2019 created static word embeddings from an LLM by running it on a large corpus and taking the set of each word’s contextualized representations from all the places it appears in the corpus. The loadings of the first principal component of this set represent the dimensions along which the meaning of the word changes across different contexts. These loadings can themselves be used as a vector embedding which can out-perform GloVe and FastText embeddings on many word vector benchmarks, including analogy solving. This approach worked best for embeddings from the early layers of the LLM.

## Contextualized Embeddings: Key Takeaways

Contextualized embeddings by definition can represent multiple senses of each word. This is especially important for analyzing shorter texts---since the sample of words is small, the need to correctly characterize each one is greater. Contextualized embeddings are also sensitive to word order and negation (e.g. LLMs can tell the difference between "he's funny but not smart" and "he's smart but not funny"). Despite these advantages, LLM embeddings come with two downsides. First, they are computationally expensive to generate, making them unfeasible for application to large datasets. Second, unlike simple word embedding models, LLMs may sometimes organize embeddings in nonlinear patterns. For example, GPT models tend to arrange their embeddings in a spiral [@cai_etal_2021]. This is presumably why BERT text embeddings perform worse than word2vec and GloVe when analyzed using cosine similarity [@reimers_gurevych_2019]. Specialized models such as those listed on the leaderboard for the MTEB benchmark [@muennighoff_etal_2022] can help with this problem, but are unlikely to solve it entirely.

# Representing Psychological Constructs: DDR, CCR, and CAV

With modern vector-based methods, any language-based psychological measure can be represented as a vector. Psychology has used language-based measures like dictionaries and questionnaires for over a century. To smoothly continue this existing research in the age of vector spaces, we introduce three methods for quantifying existing psychological constructs in high-dimensional vector space: Distributed Dictionary Representation (DDR), Contextualized Construct Representation (CCR), and Correlational Anchored Vectors (CAV). As we introduce each method, we use it to quantify an example psychological construct: either self-referential processing or the related concept of locus of control---the feeling that one has control over one's own life [@rotter_1966]. Finally, we use each metric to test for significant differences between posts in the Reddit r/depression community (N = 995) and posts in the r/TodayIamHappy community (N = 781). As their names suggest, the former is a support group for users struggling with depression, while the latter is a forum for sharing happy experiences. This small sample of posts, which we name `reddit_emotion`, was collected from the Pushshift Reddit archives [@baumgartner_etal_2020] and is used here purely for the sake of demonstration.

## Distributed Dictionary Representation (DDR)

We begin with a straightforward sort of psychological measure---the dictionary (also known as a word list or lexicon). A dictionary is a list of words (or other units of text, generally called "tokens") associated with a given psychological or other construct. For example, a dictionary for depression might include words like “sleepy” and “down.” Psychological researchers have been constructing, validating, and publicizing dictionaries for decades. But these dictionaries are generally designed to be used by counting dictionary words in a text---How can they be applied to a vector-based analysis? @garten_etal_2018 propose a simple solution: Generate word embeddings for each word in the dictionary, and average them together to create a single DDR. The dictionary construct can then be measured by comparing text embeddings to the DDR.

DDR cannot entirely replace word counts; for linguistic concepts like conjunctions or use of the passive voice, word counts may still be psychometrically optimal. But DDR is ideal for studies of abstract constructs like emotions, that refer to the general gist of a text rather than particular words. The rich representation of word embeddings allows DDR to capture even the subtlest associations between words and constructs, and to precisely reflect the _extent_ to which each word is associated with each construct. It can do this even for texts that do not contain any dictionary words. Because embeddings are continuous and already calibrated to the probabilities of word use in language, DDR also avoids the difficult statistical problems that arise due to the strange distributions of word counts [see @teitelbaum_simchon_2024, chap. 16].

@garten_etal_2018 found that DDR works best with smaller dictionaries of only the words most directly connected to the construct being measured (around 30 words worked best in their experiments). Word embeddings work by overvaluing informative words---a desirable property for raw texts, in which uninformative words tend to be very frequent.[^ddr-1] But dictionaries only include one of each word. In longer dictionaries with more infrequent, tangentially connected words, averaging word embeddings will therefore overvalue those infrequent words and skew the DDR. This can be fixed with Garten et al.'s method of picking out only the most informative words. Alternatively, it could be fixed by measuring the frequency of each dictionary word in a corpus and weighting the average embedding by that frequency. This method is actually more consistent with the way most dictionaries are validated, by counting the frequencies of dictionary words in text (for preliminary empirical validation of this method, see Appendix A).

[^ddr-1]: For more information on this property, see "Interpreting Advanced Word Embeddings" above. Note that this property emerges naturally from the way decontextualized models like word2vec and GloVe are trained, and therefore may not hold true for contextualized embeddings.

In the example code below, we create a DDR for self-referential processing using the Linguistic Inquiry and Word Count (LIWC) dictionary of first person singular pronouns [@boyd_etal_2022]. Since LIWC dictionaries include stems in addition to full words, we first expand the dictionary by running the LIWC software on each unique word in `reddit_emotion` (full code is available in our Github repo). This results in a character vector of dictionary words, called `i_dict` in the code below. To correct for word informativeness, we weight the dictionary word embeddings by their frequency in `reddit_emotion`.

```{r}
#| label: DDR_setup
#| echo: false
#| include: false
reddit_emotion <- read_csv("example_data/reddit_emotion.csv")

reddit_emotion_corp <- corpus(
  reddit_emotion, 
  docid_field = "doc_id", 
  text_field = "text"
  )

reddit_emotion_dfm <- reddit_emotion_corp |> 
  tokens(remove_punct = TRUE, remove_url = TRUE) |> 
  dfm()

reddit_emotion_glove <- reddit_emotion_dfm |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)

# 3. Load and Embed Dictionaries

  # Format and export tokens to run through LIWC software
  reddit_emotion_tokens <- featnames(reddit_emotion_dfm)
  write_csv(tibble(text = reddit_emotion_tokens), "~/Downloads/reddit_emotion_tokens.csv")

  # Load expanded LIWC dictionaries
  reddit_emotion_tokens <- read_csv("~/Downloads/reddit_emotion_tokens_LIWC.csv") |> drop_na()
  
  i_dict <- reddit_emotion_tokens$text[reddit_emotion_tokens$i > 0]
  shehe_they_dict <- reddit_emotion_tokens$text[reddit_emotion_tokens$shehe > 0 | reddit_emotion_tokens$they > 0]

  # Remove tokens that are not in embedding model
  i_dict <- i_dict[i_dict %in% rownames(glove_pretrained)]
  shehe_they_dict <- shehe_they_dict[shehe_they_dict %in% rownames(glove_pretrained)]
  
  # Embed dictionary words
  i_dict_glove <- i_dict |> 
    tokens() |> 
    dfm() |> 
    textstat_embedding(glove_pretrained) |> 
    select(-doc_id) |> 
    as.matrix()
  rownames(i_dict_glove) <- i_dict
  
  shehe_they_dict_glove <- shehe_they_dict |> 
    tokens() |> 
    dfm() |> 
    textstat_embedding(glove_pretrained) |> 
    select(-doc_id) |> 
    as.matrix()
  rownames(shehe_they_dict_glove) <- shehe_they_dict
  
# 4. Get Dictionary Word Frequencies

i_dict_freqs <- reddit_emotion_dfm |> 
  dfm_keep(i_dict) |> 
  quanteda.textstats::textstat_frequency() |> 
  pull(frequency, name = feature)

shehe_they_dict_freqs <- reddit_emotion_dfm |> 
  dfm_keep(shehe_they_dict) |> 
  quanteda.textstats::textstat_frequency() |> 
  pull(frequency, name = feature)
  
# 5. Compute Anchored Vector

  # Aggregate dictionary embeddings with weighted average
  i_DDR <- apply(i_dict_glove, 2, weighted.mean, w = i_dict_freqs)
  shehe_they_DDR <- apply(shehe_they_dict_glove, 2, weighted.mean, w = shehe_they_dict_freqs)

  # Anchored vector
  selfref_anchored <- i_DDR - shehe_they_DDR
```

```{r}
#| eval: false
#| echo: true

# Embed Dictionary Words
i_dict_glove <- i_dict |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  as.matrix()
rownames(i_dict_glove) <- i_dict
  
# Get Dictionary Word Frequencies
i_dict_freqs <- reddit_emotion_dfm |> 
  dfm_keep(i_dict) |> 
  quanteda.textstats::textstat_frequency() |> 
  pull(frequency, name = feature)
  
# Aggregate dictionary embeddings with weighted average
i_DDR <- apply(i_dict_glove, 2, weighted.mean, w = i_dict_freqs)
```

To measure the similarity of Reddit posts to this theoretical construct, we now need only to compute the cosine similarity between the DDR and the embedding of each text. Beginning with a DFM of `reddit_emotion`, the following code performs this computation and conducts a t-test for differences between r/depression and r/TodayIamHappy.

```{r}
#| echo: true
# Embed Texts of Interest
reddit_emotion_glove <- reddit_emotion_dfm |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)
  
# Calculate Distance Metrics
reddit_emotion_scores <- reddit_emotion_glove |> 
  rowwise() |> 
  mutate(selfref = cos_sim(c_across(V1:V100), i_DDR)) |> 
  pull(selfref)

# Test Hypothesis
test_DDR <- t.test(
  x = reddit_emotion_scores[reddit_emotion$subreddit=="depression"],
  y = reddit_emotion_scores[reddit_emotion$subreddit=="TodayIamHappy"]
  )
test_DDR$p.value
test_DDR$estimate
```

We find that posts in r/depression have more self-referential language (p < .001). This is consistent with existing research indicating that the use of self-referential language increases during depressive episodes [@rude_etal_2004]. 

### DDR: Key Takeaways

DDR produces richer, more robust construct representation than traditional word counting methods [cf. @boyd_etal_2022]. Since DDR representations are continuous, they also avoid statistical problems associated with the distribution of word counts [see @baayen_2001]. Another advantage of DDR over dictionary-based word counts is that DDR enables word-by-word analysis of text. It is not very informative to count how many dictionary words are in each word (it will either be one or zero), but the embedding of each word can be compared to the DDR---how close are they in the vector space? This allows researchers to see how a construct spreads out within a single text. Relatedly, DDR only needs a dictionary that captures the essence of the construct being measured. For many constructs, this could be only a few words. This property can allow researchers to create rich construct representations for novel constructs without investing in the development of a large dictionary. One downside of DDR is that it can implicitly encode associated constructs. For example, if surprised texts tend to have positive valence in the data used to train the word embedding model, the DDR for surprise may embed some positive valence as well. This can be remedied by constructing a DDR for positive valence too, and using it as a statistical control when testing hypotheses. Lastly, DDR may not be appropriate for linguistic measures. Word embeddings encode the general gist of a text, whereas constructs like passive voice or pronoun use refer to specific words. Constructs that are concretely tied to words or grammatical forms in the text should therefore be analyzed using standard word counting methods [see @boyd_etal_2022]. 

## Contextualized Construct Representation (CCR)

Dictionaries are not the only validated psychological measures that we can apply using embeddings. With contextualized embeddings, we can extract the gist of any text and compare it to that of any other text. @atari_etal_2023 propose to do this with the most popular form of psychometric scale: the questionnaire. Psychologists have been using questionnaires to measure things for over a century, and tens of thousands of validated questionnaires are now available online. The LLM embedding of a questionnaire is referred to as a CCR.

We can use CCR to measure internal locus of control by using the questionnaire introduced by @rotter_1966. The questionnaire includes items measuring an internal locus of control (e.g. "What happens to me is my own doing"), and items measuring an external locus of control (e.g. "Sometimes I feel that I don't have enough control over the direction my life is taking"). In the code below, we compute a CCR for the internal locus of control items.^[Many questionnaires include reverse-coded items (e.g. "I often feel happy" on a depression questionnaire). The easiest way to deal with these is to manually add negations to flip their meaning (e.g. "I _do not_ often feel happy").]

```{r}
#| eval: false
#| echo: true
# Embed Questionnaire Items
internal_bge <- text::textEmbed(
  internal_items,
  model = "BAAI/bge-base-en-v1.5", # model name
  tokens_select = "[CLS]", # select only [CLS] token embedding
  layers = -1,  # last layer
  dim_name = FALSE,
  keep_token_embeddings = FALSE
)
internal_bge <- internal_bge$texts[[1]]
  
# Aggregate Questionnaire Embeddings
internal_bge <- apply(as.matrix(internal_bge), 2, mean)
```

The ubiquity of questionnaire measures in psychological research makes CCR a powerful tool. Nevertheless, researchers should keep in mind what is being represented by the CCR. Embeddings capture the overall feel of a text, including its tone and dialect. With CCR, we are comparing the feel of a questionnaire written by academics to the feel of posts written by Reddit users. By comparing these vectors, we are not just measuring how much self-referential language is in each text---we are also measuring the extent to which each text is in the style of a questionnaire written by academics. This has the potential to introduce a confounding variable into the analysis---questionnaire-ness. 

The questionnaire-ness problem suggests that CCR is most effective for analyzing texts that bear a strong similarity to the questionnaire itself. For example, the texts of interest may be participant descriptions of their own values, while the questionnaire items are statements about values in the first person (as is commonly the case). In cases like this, CCR is likely to perform optimally. With this method, researchers can compare participant responses to the questionnaire without actually administering the questionnaire itself; participants can answer in their own words, which CCR will compare to the wording of the questionnaire (for preliminary empirical validation of this recommendation, see Appendix B). 

### Anchored Vectors in CCR and DDR

A second potential remedy for the questionnaire-ness problem is to compute CCRs for both the construct of interest and its opposite, and to operationalize the construct as the anchored vector between them. In the code below, we compute an anchored vector for locus of control by subtracting the CCR of the internal locus of control items from that of the external locus of control items. This anchored vector then measures internal _as opposed to external_ locus of control.

```{r}
#| include: false
internal_bge <- readRDS("example_data/internal_bge.rds")
external_bge <- readRDS("example_data/external_bge.rds")
```

```{r}
#| echo: true
# Anchored vector
loc_anchored <- internal_bge - external_bge
```

Anchored vectors can be applied to any construct that has a clear opposite, whether operationalized through CCR, DDR, or other methods. Although their application to contextualized embeddings relies on strong assumptions about the linearity of the contextualized embedding space, they have been shown to work reasonably well for a variety of constructs and models [@grand_etal_2022]. For an empirical demonstration of the robustness of anchored vectors for DDR, see Appendix A. For an empirical demonstration of their applicability to CCR, see Appendix B. 

In the following code, we use the anchored vector for locus of control that we computed above (`loc_anchored`) to measure locus of control in `reddit_emotions` and to test the hypothesis that posts in r/depression exhibit more external locus of control than those in r/TodayIamHappy [cf. @simchon_etal_2023].

```{r}
#| label: CCR-setup
#| include: false
reddit_emotion_bge <- read_csv("example_data/reddit_emotion_bge.csv")
```

```{r}
#| echo: true
# Calculate Dot Product With Anchored Vector
reddit_emotion_loc <- reddit_emotion_bge |> 
  rowwise() |> 
  mutate(loc = dot_prod(c_across(Dim1:Dim768), loc_anchored)) |> 
  pull(loc)
  
# Test Hypothesis
test_CCR <- t.test(
  x = reddit_emotion_loc[reddit_emotion$subreddit=="depression"],
  y = reddit_emotion_loc[reddit_emotion$subreddit=="TodayIamHappy"]
)
test_CCR$p.value
test_CCR$estimate
```

We find that posts in r/TodayIamHappy have more internal (as opposed to external) locus of control than those in r/depression (p < .001).

### CCR: Key Takeaways

CCR is a powerful tool because it can apply existing questionnaires---a fundamental tool in psychological research---to modern automated text analysis. In this sense it makes effective use of contextualized embeddings provided by state of the art LLMs. CCR thus allows psychologists to efficiently implement questionnaire-based research designs that nevertheless allow participants to compose responses in their own words. One limitation of CCR is its limited ability to generalize---it is thus contraindicated for texts that do not contain similar content and wording to the embedded questionnaire. Nevertheless the risk of questionnaire-ness described above can be mitigated by constructing an anchored vector, discussed below.

## Correlational Anchored Vectors (CAV)

```{r}
#| label: correlational-setup
#| include: false
d_train_bge <- read_csv("example_data/d_train_bge.csv")
d_train <- read_csv("example_data/power_narratives.csv") |> 
  mutate(condition = factor(condition, levels = c("Control", "Power")))
d_train <- d_train |> 
  bind_cols(d_train_bge)
```

An anchored vector is simply a direction in the embedding space. Rather than finding this direction by subtracting a negative construct embedding from a positive one, as we have done so far in this guide, we can use machine learning to find the direction that best represents the construct in a training dataset. This provides an alternative to DDR and CCR, by which construct representations are learned directly from examples. Such examples may be the product of human raters or of an experimental manipulation. For the example below we use data from @kasprzyk_calinjageman_2014, who asked participants to write about either incidents in which they had power over others (Power) or incidents in which other people had power over them (Control). 

```{r}
#| eval: false
#| echo: true
# Embed Training Data
d_train <- read_csv("example_data/power_narratives.csv") |> 
  mutate(condition = factor(condition, levels = c("Control", "Power")))

d_train_bge <- textEmbed(
  d_train$text,
  model = "BAAI/bge-base-en-v1.5", # model name
  tokens_select = "[CLS]", # select only [CLS] token embedding
  layers = -1,  # last layer
  dim_name = FALSE,
  keep_token_embeddings = FALSE
)
  
# Rejoin Embeddings to Labelled Training Data
d_train <- d_train |> 
  bind_cols(d_train_bge$texts[[1]])
```

With Partial Least Squares (PLS) regression [@mevik_wehrens_2007; @wold_etal_2001], which finds directions in the feature space that best correlate with the dependent variable (in this case Power as opposed to Control), we create a CAV. PLS is implemented here with the *caret* package [@kuhn_2008].

```{r}
#| echo: true
#| output: false
# Partial Least Squares (PLS) regression
set.seed(2024)
pls_control <- caret::train(
  condition ~ ., 
  data = select(d_train, condition, Dim1:Dim768), 
  method = "pls",
  scale = FALSE,  # keep original embedding dimensions
  trControl = caret::trainControl("cv", number = 10),  # cross-validation
  tuneLength = 1  # only 1 component (our anchored vector)
)
```

```{r}
#| echo: true
# Extract CAV
projection <- pls_control$finalModel$projection[,1]
power_loading <- pls_control$finalModel$Yloadings["Power",1]
power_cav <- projection*power_loading

# Calculate Distance Metrics
reddit_emotion_control <- reddit_emotion_bge |> 
  rowwise() |> 
  mutate(control = dot_prod(c_across(Dim1:Dim768), power_cav)) |> 
  pull(control)

# Test Hypothesis
test_correlational <- t.test(
  x = reddit_emotion_control[reddit_emotion$subreddit=="depression"],
  y = reddit_emotion_control[reddit_emotion$subreddit=="TodayIamHappy"]
)
test_correlational$p.value
test_correlational$estimate
```

We find that posts in r/TodayIamHappy share more characteristics in common with narratives of the speaker having control over others than did posts in r/depression (p < .001). 

### CAV: Key Takeaways

CAV allows the application of closely related texts. Given a training set that closely resembles the texts of interest, CAV provides a relatively simple metric that is likely to be highly accurate and unconfounded by differences in text genre. On the other hand, CAV requires applicable training data and introduces questions of validity when applying patterns in the training set to the texts of interest.

## Other Machine Learning Methods

Using PLS regression to generate a CAV is a very simple way to leverage machine learning for interpreting embeddings. The simplicity of dealing with a single direction in embedding space can provide useful intuition and transparency for researchers. Nevertheless, it sacrifices some of the predictive capacities available with modern machine learning methods. With more complex algorithms, the nonlinearity of contextualized embedding spaces becomes less of a problem. Given enough training data, one can specify a model that can capture nonlinear patterns, such as a support vector machine. More complex methods also allow the simultaneous use of embeddings from multiple layers of an LLM. A comprehensive review of machine learning algorithms for psychological features is beyond the scope of this guide. See @kjell_etal_2022 for a useful introductory primer.

# Conclusion

In this guide, we have introduced the fundamentals of neural word embedding models including word2vec, GloVe, and fastText. In doing so, we have put special emphasis on the geometric properties of the embeddings created by these models. For example, the correspondence between the magnitude of a word embedding and its semantic informativeness is guaranteed by the architecture of such models. This correspondence also forms the basis for the much-touted additive analogical property of word embeddings [@mikolov_etal_2013b; @ethayarajh_etal_2019]. This lies in contrast to embeddings generated by LLMs, which may have non-linear or non-zero-centered organizational patterns [@cai_etal_2021]. Decontextualized word embedding models therefore have two major advantages when compared to LLM embeddings: their reliable geometries and their computational efficiency. On the other hand, the contextualizing abilities of LLMs can often make up for such deficiencies, especially with the help of recent models specialized for use in semantic embedding. 

With the necessary background provided above, we have reviewed three methods of quantifying psychological constructs in embedding spaces: DDR, CCR, and CAV. We recommend DDR for abstract constructs relating to the overall feel of a text, especially when the research requires that these constructs generalize to various genres of text. When using dictionaries validated for use in word counting, we recommend weighting words by their frequency when computing the average DDR. We recommend CCR for cases in which texts are relatively similar in content to the embedded questionnaire, such as experiments in which participants are asked to respond to a related prompt. CAV is appropriate only when suitably large and reliable training data is available. 

# References

::: {#refs}
:::

# Appendix A

# DDR Metrics

```{r}
#| include: false
average_vector <- function(mat){
  mat <- as.matrix(mat)
  apply(mat, 2, mean)
}

# outputs 
new_scores <- function(dat, cols, pos_mean, neg_mean, prefix = "",
                       schemes = c("mean_dot", "mean_cos", "mean_euc", 
                                   "negdiff_dot", "negdiff_cos", "negdiff_euc", 
                                   "anchoredvec_norm")){
  dat <- dat |> rowwise()
  for(scheme in schemes){
    new_col_name = paste0(prefix, scheme)
    ccr <- pos_mean
    ccr_neg <- neg_mean
    if(str_detect(scheme, "negdiff_")){
      ccr <- ccr - ccr_neg
    }
    
    if(str_detect(scheme, "anchoredvec_")){
      if(scheme == "anchoredvec_norm"){
        ccr <- ccr/sqrt(sum(ccr^2))
        ccr_neg <- ccr_neg/sqrt(sum(ccr_neg^2))
      }
      dat <- dat |> mutate(!!new_col_name := anchored_sim(c_across({{cols}}), ccr, ccr_neg))
    }else{
      if(str_detect(scheme, "_dot")){
        dat <- dat |> mutate(!!new_col_name := dot_prod(c_across({{cols}}), ccr))
      }else if(str_detect(scheme, "_cos")){
        dat <- dat |> mutate(!!new_col_name := cos_sim(c_across({{cols}}), ccr))
      }else if(str_detect(scheme, "_euc")){
        dat <- dat |> mutate(!!new_col_name := -euc_dist(c_across({{cols}}), ccr))
      }
    }
  }
  dat |> ungroup() |> select(-{{cols}})
}
```

@garten_etal_2018 found that DDR works best with smaller dictionaries of only the words most directly connected to the construct being measured (around 30 words worked best in their experiments). Here we replicate their study using slightly different methods, and extend it to a variety of vector-based metrics, including anchored vectors. We also investigate the impact of weighting the averaged dictionary representation by token frequency, which we suggested would eliminate the observed superiority of smaller dictionaries.

## Benchmark 1: Negative Sentiment in Movie Reviews

As an initial benchmark, we used the same data used by @garten_etal_2018 in their investigation of dictionary size: a set of 2000 movie reviews, half labeled as negative and half as positive [@pang_lee_2005]. For vector representations of words and texts, we used a publicly available GloVe model trained on 2B Tweets to produce 100-dimensional embeddings [@pennington_etal_2014]. For construct representations, we used the positive tone and negative tone dictionaries from LIWC-22 [@boyd_etal_2022], expanded on the movie reviews dataset. The primary DDR was the average embedding of the negative tone dictionary, while for anchored vectors the average embedding of the positive tone dictionary was used as a second anchor.

We investigated the following metrics:

-   **Cosine similarity** with the negative tone DDR
-   **Cosine similarity with the anchored vector** (equivalent to projection of the normalized text vector onto the anchored vector)
-   **Dot product** with the negative tone DDR
-   **Dot product with the anchored vector** (equivalent to projection of the raw text vector onto the anchored vector)
-   **Dot product with the pre-normalized anchored vector** (i.e. positive and negative DDR embeddings normalized before calculating the anchored vector)
-   **Euclidean distance** from the negative tone DDR
-   **Euclidean distance from the anchored vector**

To evaluate the predictive value of each metric, we trained a univariate logistic regression model for each metric at each dictionary size. We then computed an F1 score for the model's predictions on the training set, with the model's threshold set at 0.5 (i.e. any text given a probability of greater than 0.5 of being negative was considered as having been predicted to be negative).

```{r}
#| echo: false
#| eval: false

# word embeddings
path_to_glove <- "~/Projects/ds4psych/data/glove/glove.twitter.27B.100d.txt"
glove_dimensions <- as.numeric(str_extract(path_to_glove, "[:digit:]+(?=d\\.txt)"))
glove_pretrained <- data.table::fread(
  path_to_glove, 
  quote = "",
  col.names = c("token", paste0("dim_", 1:glove_dimensions))
  ) |> 
  distinct(token, .keep_all = TRUE) |> 
  remove_rownames() |> 
  column_to_rownames("token") |> 
  as.matrix()
class(glove_pretrained) <- "embeddings"

# Load Data
reviews_files_neg <- list.files("benchmarks/DDR/txt_sentoken/neg", full.names = TRUE)
reviews_files_pos <- list.files("benchmarks/DDR/txt_sentoken/pos", full.names = TRUE)

reviews_neg <- sapply(reviews_files_neg, function(x) paste(readLines(x), collapse = " "))
reviews_pos <- sapply(reviews_files_pos, function(x) paste(readLines(x), collapse = " "))

reviews <- tibble(
  text = c(reviews_neg, reviews_pos),
  label = factor(rep(c("neg", "pos"), each = length(reviews_neg)))
) |> 
  mutate(label_int = as.integer(label) - 1L)

rm(reviews_neg, reviews_pos)

# Load Dictionaries (LIWC-22 expanded on corpus)
reviews_dfm <- reviews$text |> 
  tokens(remove_punct = TRUE, remove_url = TRUE) |> 
  dfm()
# reviews_tokens <- reviews_dfm |> 
#   featnames()
# tibble(text = reviews_tokens) |> 
#   write_csv("benchmarks/DDR/reviews_tokens.csv")

pos_dict <- read_csv("benchmarks/DDR/reviews_tokens.csv") |> 
  filter(tone_pos > 0) |> 
  pull(text)
neg_dict <- read_csv("benchmarks/DDR/reviews_tokens.csv") |> 
  filter(tone_neg > 0) |> 
  pull(text)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## DICTIONARY EMBEDDINGS (by word)

pos_dict_glove <- pos_dict |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  as.matrix()
rownames(pos_dict_glove) <- pos_dict

neg_dict_glove <- neg_dict |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  as.matrix()
rownames(neg_dict_glove) <- neg_dict

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## DICTIONARY WORD FREQUENCIES

pos_dict_freqs <- reviews_dfm |> 
  dfm_keep(pos_dict) |> 
  quanteda.textstats::textstat_frequency() |> 
  pull(frequency, name = feature)

neg_dict_freqs <- reviews_dfm |> 
  dfm_keep(neg_dict) |> 
  quanteda.textstats::textstat_frequency() |> 
  pull(frequency, name = feature)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~
## BENCHMARK TEXT EMBEDDINGS

reviews_glove <- reviews_dfm |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## RUN MODELS (construct = negative tone)

results_grid <- expand_grid(
  dict_size = c(10, 20, 30, 40, 50, 100, 200, 400),
  seed = 1:200,
  model = c("raw", "freq_weight"),
  scheme = c("mean_dot", "mean_cos", "mean_euc", 
             "negdiff_dot", "negdiff_cos", "negdiff_euc", 
             "anchoredvec_norm")
) |> 
  mutate(
    model_disp = if_else(model == "raw", "Equal Weighting", "Frequency Weighted"),
    scheme_disp = case_match(
      scheme,
      "mean_dot" ~ "Dot", 
      "mean_cos" ~ "Cosine", 
      "mean_euc" ~ "Euclidean", 
      "negdiff_dot" ~ "Dot (Anchored Vector)", 
      "negdiff_cos" ~ "Cosine (Anchored Vector)", 
      "negdiff_euc" ~ "Euclidean (Anchored Vector)", 
      "anchoredvec_norm" ~ "Dot (Pre-normalized Anchored Vector)"
      )
  )

results_grid <- results_grid |> 
  mutate(Acc = NA, F1 = NA, Beta = NA, sig = NA)

model <- "none"
for (row in 1:nrow(results_grid)) {
  # recalculate when needed
  if(model != results_grid$model[row]){
    message("Calculating scores for row ", row, "/", nrow(results_grid))
    dict_size <- results_grid$dict_size[row]
    seed <- results_grid$seed[row]
    model <- results_grid$model[row]
    
    if(model == "raw"){
      set.seed(seed)
      neg_DDR <- average_vector(neg_dict_glove[sample(1:nrow(neg_dict_glove), dict_size),])
      set.seed(seed)
      pos_DDR <- average_vector(pos_dict_glove[sample(1:nrow(pos_dict_glove), dict_size),])
    }else{
      set.seed(seed)
      neg_DDR <- neg_dict_glove[sample(1:nrow(neg_dict_glove), dict_size),]
      neg_DDR <- apply(neg_DDR, 2, weighted.mean, w = neg_dict_freqs[rownames(neg_DDR)], na.rm = TRUE)
      set.seed(seed)
      pos_DDR <- pos_dict_glove[sample(1:nrow(pos_dict_glove), dict_size),]
      pos_DDR <- apply(pos_DDR, 2, weighted.mean, w = pos_dict_freqs[rownames(pos_DDR)], na.rm = TRUE)
    }

    scores_df <- reviews |> 
      select(label, label_int) |> 
      bind_cols(new_scores(reviews_glove, dim_1:dim_100, neg_DDR, pos_DDR, prefix = paste0(model, "_")))
  }
  
  scheme <- results_grid$scheme[row]
  
  var_name <- paste(model, scheme, sep = "_")
  row_form <- as.formula(paste0("label~",var_name))
  
  mod <- glm(row_form, data = scores_df, family = binomial())
  mod_pred <- round(predict(mod, type = "response"))
  tp <- sum(mod_pred == 1 & scores_df$label_int == 1)
  fp <- sum(mod_pred == 1 & scores_df$label_int == 0)
  fn <- sum(mod_pred == 0 & scores_df$label_int == 1)
  
  results_grid$Acc[row] <- mean(mod_pred == scores_df$label_int)
  results_grid$F1[row] <- tp/(tp + (fp + fn)/2)
  results_grid$Beta[row] <- summary(mod)$coefficients[[2,"Estimate"]]
  results_grid$sig[row] <- summary(mod)$coefficients[[2,"Pr(>|z|)"]]
}

results_grid |> 
  # bind_rows(read_csv("benchmarks/DDR/results1.csv")) |> 
  write_csv("benchmarks/DDR/results1.csv")

# Check whether success is related to frequency
results_grid <- results_grid |> 
  mutate(mean_freq = NA, sd_freq = NA, entropy_freq = NA)

for (row in 1:nrow(results_grid)) {
  dict_size <- results_grid$dict_size[row]
  seed <- results_grid$seed[row]
  anchored <- str_detect(results_grid$scheme[row], "anchoredvec|negdiff")
  
  set.seed(seed)
  neg_freqs <- rownames(neg_dict_glove[sample(1:nrow(neg_dict_glove), dict_size),])
  neg_freqs <- neg_dict_freqs[neg_freqs]
  freqs <- neg_freqs
  results_grid$mean_freq[row] <- mean(freqs, na.rm = TRUE)
  results_grid$sd_freq[row] <- sd(freqs, na.rm = TRUE)
  results_grid$entropy_freq[row] <- entropy::entropy(freqs, method = "ML")
}

results_grid |> 
  write_csv("benchmarks/DDR/results1.csv")
```

When using only the primary DDR, we found that the performance of equal weighting drops sharply with increasing dictionary size, while the performance of frequency weighting continues to rise. Indeed, DDRs computed with equal weighting were likely to result in negative associations between the predictor and the outcome, even at small dictionary sizes. Surprisingly, metrics based on anchored vectors were robust to this instability. Additionally, frequency weighting was not superior to equal weighting for metrics based on anchored vectors. The overall best performing metric across all dictionary sizes was cosine similarity with the anchored vector calculated using equal weighting.

```{r}
#| echo: false
#| output: false
results_grid <- read_csv("benchmarks/DDR/results1.csv")
```

```{r}
#| label: fig-f1_by_dictsize
#| echo: false
#| fig-cap: Mean F1 by Dictionary Size
#| apa-note: Each data point represents the mean of 200 samples. F1 scores arising from negative associations between the predictor and the outcome are counted as negative.
#| fig-height: 4
#| fig-width: 6

results_grid |> 
  mutate(F1 = if_else(Beta > 0, -F1, F1),
         anchored = if_else(str_detect(scheme, "anchoredvec|negdiff"), "Anchored Vector", "Primary DDR")) |> 
  group_by(model_disp, scheme_disp, anchored, dict_size) |> 
  summarise(F1 = mean(F1), .groups = "drop") |> 
  ggplot(aes(dict_size, F1, color = scheme_disp, linetype = model_disp)) +
    geom_line() +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    labs(x = "Sampled Dictionary Size",
         y = "F1",
         color = "Metric",
         linetype = "Aggregation\nMethod") + 
    facet_wrap(~anchored) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0))
```

The pattern of results was similar among the highest performing dictionaries at each size, with two notable exceptions. First, frequency weighting was slightly better than equal weighting for all metrics (including those based on anchored vectors) except cosine similarity with the anchored vector. Second, the performance of frequency weighting among metrics using only the primary DDR did not increase with sample size.

```{r}
#| label: fig-f1_by_dictsize_top
#| echo: false
#| fig-cap: Mean F1 Score by Dictionary Size for Dictionaries Above the 80th Percentile
#| apa-note: Each data point represents the mean of 200 samples. F1 scores arising from negative associations between the predictor and the outcome are counted as negative.
#| fig-height: 4
#| fig-width: 6
results_grid |> 
  mutate(F1 = if_else(Beta > 0, -F1, F1),
         anchored = if_else(str_detect(scheme, "anchoredvec|negdiff"), "Anchored Vector", "Primary DDR")) |> 
  group_by(model_disp, scheme_disp, anchored, dict_size) |> 
  filter(F1 >= quantile(F1, probs = .8)) |> 
  summarise(F1 = mean(F1), .groups = "drop") |> 
  ggplot(aes(dict_size, F1, color = scheme_disp, linetype = model_disp)) +
    geom_line() +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    labs(x = "Sampled Dictionary Size",
         y = "F1",
         color = "Metric",
         linetype = "Aggregation\nMethod") + 
    facet_wrap(~anchored) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0))
```

```{r}
#| label: fig-neg_by_dictsize
#| echo: false
#| fig-cap: Significant Negative Effects by Dictionary Size
#| apa-note: 200 samples per data point.
#| fig-height: 4
#| fig-width: 6

results_grid |> 
  mutate(anchored = if_else(str_detect(scheme, "anchoredvec|negdiff"), "Anchored Vector", "Single DDR")) |> 
  group_by(model_disp, scheme_disp, anchored, dict_size) |> 
  summarise(false = 100*mean(Beta > 0 & sig < .05), .groups = "drop") |> 
  ggplot(aes(dict_size, false, color = scheme_disp, linetype = model_disp)) +
    geom_line() +
    scale_color_manual(values = c(
        "navyblue", "dodgerblue",
        "orange", "red2", "firebrick4",
        "seagreen2", "seagreen4"
      )) +
    labs(x = "Sampled Dictionary Size",
         y = "Significant Negative Effects (%)",
         color = "Metric",
         linetype = "Aggregation\nMethod") + 
    facet_wrap(~anchored) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0))
```

As a further validation of our proposed frequency weighting method, we investigated the performance of dictionary representations as a function of the variance of their word frequencies. We found that dictionaries with higher variation in word frequencies result in a large advantage for frequency weighted aggregation. As before, we found that this did not hold for metrics based on anchored vectors.

```{r}
#| label: fig-neg_by_freq_variance
#| echo: false
#| fig-cap: Mean F1 Score by Dictionary Frequency Variance
#| apa-note: Smoothing lines are computed with LOESS regression. F1 scores arising from negative associations between the predictor and the outcome are counted as negative.
#| fig-height: 4
#| fig-width: 8

results_grid |> 
  mutate(F1 = if_else(Beta > 0, -F1, F1),
         anchored = if_else(str_detect(scheme, "anchoredvec|negdiff"), "Anchored Vector", "Single DDR")) |> 
  ggplot(aes(sd_freq, F1, color = scheme_disp, linetype = model_disp)) +
    geom_smooth(method = "loess", formula = 'y ~ x') +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    labs(x = "SD Dictionary Word Frequency",
         y = "F1",
         color = "Metric",
         linetype = "Aggregation\nMethod") + 
    facet_wrap(~anchored) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0))
```

## Benchmark 2: Moral Foundations in Reddit Comments

To see whether the patterns observed for negative valence extend to more complex psychological constructs, we evaluated the same metrics and aggregation methods on a large dataset of Reddit comments (N = 16,123), which were manually annotated with six moral foundations: authority, care, fairness, loyalty, sanctity, and vice [@trager_etal_2022]. Each text was annotated by at least three annotators, giving a total of 53,545 cases.

To construct distributed construct representations for the moral foundations, we used the Moral Foundations Dictionary 2.0 [@frimer_etal_2019], which includes more than 200 words per foundation. Since the moral foundations do not have clear opposites, we constructed a neutral embedding by averaging the embeddings of all six foundations. This neutral embedding was used as the second anchor in anchored vector metrics.

Since the Reddit dataset was heavily imbalanced (i.e. most comments were not labeled as reflecting any given foundation), we set the classifier threshold at the empirical probability of each rating. For example, if 20% of texts in the dataset were labeled as reflecting loyalty, we would consider any text given a probability of greater than 0.2 to reflect loyalty according to the model. Using these predictions, F1 scores were computed as for the first benchmark.

```{r}
#| eval: false
#| echo: false

# Load Data
morality <- read_csv("https://huggingface.co/datasets/USC-MOLA-Lab/MFRC/resolve/main/final_mfrc_data.csv")

morality <- morality |> 
  separate_longer_delim(annotation, ",") |> 
  distinct() |> 
  pivot_wider(id_cols = c("text", "subreddit", "bucket", "annotator", "confidence"),
              names_from = "annotation", values_from = "annotation") |> 
  mutate(across(-c(text:confidence), function(x) if_else(is.na(x), 0, 1))) |> 
  rename(fairness = Equality, sanctity = Purity)

names(morality) <- str_to_lower(names(morality))


morality_unique <- morality |> 
  distinct(text)

morality_unique_dfm <- morality_unique$text |> 
  tokens(remove_punct = TRUE, remove_url = TRUE) |> 
  tokens_ngrams(n= c(1L, 2L)) |> 
  dfm() |> 
  dfm_keep(rownames(glove_pretrained))

# Load Dictionaries (Moral Foundations Dictionary 2.0)
mfd <- read_csv("benchmarks/DDR/mfd2.0.csv")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## DICTIONARY WORD FREQUENCIES

mfd_freqs <- morality_unique_dfm |> 
  dfm_keep(mfd$token) |> 
  quanteda.textstats::textstat_frequency() |> 
  pull(frequency, name = feature)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## DICTIONARY EMBEDDINGS (by word)

mfd_glove <- mfd |> 
  group_by(foundation) |> 
  summarise(token = paste(token, collapse = " ")) |> 
  pull(token) |> 
  tokens() |> 
  tokens_ngrams(n= c(1L, 2L)) |> 
  dfm() |> 
  dfm_keep(mfd$token) |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  as.matrix()
rownames(mfd_glove) <- unique(mfd$foundation)[order(unique(mfd$foundation))]

mfd_neutral_glove <- average_vector(mfd_glove)

mfd_glove_weighted <- mfd |> 
  pull(token) |> 
  tokens() |> 
  tokens_ngrams(n= c(1L, 2L)) |> 
  dfm() |> 
  dfm_keep(mfd$token) |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  as.matrix()
rownames(mfd_glove_weighted) <- mfd$token

mfd_glove_weighted <- do.call(rbind,
  lapply(rownames(mfd_glove)[1:5], function(construct){
    construct_tokens <- mfd$token[mfd$foundation == construct]
    construct_tokens <- construct_tokens[construct_tokens %in% featnames(morality_unique_dfm)]
    construct_token_weights <- mfd_freqs[construct_tokens]
    construct_mat <- mfd_glove_weighted[construct_tokens,]
    apply(construct_mat, 2, weighted.mean, w = construct_token_weights, na.rm = TRUE)
  })
)
rownames(mfd_glove_weighted) <- rownames(mfd_glove)[1:5]
mfd_neutral_glove_weighted <- average_vector(mfd_glove)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~
## BENCHMARK TEXT EMBEDDINGS

morality_unique_glove <- morality_unique_dfm |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)

#~~~~~~~~~~~~~~~~
## COMPUTE SCORES

for (construct in rownames(mfd_glove)) {
  morality_unique <- morality_unique |> 
    bind_cols(
      new_scores(
        morality_unique_glove,
        dim_1:dim_100, 
        mfd_glove[construct,], 
        mfd_neutral_glove, 
        prefix = paste0(construct,"_raw_")
        ),
      new_scores(
        morality_unique_glove,
        dim_1:dim_100, 
        mfd_glove_weighted[construct,], 
        mfd_neutral_glove_weighted, 
        prefix = paste0(construct,"_freqweight_")
        )
    )
}

morality <- morality |> 
  left_join(morality_unique)

#~~~~~~~~~~~~
## RUN MODELS

results_grid <- expand_grid(
  construct = rownames(mfd_glove),
  model = c("raw", "freqweight"),
  scheme = c("mean_dot", "mean_cos", "mean_euc", 
             "negdiff_dot", "negdiff_cos", "negdiff_euc", 
             "anchoredvec_norm")
) |> 
  mutate(
    model_disp = if_else(model == "raw", "Equal Weighting", "Frequency Weighted"),
    scheme_disp = case_match(
      scheme,
      "mean_dot" ~ "Dot", 
      "mean_cos" ~ "Cosine", 
      "mean_euc" ~ "Euclidean", 
      "negdiff_dot" ~ "Dot (Anchored Vector)", 
      "negdiff_cos" ~ "Cosine (Anchored Vector)", 
      "negdiff_euc" ~ "Euclidean (Anchored Vector)", 
      "anchoredvec_norm" ~ "Dot (Pre-normalized Anchored Vector)"
      )
  )

results_grid <- results_grid |> 
  mutate(Acc = NA, F1 = NA, Beta = NA, sig = NA, AIC = NA)

for (row in 1:nrow(results_grid)) {
  message("Calculating scores for row ", row, "/", nrow(results_grid))
  construct <- results_grid$construct[row]
  model <- results_grid$model[row]
  scheme <- results_grid$scheme[row]
  
  var_name <- paste(construct, model, scheme, sep = "_")
  row_form <- as.formula(paste0(construct,"~",var_name))
  
  mod <- glm(row_form, data = morality, family = binomial())
  mod_pred <- as.numeric(predict(mod, type = "response") > mean(pull(morality, {{construct}})))
  tp <- sum(mod_pred == 1 & pull(morality, {{construct}}) == 1)
  fp <- sum(mod_pred == 1 & pull(morality, {{construct}}) == 0)
  fn <- sum(mod_pred == 0 & pull(morality, {{construct}}) == 1)
  
  results_grid$Acc[row] <- mean(mod_pred == pull(morality, {{construct}}))
  results_grid$F1[row] <- tp/(tp + (fp + fn)/2)
  results_grid$Beta[row] <- summary(mod)$coefficients[[2,"Estimate"]]
  results_grid$sig[row] <- summary(mod)$coefficients[[2,"Pr(>|z|)"]]
  results_grid$AIC[row] <- mod$aic
}

write_csv(results_grid, "benchmarks/DDR/results2.csv")
```

We found that frequency weighted aggregation performed better than equal weighting in all five moral foundations except loyalty, for which all metrics performed comparably. Furthermore, the pattern of optimal metrics was somewhat erratic for equal weighting, whereas with frequency weighted aggregation, cosine similarity with the DDR performed best in all five foundations.

Curiously, anchored vector metrics did not perform as well as simple similarity scores. This may be attributable to the use of a neutral embedding as the second anchor, rather than a true opposite.

```{r}
#| include: false
results_grid <- read_csv("benchmarks/DDR/results2.csv")
```

```{r}
#| label: fig-morality_equalvsfreq
#| echo: false
#| warning: false
#| fig-cap: Equal vs. Frequency Weights for Moral Foundation DDRs
#| apa-note: Vertical lines represent mean F1 scores across moral foundations.
#| fig-height: 4
#| fig-width: 8

scheme_agg <- results_grid |> 
  mutate(F1 = if_else(Beta < 0, -F1, F1)) |> 
  group_by(model_disp, scheme_disp) |> 
  summarise(F1 = mean(F1),
            F1_sd = sd(F1),
            F1_max = max(F1), .groups = "drop") |> 
  group_by(model_disp) |> 
  arrange(F1) |> 
  mutate(scheme_disp = factor(scheme_disp))

results_grid |> 
  mutate(F1 = if_else(Beta < 0, -F1, F1),
         construct_disp = str_to_title(construct)) |> 
  ggplot(
    aes(F1, construct_disp, 
        xmin = 0, xmax = F1,
        color = scheme_disp)
    ) +
    geom_linerange(position = position_dodge(width = 1/2), alpha = 0) +
    # annotate("tile", 2, seq(1,6,by=2), width = Inf, alpha = .1) +
    geom_point(position = position_dodge(width = 1/2), size = 2) +
    geom_vline(aes(xintercept = F1, color = scheme_disp), 
               linewidth = 1,
               data = scheme_agg) +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    facet_wrap(~model_disp, ncol = 1) +
    labs(x = "F1",
         y = "Moral Foundation",
         linetype = "Model",
         color = "Metric") +
    theme_bw() +
    theme(plot.caption = element_text(hjust = .1))
```

For equal weighting, Euclidean distance from the anchored vector resulted in significant negative effects in 1/5 foundations. For frequency weighted aggregation, the dot product with the pre-normalized anchored vector resulted in significant negative effects in 3/5 foundations. Otherwise no negative effects were observed.

```{r}
#| echo: false
#| eval: false
results_grid |> 
  group_by(model_disp, scheme_disp) |> 
  summarise(prop_neg = sum(Beta < 0 & sig < 0.05)/n(), .groups = "keep")
```

## Conclusions

The results of our experiments support our suggestion that the diminishing performance of larger dictionaries is due to the influence of less frequent words. We found that computing the DDR as a weighted average generally improves performance, especially for larger dictionaries of a few hundred words. We further found that metrics based on anchored vectors were largely robust to the influence of term weighting. Nevertheless, metrics based on anchored vectors require a construct with a clear opposite---while anchored vectors performed well for negative vs. positive sentiment, they did not perform well for moral foundations as compared to a neutral moral foundation embedding.

# Appendix B

# CCR Metrics

@atari_etal_2023 asked participants to describe their core values in their own words (values essay) and to likewise describe their activities in the past week (behaviors essay). They then assessed participants on 22 questionnaire-based scales. This design allowed them to validate CCR by obtaining a contextualized embedding of each questionnaire and comparing it to the contextualized embedding of each essay. In benchmark 1 we partially replicate Atari et al.'s analysis and extend it to various vector-based metrics, including anchored vectors. In benchmark 2, we investigate the extent to which the techniques generalize to a more naturalistic context.

## Benchmark 1: Prompted Essays

The anchored vector is equivalent to the embedding of the questionnaire (positive CCR) minus the embedding of the negated questionnaire (negative CCR). The individualism items were used as a negated form of the collectivism, and vice versa. To obtain negated versions of the remaining questionnaires, we queried GPT-4o and manually curated the results.[^11] We investigated the following metrics:

[^11]: All matrials, including original and negated questionnaire items, are available on our Github repo at https://github.com/rimonim/embeddings_tutorial.

-   **Cosine similarity** with the CCR
-   **Cosine similarity with the anchored vector** (equivalent to projection of the normalized text vector onto the anchored vector)
-   **Dot product** with the CCR
-   **Dot product with the anchored vector** (equivalent to projection of the raw text vector onto the anchored vector)
-   **Dot product with the pre-normalized anchored vector** (i.e. positive and negative CCR embeddings normalized before calculating the anchored vector)
-   **Euclidean distance** from the CCR
-   **Euclidean distance from the anchored vector**

Although the term CCR implies the use of contextualized embeddings, we use it here to refer to any vector embedding of a questionnaire. To obtain embeddings for participant essays and questionnaire items, we used two pretrained models:

-   **SBERT** ([`sentence-transformers/all-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)), a model similar to that used by @atari_etal_2023, designed to produce 384-dimensional contextualized embeddings amenable to use with cosine similarity.
-   **GloVe** ([`glove.twitter.27B.100d`](https://huggingface.co/stanfordnlp/glove/tree/main)) a pretrained word embedding model with a 100-dimensional embedding space. GloVe embeddings each text were obtained by aggregating the embeddings of each word in the text, discounting tokens not available in the pretrained GloVe model. Since GloVe is a decontextualized model, we used it here as a baseline for evaluating the utility of contextualized embeddings in CCR analysis.

As a further baseline, we also performed DDR using dictionaries for each construct and its opposite. These dictionaries were generated by GPT-4o and manually curated, but were not validated in any way prior to testing. We can therefore consider this DDR to be a conservative baseline for evaluating CCR methods. DDR analysis was performed with the same GloVe model described above.

All code is available in the source code for this appendix on Github.

```{r}
#| eval: false
#| echo: false
library(tidyverse)
library(quanteda)
library(text)

#~~~~~~~~~~~
## LOAD DATA

# data from https://osf.io/bu6wg/
CCR_behavioral <- read_csv("benchmarks/CCR/CCR_clean_behavioral.csv")

CCR_items <- read_csv("benchmarks/CCR/Questionnaires - Experiment - Questionnaire.csv") |> 
  pivot_longer(
    everything(),
    names_to = "construct",
    values_to = "item",
    values_drop_na = TRUE
  ) |> 
  mutate(construct = str_to_lower(str_extract(construct, "[:letter:]+"))) |> 
  arrange(construct)

names(CCR_behavioral)[3:24] <- c(
  "care", "equality", "proportionality", "loyalty", "authority", "purity", 
  "individualism", "collectivism", "sd", "po", "un", "ac", "se", "st", "co", 
  "tr", "he", "be", "religiosity", "nfc", "conservatism", "tightness"
  )

# negative items
# - individualism and collectivism were used as each other's opposites
# - otherwise, reverse items were obtained from GPT-4o and curated manually
CCR_items_neg <- read_csv("benchmarks/CCR/questionnaires_neg.csv") |> 
  pivot_longer(
    everything(),
    names_to = "construct",
    values_to = "item",
    values_drop_na = TRUE
  ) |> 
  mutate(construct = str_to_lower(str_extract(construct, "[:letter:]+"))) |> 
  arrange(construct)

# DDR items (obtained from GPT-4o and curated manually)
DDR_items <- read_csv("benchmarks/CCR/ddr_items.csv")
DDR_items_neg <- read_csv("benchmarks/CCR/ddr_items_neg.csv")
#~~~~~~~~~~~
## FUNCTIONS

# relevant scripts
source("embedding_scripts.R")

# consistent embedding scheme
contextualized_embedding <- function(x){
  textEmbed(
    x,
    model = "sentence-transformers/all-MiniLM-L12-v2", # model name
    layers = -1,  # last layer
    dim_name = FALSE,
    keep_token_embeddings = FALSE
  )
}

average_vector <- function(mat){
  mat <- as.matrix(mat)
  apply(mat, 2, mean)
}

# word embeddings
path_to_glove <- "~/Projects/ds4psych/data/glove/glove.twitter.27B.100d.txt"
glove_dimensions <- as.numeric(str_extract(path_to_glove, "[:digit:]+(?=d\\.txt)"))
glove_pretrained <- read_delim(
  path_to_glove, 
  delim = " ",
  quote = "",
  escape_double = FALSE,
  col_names = c("token", paste0("dim_", 1:glove_dimensions))
) |> distinct(token, .keep_all = TRUE) |> column_to_rownames("token") |> as.matrix()
class(glove_pretrained) <- "embeddings"

# outputs 
new_scores <- function(dat, cols, pos_mean, neg_mean, prefix = "",
                       schemes = c("mean_dot", "mean_cos", "mean_euc", 
                                   "negdiff_dot", "negdiff_cos", "negdiff_euc", 
                                   "anchoredvec_raw", "anchoredvec_norm")){
  dat <- dat |> rowwise()
  for(scheme in schemes){
    new_col_name = paste0(prefix, scheme)
    ccr <- pos_mean
    ccr_neg <- neg_mean
    if(str_detect(scheme, "negdiff_")){
      ccr <- ccr - ccr_neg
    }
    
    if(str_detect(scheme, "anchoredvec_")){
      if(scheme == "anchoredvec_norm"){
        ccr <- ccr/sqrt(sum(ccr^2))
        ccr_neg <- ccr_neg/sqrt(sum(ccr_neg^2))
      }
      dat <- dat |> mutate(!!new_col_name := anchored_sim(c_across({{cols}}), ccr, ccr_neg))
    }else{
      if(str_detect(scheme, "_dot")){
        dat <- dat |> mutate(!!new_col_name := dot_prod(c_across({{cols}}), ccr))
      }else if(str_detect(scheme, "_cos")){
        dat <- dat |> mutate(!!new_col_name := cos_sim(c_across({{cols}}), ccr))
      }else if(str_detect(scheme, "_euc")){
        dat <- dat |> mutate(!!new_col_name := -euc_dist(c_across({{cols}}), ccr))
      }
    }
  }
  dat |> ungroup() |> select(-{{cols}})
}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## QUESTIONNAIRE EMBEDDINGS (matrices)

CCR_items_glove <- CCR_items$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  bind_cols(CCR_items) |> 
  group_by(construct) |> 
  summarise(across(dim_1:dim_100, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

CCR_items_neg_glove <- CCR_items_neg$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id) |> 
  bind_cols(CCR_items) |> 
  group_by(construct) |> 
  summarise(across(dim_1:dim_100, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

CCR_items_sbert <- contextualized_embedding(CCR_items$item)$texts[[1]] |> 
  bind_cols(CCR_items) |> 
  group_by(construct) |> 
  summarise(across(Dim1:Dim384, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

CCR_items_neg_sbert <- contextualized_embedding(CCR_items_neg$item)$texts[[1]] |> 
  bind_cols(CCR_items_neg) |> 
  group_by(construct) |> 
  summarise(across(Dim1:Dim384, mean)) |> 
  column_to_rownames("construct") |> 
  as.matrix()

DDR_items_glove <- DDR_items$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  bind_cols(DDR_items) |> 
  select(-doc_id, -item) |> 
  column_to_rownames("construct") |> 
  as.matrix()

DDR_items_neg_glove <- DDR_items_neg$item |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  bind_cols(DDR_items_neg) |> 
  select(-doc_id, -item) |> 
  column_to_rownames("construct") |> 
  as.matrix()

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## PARTICIPANT TEXT EMBEDDINGS

# response embeddings
values_survey_sbert <- contextualized_embedding(CCR_behavioral$ValuesSurvey)
# saveRDS(values_survey_sbert, "~/Projects/ds4psych/data/values_survey_sbert.rds")
behaviors_survey_sbert <- contextualized_embedding(CCR_behavioral$BehaviorsSurvey)
# saveRDS(behaviors_survey_sbert, "~/Projects/ds4psych/data/behaviors_survey_sbert.rds")

values_survey_sbert <- readRDS("~/Projects/ds4psych/data/values_survey_sbert.rds")
behaviors_survey_sbert <- readRDS("~/Projects/ds4psych/data/behaviors_survey_sbert.rds")

values_survey_glove <- CCR_behavioral$ValuesSurvey |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)
behaviors_survey_glove <- CCR_behavioral$BehaviorsSurvey |> 
  tokens() |> 
  dfm() |> 
  textstat_embedding(glove_pretrained) |> 
  select(-doc_id)

#~~~~~~~~~~~~~~~~
## COMPUTE SCORES

for (construct in rownames(CCR_items_sbert)) {
  CCR_behavioral <- CCR_behavioral |> 
    bind_cols(
      new_scores(
        values_survey_sbert$texts[[1]],
        Dim1:Dim384, 
        CCR_items_sbert[construct,], 
        CCR_items_neg_sbert[construct,], 
        prefix = paste0(construct,"_values_sbert_")
        ),
      new_scores(
        values_survey_glove,
        dim_1:dim_100, 
        CCR_items_glove[construct,], 
        CCR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_values_glove_")
        ),
      new_scores(
        values_survey_glove,
        dim_1:dim_100, 
        DDR_items_glove[construct,], 
        DDR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_values_ddr_")
        ),
      new_scores(
        behaviors_survey_sbert$texts[[1]],
        Dim1:Dim384, 
        CCR_items_sbert[construct,], 
        CCR_items_neg_sbert[construct,], 
        prefix = paste0(construct,"_behaviors_sbert_")
        ),
      new_scores(
        behaviors_survey_glove,
        dim_1:dim_100, 
        CCR_items_glove[construct,], 
        CCR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_behaviors_glove_")
        ),
      new_scores(
        behaviors_survey_glove,
        dim_1:dim_100, 
        DDR_items_glove[construct,], 
        DDR_items_neg_glove[construct,], 
        prefix = paste0(construct,"_behaviors_ddr_")
        )
    )
}

#~~~~~~~~~~~~
## RUN MODELS

results_grid <- expand_grid(
  construct = rownames(CCR_items_sbert),
  text = c("behaviors", "values"),
  model = c("glove", "sbert", "ddr"),
  scheme = c("mean_dot", "mean_cos", "mean_euc", 
             "negdiff_dot", "negdiff_cos", "negdiff_euc", 
             "anchoredvec_norm")
) |> bind_cols(expand_grid(
  construct_disp = c(
    "Achievement", "Authority", "Benevolence", "Care", 
    "Conformity", "Collectivism", "Conservatism", 
    "Equality", "Hedonism", "Individualism", 
    "Loyalty", "Need for Cognition", "Power", "Proportionality", 
    "Purity", "Religiosity", "Self-Direction", "Safety", 
    "Stimulation", "Tightness", "Tradition", "Universalism"
    ),
  text_disp = c("Behaviors Essay", "Values Essay"),
  model_disp = c("GloVe", "SBERT", "DDR"),
  scheme_disp = c(
    "Dot", "Cosine", "Euclidean", 
    "Dot (Anchored Vector)", "Cosine (Anchored Vector)", "Euclidean (Anchored Vector)", 
    "Dot (Pre-normalized Anchored Vector)"
    )
))

results_grid <- results_grid |> 
  mutate(R2 = NA, Beta = NA, SE = NA, sig = NA)

for (row in 1:nrow(results_grid)) {
  construct <- results_grid$construct[row]
  text <- results_grid$text[row]
  model <- results_grid$model[row]
  scheme <- results_grid$scheme[row]
  
  var_name <- paste(construct, text, model, scheme, sep = "_")
  row_form <- as.formula(paste0("scale(",construct,")~scale(",var_name,")"))
  
  mod <- lm(row_form, data = CCR_behavioral)
  results_grid$R2[row] <- summary(mod)$r.squared
  results_grid$Beta[row] <- summary(mod)$coefficients[[2,"Estimate"]]
  results_grid$SE[row] <- summary(mod)$coefficients[[2,"Std. Error"]]
  results_grid$sig[row] <- summary(mod)$coefficients[[2,"Pr(>|t|)"]]
}

write_csv(results_grid, "benchmarks/CCR/results.csv")
```

### Results: Values Essay

In predicting questionnaire responses using participant values essays, the most effective metrics were as follows:

**DDR:**

-   Dot product with anchored vector (mean R^2^ = 0.016)
-   Dot product with pre-normalized anchored vector (mean R^2^ = 0.015)
-   Cosine similarity with anchored vector (mean R^2^ = 0.012)

**GloVe:**

-   Cosine similarity with anchored vector (mean R^2^ = 0.010)
-   Dot product with pre-normalized anchored vector (mean R^2^ = 0.009)

**SBERT:**

-   Dot product with anchored vector (mean R^2^ = 0.023)
-   Cosine similarity with anchored vector (mean R^2^ = 0.023)
-   Dot product with pre-normalized anchored vector (mean R^2^ = 0.022)

```{r}
#| include: false
results_grid <- read_csv("benchmarks/CCR/results.csv")

construct_agg <- results_grid |> 
  mutate(R2 = if_else(sig < .05 | Beta > 0, R2, 0)) |> 
  group_by(text, construct, construct_disp) |> 
  summarise(R2 = mean(R2),
            R2_sd = sd(R2),
            R2_max = max(R2)) |> 
  group_by(text) |> 
  arrange(R2_max) |> 
  mutate(construct_disp = factor(construct_disp))

scheme_agg <- results_grid |> 
  mutate(R2 = if_else(sig < .05 | Beta > 0, R2, 0)) |> 
  group_by(text, model_disp, scheme_disp) |> 
  summarise(R2 = mean(R2),
            R2_sd = sd(R2),
            R2_max = max(R2)) |> 
  group_by(text, model_disp) |> 
  arrange(R2) |> 
  mutate(scheme_disp = factor(scheme_disp))
```

```{r}
#| label: fig-values_essay
#| echo: false
#| warning: false
#| fig-cap: Values Essay
#| apa-note: Negative or insignificant effects (p > 0.05) are displayed as translucent and are considered to be 0 in metric-wise averages.
#| fig-height: 8
#| fig-width: 8

results_grid |> 
  filter(text == "values") |> 
  mutate(construct_disp = factor(
    construct_disp, 
    levels = construct_agg$construct_disp[construct_agg$text=="values"]
    )) |> 
  ggplot(
    aes(R2*100, construct_disp, 
        xmin = 0, xmax = R2*100,
        color = scheme_disp, alpha = sig < .05 & Beta > 0)
    ) +
    geom_linerange(position = position_dodge(width = 4/5), alpha = 0) +
    annotate("tile", 3, seq(1,21,by=2), width = Inf, alpha = .1) +
    geom_point(position = position_dodge(width = 4/5), size = 2) +
    geom_vline(aes(xintercept = R2*100, color = scheme_disp), 
               linewidth = 1,
               data = scheme_agg |> filter(text=="values")) +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    facet_wrap(~model_disp, ncol = 1) +
    guides(alpha = "none") +
    labs(x = "Variance Explained (%)",
         y = "Questionnaire",
         linetype = "Model",
         color = "Metric") +
    theme_bw() +
    theme(plot.caption = element_text(hjust = .1))
```

For values essays, SBERT was consistently more effective than GloVe, and generally better than DDR. In all models, Euclidean distance metrics were minimally effective and most likely to result in significant negative effects. The success of GloVe-based CCR with metrics that involve anchored vectors is surprising, since most negated questionnaire items only differ from the originals by the words "do not" or similarly uninformative negations.

```{r}
#| label: fig-values_essay_neg
#| echo: false
#| fig-cap: Significant Negative Effects in Values Essay
#| fig-height: 3
#| fig-width: 8
results_grid |> 
  filter(text == "values") |> 
  group_by(model_disp, scheme_disp) |> 
  summarise(prop_neg = sum(Beta < 0 & sig < 0.05)/n(), .groups = "keep") |> 
  ggplot(aes(scheme_disp, prop_neg*100, fill = scheme_disp)) +
    geom_bar(stat = "identity") +
    geom_hline(yintercept = 0, linewidth = 1.5) +
    facet_wrap(~model_disp) +
    scale_fill_manual(values = c(
        "navyblue", "dodgerblue",
        "orange", "red2", "firebrick4",
        "seagreen2", "seagreen4"
      )) +
    labs(y = "Significant Negative Effects (%)",
         fill = "Metric") +
    theme_bw() +
    theme(axis.text.x = element_blank(),
          axis.title.x = element_blank())
```

### Results: Behaviors Essay

In predicting questionnaire responses using participant behaviors essays, the most effective metrics were as follows:

**DDR:**

-   Dot product with anchored vector (mean R^2^ = 0.017)
-   Dot product with CCR (mean R^2^ = 0.016)
-   Euclidean distance from anchored vector (mean R^2^ = 0.014)

**GloVe:**

-   Dot product with CCR (mean R^2^ = 0.014)
-   Euclidean distance from anchored vector (mean R^2^ = 0.014)
-   Dot product with anchored vector (mean R^2^ = 0.006)

**SBERT:**

-   Euclidean distance from anchored vector (mean R^2^ = 0.008)
-   Dot product with CCR (mean R^2^ = 0.006)

```{r}
#| label: fig-behaviors_essay
#| echo: false
#| warning: false
#| fig-cap: Behaviors Essay
#| apa-note: Negative or insignificant effects (p > 0.05) are displayed as translucent and are considered to be 0 in metric-wise averages.
#| fig-height: 8
#| fig-width: 8
results_grid |> 
  filter(text == "behaviors") |> 
  mutate(construct_disp = factor(
    construct_disp, 
    levels = construct_agg$construct_disp[construct_agg$text=="behaviors"]
    )) |> 
  ggplot(
    aes(R2*100, construct_disp, 
        xmin = 0, xmax = R2*100,
        color = scheme_disp, alpha = sig < .05 & Beta > 0)
    ) +
    geom_linerange(position = position_dodge(width = 4/5), alpha = 0) +
    annotate("tile", 3, seq(1,21,by=2), width = Inf, alpha = .1) +
    geom_point(position = position_dodge(width = 4/5), size = 2) +
    geom_vline(aes(xintercept = R2*100, color = scheme_disp), 
               linewidth = 1,
               data = scheme_agg |> filter(text=="behaviors")) +
    scale_color_manual(values = c(
      "navyblue", "dodgerblue",
      "orange", "red2", "firebrick4",
      "seagreen2", "seagreen4"
    )) +
    facet_wrap(~model_disp, ncol = 1) +
    guides(alpha = "none") +
    labs(x = "Variance Explained (%)",
         y = "Questionnaire",
         linetype = "Model",
         color = "Metric") +
    theme_bw() +
    theme(plot.caption = element_text(hjust = .1))
```

For behaviors essays, GloVe-based CCR was more consistently effective than SBERT, and DDR was most effective overall. While Euclidean distance from the anchored vector was most effective on average for both models, it was also most likely to result in significant negative effects.

```{r}
#| label: fig-behaviors_essay_neg
#| echo: false
#| fig-cap: Significant Negative Effects in Behaviors Essay
#| fig-height: 3
#| fig-width: 8
results_grid |> 
  filter(text == "behaviors") |> 
  group_by(model_disp, scheme_disp) |> 
  summarise(prop_neg = sum(Beta < 0 & sig < 0.05)/n(), .groups = "keep") |> 
  ggplot(aes(scheme_disp, prop_neg*100, fill = scheme_disp)) +
    geom_bar(stat = "identity") +
    geom_hline(yintercept = 0, linewidth = 1.5) +
    facet_wrap(~model_disp) +
    scale_fill_manual(values = c(
        "navyblue", "dodgerblue",
        "orange", "red2", "firebrick4",
        "seagreen2", "seagreen4"
      )) +
    labs(y = "Significant Negative Effects (%)",
         fill = "Metric") +
    theme_bw() +
    theme(axis.text.x = element_blank(),
          axis.title.x = element_blank())
```

## Conclusions

CCR [@atari_etal_2023] proposes to measure psychological constructs in texts by comparing text embeddings to embeddings of questionnaires. This approach to text analysis can be highly effective, but is sensitive to the nature of both questionnaires and texts. Some questionnaires do not appear to be amenable to CCR at all, including those used here for tightness, collectivism, individualism, proportionality, equality, and safety. Among those scales that were amenable to CCR metrics, the pattern of optimal metrics was greatly influenced by the content of the texts being analyzed.

Values essays tend to be similar in content to the values questionnaires being used, since the questionnaire items almost all consist of statements in the first person. Contextualized embeddings from an SBERT model appear to perform best in these sorts of situations.

Behaviors essays, which bear little resemblance to questionnaire items in either tone or content, behave very differently. In particular, GloVe embeddings of the questionnaires tend to perform better. This may be due to the consistent geometric properties of GloVe embeddings, which can more reliably encode semantic relationships between very different contexts. Alternatively, this may be due to the datasets used to train SBERT models, which emphasize topical similarity rather than similarity in tone [@reimers_gurevych_2019]. Unsurprisingly, GloVe embeddings of dictionaries associated with the questionnaires performed best overall.

Overall, the results of this analysis suggest that CCR is can be a powerful tool, but that it should be used primarily in contexts in which the content of the questionnaires is similar to that of the texts being analyzed. In such cases, we recommend negating the questionnaire items, computing an anchored vector, and scoring texts by the dot products of their embeddings with the anchored vector.

When the content of the questionnaires is not similar to that of the texts being analyzed, we recommend using DDR with negative and positive versions of each dictionary, and scoring texts by the dot products of their embeddings with the anchored vector. This approach appears to outperform CCR even with unvalidated dictionaries generated by GPT-4o.
